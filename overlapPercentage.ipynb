{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "508aad10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29371c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "numEpochs = 50 # number of epochs to train the GPT+PT model\n",
    "embeddingSize = 512 # the hidden dimension of the representation of both GPT and PT\n",
    "numPoints=500 # number of points that we are going to receive to make a prediction about f given x and y, if you don't know then use the maximum\n",
    "numVars=5 # the dimenstion of input points x, if you don't know then use the maximum\n",
    "numYs=1 # the dimension of output points y = f(x), if you don't know then use the maximum\n",
    "blockSize = 100 # spatial extent of the model for its context\n",
    "batchSize = 128 # batch size of training data\n",
    "dataDir = 'D:/Datasets/Symbolic Dataset/Datasets/FirstDataGenerator'\n",
    "dataInfo = 'XYE_{}Var_{}Points_{}EmbeddingSize'.format(numVars, numPoints, embeddingSize)\n",
    "target = 'Skeleton' #'Skeleton' #'EQ'\n",
    "dataFolder = \"1-5Var_RandSupport_RandLength_0to3_3.1to6_100to500Points\"\n",
    "addr = './SavedModels/bestModel/' # where to save model\n",
    "maxNumFiles = 30\n",
    "bestLoss = None # if there is any model to load as pre-trained one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b756d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, block_size, chars, target='EQ'):\n",
    "        data_size, vocab_size = len(data), len(chars)\n",
    "        print('data has %d examples, %d unique.' % (data_size, vocab_size))\n",
    "        \n",
    "        self.stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "        self.itos = { i:ch for i,ch in enumerate(chars) }\n",
    "        \n",
    "        # padding token\n",
    "        self.paddingToken = '_'\n",
    "        self.paddingID = self.stoi[self.paddingToken]\n",
    "        self.stoi[self.paddingToken] = self.paddingID\n",
    "        self.itos[self.paddingID] = self.paddingToken\n",
    "        self.threshold = [-1,1]\n",
    "        \n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.data = data # it should be a list of examples\n",
    "        self.target = target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)-1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # grab an example from the data\n",
    "        chunk = self.data[idx] # sequence of tokens including x, y, eq, etc.\n",
    "        \n",
    "        try:\n",
    "            chunk = json.loads(chunk) # convert the sequence tokens to a dictionary\n",
    "        except:\n",
    "            print(\"Couldn't convert to json: {}\".format(chunk))\n",
    "            \n",
    "        # encode every character in the equation to an integer\n",
    "        # < is SOS, > is EOS\n",
    "        dix = [self.stoi[s] for s in '<'+chunk[self.target]+'>']\n",
    "        inputs = dix[:-1]\n",
    "        outputs = dix[1:]\n",
    "        \n",
    "        # add the padding to the equations\n",
    "        paddingSize = max(self.block_size-len(inputs),0)\n",
    "        paddingList = [self.paddingID]*paddingSize\n",
    "        inputs += paddingList\n",
    "        outputs += paddingList \n",
    "        \n",
    "        # make sure it is not more than what should be\n",
    "        inputs = inputs[:self.block_size]\n",
    "        outputs = outputs[:self.block_size]\n",
    "        \n",
    "        # extract points from the input sequence\n",
    "        points = torch.zeros(numVars+numYs, numPoints)\n",
    "        for idx, xy in enumerate(zip(chunk['X'], chunk['Y'])):\n",
    "            x = xy[0] + [0]*(max(numVars-len(xy[0]),0)) # padding\n",
    "            y = [xy[1]] if type(xy[1]) == float or type(xy[1]) == int else xy[1]\n",
    "            y = y + [0]*(max(numYs-len(y),0)) # padding\n",
    "            p = x+y # because it is only one point \n",
    "            p = torch.tensor(p)\n",
    "            #replace nan and inf\n",
    "            p = torch.nan_to_num(p, nan=0.0, \n",
    "                                 posinf=self.threshold[1], \n",
    "                                 neginf=self.threshold[0])\n",
    "            p[p>self.threshold[1]] = self.threshold[1] # clip the upper bound\n",
    "            p[p<self.threshold[0]] = self.threshold[0] # clip the lower bound\n",
    "            points[:,idx] = p\n",
    "        \n",
    "        \"\"\"\n",
    "        arrange data and targets so that the first i elements of x\n",
    "        will be asked to predict the i-th element of y. Notice that\n",
    "        the eventual language model will actually make block_size\n",
    "        individual predictions at the same time based on this data,\n",
    "        so we are being clever and amortizing the cost of the forward\n",
    "        pass of the network. So for example if block_size is 4, then\n",
    "        we could e.g. sample a chunk of text \"hello\", the integers in\n",
    "        x will correspond to \"hell\" and in y will be \"ello\". This will\n",
    "        then actually \"multitask\" 4 separate examples at the same time\n",
    "        in the language model:\n",
    "        - given just \"h\", please predict \"e\" as next\n",
    "        - given \"he\" please predict \"l\" next\n",
    "        - given \"hel\" predict \"l\" next\n",
    "        - given \"hell\" predict \"o\" next\n",
    "        \n",
    "        In addition, because the DataLoader will create batches of examples,\n",
    "        every forward/backward pass during traning will simultaneously train\n",
    "        a LOT of predictions, amortizing a lot of computation. In particular,\n",
    "        for a batched input of integers X (B, T) where B is batch size and\n",
    "        T is block_size and Y (B, T), the network will during training be\n",
    "        simultaneously training to make B*T predictions, all at once! Of course,\n",
    "        at test time we can paralellize across batch B, but unlike during training\n",
    "        we cannot parallelize across the time dimension T - we have to run\n",
    "        a forward pass of the network to recover the next single character of the \n",
    "        sequence along each batch dimension, and repeatedly always feed in a next\n",
    "        character to get the next one.\n",
    "        \n",
    "        So yes there is a big asymmetry between train/test time of autoregressive\n",
    "        models. During training we can go B*T at a time with every forward pass,\n",
    "        but during test time we can only go B at a time, T times, with T forward \n",
    "        passes.\n",
    "        \"\"\"\n",
    "        inputs = torch.tensor(inputs, dtype=torch.long)\n",
    "        outputs = torch.tensor(outputs, dtype=torch.long)\n",
    "        return inputs, outputs, points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1432ac82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "def processDataFiles(path, files):\n",
    "    text = ''\"\"\n",
    "    for f in tqdm(files):\n",
    "        with open(path+f, 'r') as h: \n",
    "            lines = h.read() # don't worry we won't run out of file handles\n",
    "            if lines[-1]==-1:\n",
    "                lines = lines[:-1]\n",
    "            text += lines #json.loads(line)       \n",
    "            \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "075864e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 30/30 [00:34<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 300000 examples, 48 unique.\n"
     ]
    }
   ],
   "source": [
    "path = '{}/{}/Train/'.format(dataDir, dataFolder)\n",
    "files = os.listdir(path)[:maxNumFiles]\n",
    "text = processDataFiles(path, files)\n",
    "chars = sorted(list(set(text))+['_','T','<','>']) # extract unique characters from the text before converting the text to a list\n",
    "# T is for the test data\n",
    "text = text.split('\\n') # convert the raw text to a set of examples\n",
    "text = text[:-1] if len(text[-1]) == 0 else text\n",
    "random.shuffle(text) # shuffle the dataset, it's important for combined number of variables\n",
    "train_dataset = CharDataset(text, blockSize, chars, target=target) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a638018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:tensor([21, 43, 41, 42, 44,  3, 33, 34, 43,  3, 43, 36, 39,  3, 23,  5, 45, 14,\n",
      "         6, 23,  4,  4,  4, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,\n",
      "        32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,\n",
      "        32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,\n",
      "        32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,\n",
      "        32, 32, 32, 32, 32, 32, 32, 32, 32, 32])\n",
      "id:99695\n",
      "inputs:<sqrt(abs(sin(C*x4+C)))_____________________________________________________________________________\n",
      "outputs:sqrt(abs(sin(C*x4+C)))>_____________________________________________________________________________\n",
      "points:tensor([[1.0000, 1.0000, 1.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [1.0000, 1.0000, 1.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [1.0000, 1.0000, 1.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [1.0000, 0.9400, 1.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [1.0000, 1.0000, 0.2900,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.9600, 0.6000, 0.9500,  ..., 0.0000, 0.0000, 0.0000]])\n",
      "tensor(0.) tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(train_dataset.__len__())\n",
    "inputs, outputs, points = train_dataset.__getitem__(idx)\n",
    "print('inputs:{}'.format(inputs))\n",
    "inputs = ''.join([train_dataset.itos[int(i)] for i in inputs])\n",
    "outputs = ''.join([train_dataset.itos[int(i)] for i in outputs])\n",
    "print('id:{}\\ninputs:{}\\noutputs:{}\\npoints:{}'.format(idx,inputs,outputs,points))\n",
    "minimum, maximum = points.min(), points.max()\n",
    "print(minimum, maximum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6cafae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1001 examples, 48 unique.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "path = '{}/{}/Test/'.format(dataDir,dataFolder)\n",
    "files = os.listdir(path)[:maxNumFiles]\n",
    "textTest = processDataFiles(path, files)\n",
    "textTest = textTest.split('\\n') # convert the raw text to a set of examples\n",
    "# test_dataset_target = CharDataset(textTest, blockSize, chars, target=target)\n",
    "test_dataset = CharDataset(textTest, blockSize, chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6acbd34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.) tensor(1.)\n",
      "id:848\n",
      "inputs:<x3+0.65*sin(x5)____________________________________________________________________________________\n",
      "outputs:x3+0.65*sin(x5)>____________________________________________________________________________________\n",
      "points:tensor([[1.0000, 1.0000, 0.8900,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [1.0000, 0.2500, 1.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [1.0000, 1.0000, 1.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [1.0000, 1.0000, 1.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [1.0000, 1.0000, 1.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [1.0000, 1.0000, 1.0000,  ..., 0.0000, 0.0000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(test_dataset.__len__())\n",
    "inputs, outputs, points = test_dataset.__getitem__(idx)\n",
    "print(points.min(), points.max())\n",
    "inputs = ''.join([train_dataset.itos[int(i)] for i in inputs])\n",
    "outputs = ''.join([train_dataset.itos[int(i)] for i in outputs])\n",
    "print('id:{}\\ninputs:{}\\noutputs:{}\\npoints:{}'.format(idx,inputs,outputs,points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "965ef98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 300000/300000 [01:06<00:00, 4510.53it/s]\n"
     ]
    }
   ],
   "source": [
    "trainEquations = {}\n",
    "trainSamples = {}\n",
    "for dicTrain in tqdm(text):        \n",
    "        trainSamples[dicTrain] = 1\n",
    "        dicTrain = json.loads(dicTrain)\n",
    "        trainEquations[dicTrain['EQ']] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "48e3fd89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1001it [00:00, 2099.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175/938=0.1865671641791045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# for every equations in the test set, check if exist in the train set\n",
    "overlapEq = {}\n",
    "overlapSamples = {}\n",
    "testEquations = {}\n",
    "for i, dicTest in tqdm(enumerate(textTest)):\n",
    "    if dicTest == '':\n",
    "        continue\n",
    "    \n",
    "    if dicTest in trainSamples:\n",
    "        overlapSamples[dicTest] = 1\n",
    "    \n",
    "    dicTest = json.loads(dicTest)\n",
    "    # go over all training set and see if it has it or not\n",
    "    #print(\"---> {}: {}/{}\".format(i, len(overlap), len(textTest)))\n",
    "    \n",
    "    if dicTest['EQ'] in trainEquations:\n",
    "        overlapEq[dicTest['EQ']] = 1\n",
    "        \n",
    "    testEquations[dicTest['EQ']] = 1\n",
    "print('{}/{}={}'.format(len(overlap), len(testEquations), len(overlap)/len(testEquations)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c69726",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
