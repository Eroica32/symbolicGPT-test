{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logging\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make deterministic\n",
    "from mingpt.utils import set_seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "embeddingSize=512\n",
    "numPoints=100 # number of points that we are going to receive to make a prediction about f given x and y\n",
    "numVars=3 # the dimenstion of input points x\n",
    "numYs=1 # the dimension of output points y = f(x)\n",
    "blockSize = 60 # spatial extent of the model for its context\n",
    "batchSize = 256\n",
    "dataInfo = 'XYE_3Var_100Points'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, block_size, chars):\n",
    "        data_size, vocab_size = len(data), len(chars)\n",
    "        print('data has %d examples, %d unique.' % (data_size, vocab_size))\n",
    "        \n",
    "        self.stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "        self.itos = { i:ch for i,ch in enumerate(chars) }\n",
    "        \n",
    "        # padding token\n",
    "        self.paddingToken = '_'\n",
    "        self.paddingID = self.stoi[self.paddingToken]\n",
    "        self.stoi[self.paddingToken] = self.paddingID\n",
    "        self.itos[self.paddingID] = self.paddingToken\n",
    "        self.threshold = [-1000,1000]\n",
    "        \n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.data = data # it should be a list of examples\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)-1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # grab an example from the data\n",
    "        chunk = self.data[idx] # sequence of tokens including x, y, eq, etc.\n",
    "        chunk = json.loads(chunk) # convert the sequence tokens to a dictionary\n",
    "        \n",
    "        # encode every character in the equation to an integer\n",
    "        # < is SOS, > is EOS\n",
    "        dix = [self.stoi[s] for s in '<'+chunk['EQ']+'>']\n",
    "        inputs = dix[:-1]\n",
    "        outputs = dix[1:]\n",
    "        \n",
    "        # add the padding to the equations\n",
    "        paddingSize = max(self.block_size-len(inputs),0)\n",
    "        paddingList = [self.paddingID]*paddingSize\n",
    "        inputs += paddingList\n",
    "        outputs += paddingList \n",
    "        \n",
    "        # make sure it is not more than what should be\n",
    "        inputs = inputs[:self.block_size]\n",
    "        outputs = outputs[:self.block_size]\n",
    "        \n",
    "        # extract points from the input sequence\n",
    "        points = torch.zeros(numVars+numYs, numPoints)\n",
    "        for idx, xy in enumerate(zip(chunk['X'], chunk['Y'])):\n",
    "            x = xy[0] + [0]*(max(numVars-len(xy[0]),0)) # padding\n",
    "            y = [xy[1]] if type(xy[1])== float else xy[1]\n",
    "            y = y + [0]*(max(numYs-len(y),0)) # padding\n",
    "            p = x+y # because it is only one point \n",
    "            p = torch.tensor(p)\n",
    "            #replace nan and inf\n",
    "            p = torch.nan_to_num(p, nan=0.0, \n",
    "                                 posinf=self.threshold[1], \n",
    "                                 neginf=self.threshold[0])\n",
    "            p[p>self.threshold[1]] = self.threshold[1] # clip the upper bound\n",
    "            p[p<self.threshold[0]] = self.threshold[0] # clip the lower bound\n",
    "            points[:,idx] = p\n",
    "        \n",
    "        \"\"\"\n",
    "        arrange data and targets so that the first i elements of x\n",
    "        will be asked to predict the i-th element of y. Notice that\n",
    "        the eventual language model will actually make block_size\n",
    "        individual predictions at the same time based on this data,\n",
    "        so we are being clever and amortizing the cost of the forward\n",
    "        pass of the network. So for example if block_size is 4, then\n",
    "        we could e.g. sample a chunk of text \"hello\", the integers in\n",
    "        x will correspond to \"hell\" and in y will be \"ello\". This will\n",
    "        then actually \"multitask\" 4 separate examples at the same time\n",
    "        in the language model:\n",
    "        - given just \"h\", please predict \"e\" as next\n",
    "        - given \"he\" please predict \"l\" next\n",
    "        - given \"hel\" predict \"l\" next\n",
    "        - given \"hell\" predict \"o\" next\n",
    "        \n",
    "        In addition, because the DataLoader will create batches of examples,\n",
    "        every forward/backward pass during traning will simultaneously train\n",
    "        a LOT of predictions, amortizing a lot of computation. In particular,\n",
    "        for a batched input of integers X (B, T) where B is batch size and\n",
    "        T is block_size and Y (B, T), the network will during training be\n",
    "        simultaneously training to make B*T predictions, all at once! Of course,\n",
    "        at test time we can paralellize across batch B, but unlike during training\n",
    "        we cannot parallelize across the time dimension T - we have to run\n",
    "        a forward pass of the network to recover the next single character of the \n",
    "        sequence along each batch dimension, and repeatedly always feed in a next\n",
    "        character to get the next one.\n",
    "        \n",
    "        So yes there is a big asymmetry between train/test time of autoregressive\n",
    "        models. During training we can go B*T at a time with every forward pass,\n",
    "        but during test time we can only go B at a time, T times, with T forward \n",
    "        passes.\n",
    "        \"\"\"\n",
    "        inputs = torch.tensor(inputs, dtype=torch.long)\n",
    "        outputs = torch.tensor(outputs, dtype=torch.long)\n",
    "        return inputs, outputs, points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "def processDataFiles(files):\n",
    "    text = ''\"\"\n",
    "    for f in tqdm(files):\n",
    "        with open(f, 'r') as h: \n",
    "            lines = h.read() # don't worry we won't run out of file handles\n",
    "            text += lines #json.loads(line)                \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:09<00:00,  1.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1685219 examples, 51 unique.\n"
     ]
    }
   ],
   "source": [
    "#path = 'D:\\Datasets\\Symbolic Dataset\\Datasets\\Mesh_Simple_GPT2_Sorted\\TrainDatasetFixed\\*.json'\n",
    "path = 'D:/Datasets/Symbolic Dataset/Datasets/3Var_-3to3_3.1to10/Train/*.json'\n",
    "files = glob.glob(path)\n",
    "text = processDataFiles(files)\n",
    "chars = sorted(list(set(text))+['_','T','<','>']) # extract unique characters from the text before converting the text to a list\n",
    "# T is for the test data\n",
    "text = text.split('\\n') # convert the raw text to a set of examples\n",
    "train_dataset = CharDataset(text, blockSize, chars) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:tensor([22,  8, 15,  9, 18, 20,  5, 34, 47, 42,  3, 14,  9, 20, 17,  5, 47, 12,\n",
      "         4,  5, 45, 37, 40,  3, 12,  9, 16, 12,  5, 47, 12,  8, 12,  9, 19, 12,\n",
      "         4, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,\n",
      "        33, 33, 33, 33, 33, 33])\n",
      "id:121958\n",
      "inputs:<-4.79*exp(3.96*x1)*sin(1.51*x1-1.81)_______________________\n",
      "outputs:-4.79*exp(3.96*x1)*sin(1.51*x1-1.81)>_______________________\n",
      "points:tensor([[ 1.2500e+00, -2.0200e+00, -1.4400e+00, -1.3600e+00,  2.7000e-01,\n",
      "         -9.1000e-01, -1.3100e+00, -2.7000e+00,  1.9000e+00,  2.1900e+00,\n",
      "         -2.6700e+00,  1.4900e+00,  1.8900e+00,  3.1000e-01, -1.7000e-01,\n",
      "         -1.1700e+00,  2.4800e+00, -8.8000e-01, -2.5100e+00, -2.1500e+00,\n",
      "         -8.6000e-01, -1.4200e+00, -8.4000e-01, -1.0100e+00,  1.7400e+00,\n",
      "         -1.4800e+00,  2.4500e+00,  2.6300e+00, -1.2900e+00,  2.5700e+00,\n",
      "          2.3600e+00, -2.3200e+00, -8.6000e-01,  1.8500e+00, -1.8500e+00,\n",
      "          4.0000e-01, -6.4000e-01, -4.7000e-01, -1.3500e+00,  2.0100e+00,\n",
      "          4.5000e-01, -2.7000e+00,  1.7200e+00, -4.9000e-01,  1.9300e+00,\n",
      "         -1.4400e+00,  1.8400e+00,  2.8700e+00, -4.7000e-01,  7.0000e-01,\n",
      "         -5.0000e-01, -6.0000e-01,  5.7000e-01,  2.9900e+00,  9.8000e-01,\n",
      "          3.5000e-01,  1.2000e+00, -2.7700e+00,  1.3200e+00,  1.0000e+00,\n",
      "          1.2000e-01, -1.3800e+00,  2.5000e+00, -1.8300e+00,  2.0100e+00,\n",
      "          2.0800e+00,  2.6500e+00,  1.8900e+00, -1.7600e+00, -1.6900e+00,\n",
      "          1.4000e+00,  1.1300e+00, -1.2400e+00, -1.8300e+00, -2.1000e-01,\n",
      "         -2.4200e+00, -1.0100e+00,  1.6400e+00,  1.6900e+00,  2.9900e+00,\n",
      "         -9.6000e-01,  5.6000e-01,  5.6000e-01, -1.7500e+00,  2.3600e+00,\n",
      "          2.7600e+00, -1.2500e+00,  1.6900e+00,  2.2900e+00,  1.5100e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [-2.8300e+00,  2.2100e+00, -3.5000e-01,  1.5700e+00, -1.0000e-01,\n",
      "          5.1000e-01, -2.0000e+00,  1.7800e+00,  1.9400e+00, -1.6500e+00,\n",
      "          1.9000e-01, -1.7100e+00, -1.6400e+00, -6.1000e-01, -1.2500e+00,\n",
      "          2.1100e+00,  1.8800e+00, -2.7100e+00, -2.7400e+00,  1.4000e+00,\n",
      "          2.5600e+00, -3.5000e-01, -1.2000e+00, -2.8000e+00, -1.0600e+00,\n",
      "         -1.7200e+00, -4.7000e-01,  2.8000e-01, -3.2000e-01,  2.7000e+00,\n",
      "          2.0600e+00,  1.5000e-01, -2.0000e-01,  2.0100e+00,  3.6000e-01,\n",
      "          5.4000e-01, -1.5000e-01,  1.5400e+00, -7.4000e-01,  3.0000e-02,\n",
      "         -2.8400e+00, -2.6400e+00, -1.9100e+00,  1.1700e+00,  1.0300e+00,\n",
      "          3.0000e+00,  1.2200e+00, -2.5500e+00, -2.9200e+00, -6.0000e-01,\n",
      "          2.4100e+00, -1.2100e+00,  7.8000e-01, -1.4100e+00, -4.4000e-01,\n",
      "         -2.4000e+00,  1.5600e+00,  2.2100e+00,  1.0900e+00,  1.4600e+00,\n",
      "         -2.1900e+00,  1.2700e+00,  1.8500e+00,  6.2000e-01, -2.8100e+00,\n",
      "          2.0100e+00, -2.4500e+00,  1.5000e+00, -1.8200e+00,  2.0300e+00,\n",
      "         -1.8900e+00,  2.3400e+00,  2.8900e+00, -3.0000e+00,  1.8200e+00,\n",
      "         -1.2900e+00,  2.7600e+00, -3.1000e-01,  1.9400e+00,  1.4000e-01,\n",
      "         -2.3800e+00, -1.3800e+00, -2.5700e+00, -8.6000e-01, -3.5000e-01,\n",
      "          1.3500e+00,  2.2000e+00,  9.2000e-01,  1.4900e+00,  1.2700e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 1.9600e+00,  1.5000e-01, -2.9600e+00, -2.4000e-01,  1.4800e+00,\n",
      "         -2.2100e+00,  1.2700e+00, -2.8000e+00,  1.1200e+00, -7.8000e-01,\n",
      "          6.5000e-01,  2.7300e+00, -2.4200e+00,  2.0300e+00, -2.6400e+00,\n",
      "          8.1000e-01, -2.9000e+00, -7.7000e-01,  1.2200e+00,  2.7900e+00,\n",
      "          1.7700e+00, -1.7000e-01, -2.7200e+00,  2.1000e-01, -5.0000e-02,\n",
      "         -2.4700e+00, -1.1300e+00,  2.2700e+00, -2.7600e+00,  9.8000e-01,\n",
      "         -4.7000e-01, -4.5000e-01, -2.5100e+00, -3.1000e-01,  1.5100e+00,\n",
      "         -1.6900e+00, -2.2500e+00, -2.6700e+00,  2.4400e+00, -2.5400e+00,\n",
      "         -1.5300e+00, -1.3000e+00, -8.0000e-02, -6.6000e-01, -3.0000e-02,\n",
      "         -3.9000e-01,  2.0000e-02,  2.5400e+00, -1.2300e+00, -5.0000e-02,\n",
      "         -5.3000e-01, -2.6800e+00,  2.0400e+00,  9.0000e-01,  1.7900e+00,\n",
      "          2.3700e+00, -2.9000e+00,  1.4500e+00, -1.6400e+00,  1.1900e+00,\n",
      "         -4.0000e-01,  2.1800e+00, -1.1500e+00,  1.1200e+00, -3.0000e+00,\n",
      "         -1.7100e+00, -5.2000e-01, -4.2000e-01, -2.8600e+00,  1.9600e+00,\n",
      "          2.9800e+00,  1.8900e+00, -2.1400e+00,  3.0000e-01, -1.3900e+00,\n",
      "          1.6100e+00, -1.3500e+00,  2.9700e+00,  2.4200e+00, -2.5000e-01,\n",
      "         -3.5000e-01,  6.2000e-01,  7.2000e-01, -1.8000e+00, -1.3400e+00,\n",
      "         -2.0500e+00, -5.2000e-01,  2.4200e+00, -1.2600e+00,  2.9000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [-5.0330e+01, -0.0000e+00, -1.0000e-02, -1.0000e-02,  1.3770e+01,\n",
      "         -1.0000e-02, -2.0000e-02, -0.0000e+00, -1.0000e+03, -1.0000e+03,\n",
      "         -0.0000e+00, -7.4196e+02, -1.0000e+03,  1.5950e+01,  2.1400e+00,\n",
      "         -2.0000e-02, -1.0000e+03, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "          0.0000e+00, -1.0000e-02,  1.0000e-02, -2.0000e-02, -1.0000e+03,\n",
      "         -1.0000e-02, -1.0000e+03, -1.0000e+03, -2.0000e-02, -1.0000e+03,\n",
      "         -1.0000e+03, -0.0000e+00,  0.0000e+00, -1.0000e+03, -0.0000e+00,\n",
      "          2.1860e+01,  1.3000e-01,  4.3000e-01, -1.0000e-02, -1.0000e+03,\n",
      "          2.5810e+01, -0.0000e+00, -1.0000e+03,  3.8000e-01, -1.0000e+03,\n",
      "         -1.0000e-02, -1.0000e+03, -1.0000e+03,  4.3000e-01,  5.2640e+01,\n",
      "          3.6000e-01,  1.8000e-01,  3.7350e+01, -1.0000e+03,  7.6120e+01,\n",
      "          1.8390e+01,  6.6000e-01, -0.0000e+00, -1.6016e+02,  7.5170e+01,\n",
      "          7.6900e+00, -1.0000e-02, -1.0000e+03, -0.0000e+00, -1.0000e+03,\n",
      "         -1.0000e+03, -1.0000e+03, -1.0000e+03, -0.0000e+00, -1.0000e-02,\n",
      "         -3.6388e+02,  4.4960e+01, -2.0000e-02, -0.0000e+00,  1.7700e+00,\n",
      "         -0.0000e+00, -2.0000e-02, -1.0000e+03, -1.0000e+03, -1.0000e+03,\n",
      "         -1.0000e-02,  3.6280e+01,  3.6280e+01, -0.0000e+00, -1.0000e+03,\n",
      "         -1.0000e+03, -2.0000e-02, -1.0000e+03, -1.0000e+03, -8.5473e+02,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(train_dataset.__len__())\n",
    "inputs, outputs, points = train_dataset.__getitem__(idx)\n",
    "print('inputs:{}'.format(inputs))\n",
    "inputs = ''.join([train_dataset.itos[int(i)] for i in inputs])\n",
    "outputs = ''.join([train_dataset.itos[int(i)] for i in outputs])\n",
    "print('id:{}\\ninputs:{}\\noutputs:{}\\npoints:{}'.format(idx,inputs,outputs,points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 77.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1001 examples, 51 unique.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#path = 'D:\\Datasets\\Symbolic Dataset\\Datasets\\Mesh_Simple_GPT2_Sorted\\TestDataset\\*.json'\n",
    "path = 'D:/Datasets/Symbolic Dataset/Datasets/3Var_-3to3_3.1to10/Test/*.json'\n",
    "files = glob.glob(path)\n",
    "textTest = processDataFiles([files[0]])\n",
    "textTest = textTest.split('\\n') # convert the raw text to a set of examples\n",
    "test_dataset = CharDataset(textTest, blockSize, chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1000.) tensor(1000.)\n",
      "id:435\n",
      "inputs:<x1+4.82*x3-2.97*exp(3.76*x1)_______________________________\n",
      "outputs:x1+4.82*x3-2.97*exp(3.76*x1)>_______________________________\n",
      "points:tensor([[-1.7800e+00,  2.5700e+00, -9.2000e-01,  1.1800e+00,  1.3100e+00,\n",
      "         -1.0000e+00,  1.5700e+00, -1.4700e+00, -2.7200e+00,  2.8500e+00,\n",
      "          1.6400e+00,  2.8900e+00, -2.6000e-01,  1.4200e+00,  5.0000e-01,\n",
      "          2.3500e+00, -1.0100e+00, -9.1000e-01, -1.0600e+00, -2.1000e+00,\n",
      "         -1.5000e+00,  2.3600e+00,  1.0500e+00, -4.0000e-01,  1.0400e+00,\n",
      "         -1.3000e+00,  1.1000e+00, -2.0600e+00, -1.6300e+00, -6.5000e-01,\n",
      "          1.9000e+00,  1.4000e+00,  7.2000e-01,  3.2000e-01, -1.0500e+00,\n",
      "         -2.3200e+00,  2.2800e+00, -2.8000e+00,  8.1000e-01, -2.0000e-02,\n",
      "          2.7000e-01,  8.1000e-01, -6.8000e-01, -1.6800e+00,  7.2000e-01,\n",
      "         -4.3000e-01, -2.5500e+00, -5.2000e-01, -6.2000e-01,  2.6300e+00,\n",
      "          4.5000e-01,  7.3000e-01, -1.6700e+00, -2.5400e+00, -1.8000e-01,\n",
      "         -2.5500e+00, -1.2600e+00,  2.3100e+00, -4.2000e-01,  3.2000e-01,\n",
      "         -2.7400e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 2.8500e+00,  2.4900e+00, -1.0500e+00,  4.2000e-01, -9.2000e-01,\n",
      "         -2.5900e+00, -1.1200e+00, -2.3500e+00,  1.4000e+00, -6.6000e-01,\n",
      "          1.4800e+00, -2.4400e+00,  7.4000e-01, -1.0800e+00, -1.8700e+00,\n",
      "          1.4200e+00,  2.2600e+00, -9.8000e-01, -2.3000e+00, -1.5700e+00,\n",
      "          1.7100e+00, -2.8700e+00, -9.2000e-01, -2.3700e+00, -2.0400e+00,\n",
      "          1.1200e+00,  6.8000e-01, -3.8000e-01,  1.5000e-01, -2.0000e-01,\n",
      "         -2.6400e+00,  2.8100e+00,  1.3000e-01,  6.6000e-01,  1.9100e+00,\n",
      "          2.5400e+00,  9.8000e-01, -1.7100e+00,  2.0400e+00, -1.5600e+00,\n",
      "         -5.0000e-02,  2.5000e+00, -2.2200e+00, -1.1700e+00,  2.9700e+00,\n",
      "         -2.6000e-01, -2.5500e+00,  2.8400e+00,  1.8500e+00,  2.9200e+00,\n",
      "          4.1000e-01,  1.1100e+00,  1.2700e+00, -1.8500e+00,  2.5000e-01,\n",
      "          9.2000e-01,  4.0000e-01,  4.4000e-01,  1.2300e+00, -1.6900e+00,\n",
      "         -7.1000e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [-9.3000e-01,  3.5000e-01, -1.5900e+00, -1.6100e+00,  2.7300e+00,\n",
      "          2.9600e+00, -2.9000e+00, -1.4000e+00, -1.4400e+00,  2.6200e+00,\n",
      "         -2.6400e+00, -6.2000e-01, -5.1000e-01, -1.8500e+00,  1.7000e+00,\n",
      "          1.0300e+00,  2.7400e+00,  2.0500e+00,  0.0000e+00, -2.6600e+00,\n",
      "         -1.4600e+00,  1.7800e+00,  1.1000e+00, -5.4000e-01, -2.8600e+00,\n",
      "         -2.2000e+00,  1.5600e+00,  2.0900e+00,  7.0000e-02, -5.3000e-01,\n",
      "         -2.2400e+00,  1.2100e+00,  2.7600e+00, -4.1000e-01,  1.7100e+00,\n",
      "         -2.4500e+00, -9.2000e-01,  4.6000e-01,  8.0000e-01,  2.0100e+00,\n",
      "          1.3400e+00, -1.7300e+00, -1.0200e+00,  2.7900e+00, -1.5500e+00,\n",
      "         -4.0000e-01,  1.4100e+00,  2.7600e+00,  7.0000e-02,  2.0900e+00,\n",
      "         -1.0800e+00,  1.0100e+00, -4.9000e-01, -9.7000e-01, -1.8500e+00,\n",
      "         -1.7600e+00,  1.1600e+00,  1.8900e+00, -1.6700e+00, -8.0000e-02,\n",
      "         -1.1300e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [-1.0000e-02,  1.0000e+03, -3.6000e-01, -8.1218e+02,  1.0000e+03,\n",
      "          2.4000e-01, -1.0000e+03, -4.0000e-02, -0.0000e+00,  1.0000e+03,\n",
      "         -1.0000e+03, -1.0000e+03, -2.1400e+00, -1.0000e+03,  3.7650e+01,\n",
      "          1.0000e+03,  2.1000e-01,  2.0000e-01, -7.0000e-02, -1.0000e-02,\n",
      "         -4.0000e-02,  1.0000e+03,  1.7641e+02, -1.3300e+00, -7.8901e+02,\n",
      "         -1.1000e-01,  3.5563e+02,  0.0000e+00, -1.0000e-02, -5.3000e-01,\n",
      "         -1.0000e+03,  8.3015e+02,  1.6640e+02, -1.5440e+01,  8.0000e-02,\n",
      "         -0.0000e+00, -1.0000e+03, -0.0000e+00,  3.5860e+01,  6.2200e+00,\n",
      "          1.0400e+01, -2.2172e+02, -6.6000e-01,  2.0000e-02, -1.4630e+02,\n",
      "         -1.0600e+00,  0.0000e+00,  1.3900e+00, -3.2000e-01,  1.0000e+03,\n",
      "         -4.2070e+01,  4.1110e+01, -1.0000e-02, -0.0000e+00, -6.1300e+00,\n",
      "         -0.0000e+00,  1.0000e-02,  1.0000e+03, -2.3600e+00, -1.0130e+01,\n",
      "         -0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(test_dataset.__len__())\n",
    "inputs, outputs, points = train_dataset.__getitem__(idx)\n",
    "print(points.min(), points.max())\n",
    "inputs = ''.join([train_dataset.itos[int(i)] for i in inputs])\n",
    "outputs = ''.join([train_dataset.itos[int(i)] for i in outputs])\n",
    "print('id:{}\\ninputs:{}\\noutputs:{}\\npoints:{}'.format(idx,inputs,outputs,points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/20/2021 14:23:34 - INFO - mingpt.model -   number of parameters: 3.056334e+07\n"
     ]
    }
   ],
   "source": [
    "from mingpt.model import GPT, GPTConfig, PointNetConfig\n",
    "pconf = PointNetConfig(embeddingSize=embeddingSize, \n",
    "                       numberofPoints=numPoints, \n",
    "                       numberofVars=numVars, \n",
    "                       numberofYs=numYs)\n",
    "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size,\n",
    "                  n_layer=8, n_head=8, n_embd=embeddingSize, padding_idx=train_dataset.paddingID)\n",
    "model = GPT(mconf, pconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 123: train loss 1.63433. lr 5.998701e-04:   2%|▍                     | 124/6583 [02:13<1:55:50,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:tensor([22, 39, 41, 36,  3, 12,  8, 11,  9, 17, 16,  5, 47, 12,  4, 10, 13,  6,\n",
      "        11,  9, 17, 12, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,\n",
      "        33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,\n",
      "        33, 33, 33, 33, 33, 33], device='cuda:0')\n",
      "Logit:tensor([ 8, 41, 36,  3, 47,  9, 11,  9, 20,  5,  5, 47, 13,  6, 23, 13, 23, 12,\n",
      "         9, 20, 23, 23, 23, 23, 23, 23, 23, 20, 23, 23, 23, 23, 23, 23, 23, 23,\n",
      "        23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 40, 23, 23, 23, 23, 23,\n",
      "        40, 40, 40, 40, 40, 41], device='cuda:0')\n",
      "Input:<log(1-0.65*x1)/2+0.61______________________________________\n",
      "Logit:-og(x.0.9**x2+>2>1.9>>>>>>>9>>>>>>>>>>>>>>>>>>>>n>>>>>nnnnno\n",
      "Target:tensor([39, 41, 36,  3, 12,  8, 11,  9, 17, 16,  5, 47, 12,  4, 10, 13,  6, 11,\n",
      "         9, 17, 12, 23, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,\n",
      "        33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,\n",
      "        33, 33, 33, 33, 33, 33], device='cuda:0')\n",
      "Target:log(1-0.65*x1)/2+0.61>______________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 393: train loss 1.01493. lr 5.986795e-04:   6%|█▎                    | 394/6583 [07:01<1:48:30,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:tensor([22, 34, 47, 42,  3, 13,  9, 12, 14,  5, 45, 43, 44, 46,  3, 47, 13,  6,\n",
      "        11,  9, 12, 15,  4,  4, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,\n",
      "        33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,\n",
      "        33, 33, 33, 33, 33, 33], device='cuda:0')\n",
      "Logit:tensor([11, 47, 42,  3, 12,  9, 11,  5,  5, 45, 43, 44, 46,  3, 11, 13,  8, 11,\n",
      "         9, 11, 13,  4,  4, 23, 23, 34, 23, 23, 34, 23, 23, 23, 23, 23, 23,  6,\n",
      "        23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23,\n",
      "        23, 23, 23, 46, 12, 46], device='cuda:0')\n",
      "Input:<exp(2.13*sqrt(x2+0.14))____________________________________\n",
      "Logit:0xp(1.0**sqrt(02-0.02))>>e>>e>>>>>>+>>>>>>>>>>>>>>>>>>>>>t1t\n",
      "Target:tensor([34, 47, 42,  3, 13,  9, 12, 14,  5, 45, 43, 44, 46,  3, 47, 13,  6, 11,\n",
      "         9, 12, 15,  4,  4, 23, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,\n",
      "        33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,\n",
      "        33, 33, 33, 33, 33, 33], device='cuda:0')\n",
      "Target:exp(2.13*sqrt(x2+0.14))>____________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 2542: train loss 0.81672. lr 5.464640e-04:  39%|███████▋            | 2543/6583 [44:54<1:11:36,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:tensor([22, 11,  9, 19,  5, 45, 43, 44, 46,  3, 47, 12,  8, 11,  9, 20, 12,  4,\n",
      "        33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,\n",
      "        33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,\n",
      "        33, 33, 33, 33, 33, 33], device='cuda:0')\n",
      "Logit:tensor([45,  9, 18, 12,  3, 43, 44, 46,  3,  8, 12,  4, 11,  9, 18,  4,  4, 23,\n",
      "        23, 23, 42, 37, 42, 12, 42, 23, 23,  5, 23, 23,  4, 23,  4,  4,  4,  4,\n",
      "         4, 23,  4,  4, 42,  4,  4,  4,  4, 11, 13, 23, 23, 23,  4, 23, 23, 23,\n",
      "        23, 23, 23, 23, 23, 23], device='cuda:0')\n",
      "Input:<0.8*sqrt(x1-0.91)__________________________________________\n",
      "Logit:s.71(qrt(-1)0.7))>>>pip1p>>*>>)>)))))>))p))))02>>>)>>>>>>>>>\n",
      "Target:tensor([11,  9, 19,  5, 45, 43, 44, 46,  3, 47, 12,  8, 11,  9, 20, 12,  4, 23,\n",
      "        33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,\n",
      "        33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,\n",
      "        33, 33, 33, 33, 33, 33], device='cuda:0')\n",
      "Target:0.8*sqrt(x1-0.91)>__________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 2923: train loss 0.79284. lr 5.299166e-04:  44%|████████▉           | 2924/6583 [51:36<1:03:44,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:tensor([22, 39, 41, 36,  3, 11,  9, 20, 12,  5, 47, 13,  8, 15,  9, 14, 15,  4,\n",
      "         6, 11,  9, 11, 19,  5, 34, 47, 42,  3,  8, 14,  9, 18, 17,  5, 47, 14,\n",
      "         4, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,\n",
      "        33, 33, 33, 33, 33, 33], device='cuda:0')\n",
      "Logit:tensor([34, 41, 36,  3, 47,  9, 14, 18,  5, 47, 14,  8, 14,  9, 18, 18,  4,  6,\n",
      "        11,  9, 11, 13,  5, 34, 47, 42,  3,  8, 14,  9, 11, 18,  5, 47, 14,  4,\n",
      "        23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23,\n",
      "        23, 23, 23, 23, 23, 23], device='cuda:0')\n",
      "Input:<log(0.91*x2-4.34)+0.08*exp(-3.76*x3)_______________________\n",
      "Logit:eog(x.37*x3-3.77)+0.02*exp(-3.07*x3)>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Target:tensor([39, 41, 36,  3, 11,  9, 20, 12,  5, 47, 13,  8, 15,  9, 14, 15,  4,  6,\n",
      "        11,  9, 11, 19,  5, 34, 47, 42,  3,  8, 14,  9, 18, 17,  5, 47, 14,  4,\n",
      "        23, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,\n",
      "        33, 33, 33, 33, 33, 33], device='cuda:0')\n",
      "Target:log(0.91*x2-4.34)+0.08*exp(-3.76*x3)>_______________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 3516: train loss 0.81311. lr 5.004389e-04:  53%|██████████▋         | 3517/6583 [1:02:02<53:44,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:tensor([22, 12, 12, 16,  9, 15, 15,  5, 47, 12,  5, 47, 14,  5, 34, 47, 42,  3,\n",
      "        47, 12,  4,  8, 12,  9, 15, 16,  5, 47, 12,  8, 17,  9, 13, 20,  5, 47,\n",
      "        14,  6, 12,  9, 12, 13, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,\n",
      "        33, 33, 33, 33, 33, 33], device='cuda:0')\n",
      "Logit:tensor([ 8,  9,  9,  9, 20, 16,  5, 47, 12,  5, 47, 14,  8, 45, 47, 42,  3,  8,\n",
      "        12,  4,  8, 12,  9, 20, 20,  5, 47, 12,  8, 12,  9, 16, 20,  5, 47, 14,\n",
      "         8, 12,  9, 20, 13, 23, 23, 23, 23, 23,  9, 23, 23, 14, 23, 11, 12, 23,\n",
      "        23, 23,  8, 23, 23,  9], device='cuda:0')\n",
      "Input:<115.44*x1*x3*exp(x1)-1.45*x1-6.29*x3+1.12__________________\n",
      "Logit:-...95*x1*x3-sxp(-1)-1.99*x1-1.59*x3-1.92>>>>>.>>3>01>>>->>.\n",
      "Target:tensor([12, 12, 16,  9, 15, 15,  5, 47, 12,  5, 47, 14,  5, 34, 47, 42,  3, 47,\n",
      "        12,  4,  8, 12,  9, 15, 16,  5, 47, 12,  8, 17,  9, 13, 20,  5, 47, 14,\n",
      "         6, 12,  9, 12, 13, 23, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,\n",
      "        33, 33, 33, 33, 33, 33], device='cuda:0')\n",
      "Target:115.44*x1*x3*exp(x1)-1.45*x1-6.29*x3+1.12>__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 4100: train loss 0.76004. lr 4.674892e-04:  62%|████████████▍       | 4101/6583 [1:12:19<44:39,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:tensor([22,  8, 11,  9, 13,  5, 47, 13,  8, 11,  9, 19,  5, 47, 14,  6, 39, 41,\n",
      "        36,  3,  8, 11,  9, 15,  5, 47, 12,  8, 11,  9, 15, 14,  4,  8, 14,  9,\n",
      "        18, 17, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,\n",
      "        33, 33, 33, 33, 33, 33], device='cuda:0')\n",
      "Logit:tensor([47, 11,  9, 20, 19, 47, 12,  5, 11,  9, 15, 13, 47, 14,  8, 39, 41, 36,\n",
      "         3,  8, 47,  9, 15, 17, 47, 12,  8, 11,  9, 15,  4,  4,  8, 15,  9, 15,\n",
      "        23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23,  8,  8, 23,  8,  8,\n",
      "         8,  8,  8, 23,  8, 23], device='cuda:0')\n",
      "Input:<-0.2*x2-0.8*x3+log(-0.4*x1-0.43)-3.76______________________\n",
      "Logit:x0.98x1*0.42x3-log(-x.46x1-0.4))-4.4>>>>>>>>>>>>>-->----->->\n",
      "Target:tensor([ 8, 11,  9, 13,  5, 47, 13,  8, 11,  9, 19,  5, 47, 14,  6, 39, 41, 36,\n",
      "         3,  8, 11,  9, 15,  5, 47, 12,  8, 11,  9, 15, 14,  4,  8, 14,  9, 18,\n",
      "        17, 23, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,\n",
      "        33, 33, 33, 33, 33, 33], device='cuda:0')\n",
      "Target:-0.2*x2-0.8*x3+log(-0.4*x1-0.43)-3.76>______________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 4349: train loss 0.72937. lr 4.524134e-04:  66%|█████████████▏      | 4350/6583 [1:16:42<39:53,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:tensor([22,  8, 11,  9, 16, 13,  5, 47, 14,  6, 20,  9, 13, 15,  5, 45, 43, 44,\n",
      "        46,  3, 11,  9, 14, 15,  5, 47, 14,  6, 12,  4, 33, 33, 33, 33, 33, 33,\n",
      "        33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,\n",
      "        33, 33, 33, 33, 33, 33], device='cuda:0')\n",
      "Logit:tensor([34, 11,  9, 17, 19,  5, 47, 12,  6, 39,  9, 17, 19,  5, 45, 43, 44, 46,\n",
      "         3, 11,  9, 13,  5,  5, 47, 12,  6, 12,  4, 23, 23, 23, 14,  9, 11, 23,\n",
      "        23, 14, 23, 23, 23, 15, 18, 23, 23, 23, 23, 13, 23, 23, 23, 12, 23, 23,\n",
      "        23, 11, 43, 23, 23,  9], device='cuda:0')\n",
      "Input:<-0.52*x3+9.24*sqrt(0.34*x3+1)______________________________\n",
      "Logit:e0.68*x1+l.68*sqrt(0.2**x1+1)>>>3.0>>3>>>47>>>>2>>>1>>>0q>>.\n",
      "Target:tensor([ 8, 11,  9, 16, 13,  5, 47, 14,  6, 20,  9, 13, 15,  5, 45, 43, 44, 46,\n",
      "         3, 11,  9, 14, 15,  5, 47, 14,  6, 12,  4, 23, 33, 33, 33, 33, 33, 33,\n",
      "        33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,\n",
      "        33, 33, 33, 33, 33, 33], device='cuda:0')\n",
      "Target:-0.52*x3+9.24*sqrt(0.34*x3+1)>______________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 4496: train loss 0.76017. lr 4.432572e-04:  68%|█████████████▋      | 4497/6583 [1:19:17<36:12,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:tensor([22, 34, 47, 42,  3, 11,  9, 11, 13,  5, 47, 13,  4,  8, 45, 37, 40,  3,\n",
      "        13,  9, 19, 15,  5, 47, 13,  6, 11,  9, 11, 13,  4, 33, 33, 33, 33, 33,\n",
      "        33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,\n",
      "        33, 33, 33, 33, 33, 33], device='cuda:0')\n",
      "Logit:tensor([34, 47, 42,  3, 45,  9, 11, 13,  5, 47, 12,  4,  5, 45, 37, 40,  3, 15,\n",
      "         9, 20, 20,  5, 47, 12,  6, 15,  9, 20, 20,  4, 23, 23, 23, 23,  9,  9,\n",
      "        23, 13, 23,  9,  4, 23, 23, 23, 12, 23, 12, 23, 23, 12, 12, 12, 23, 12,\n",
      "        23, 23, 23, 23, 23,  4], device='cuda:0')\n",
      "Input:<exp(0.02*x2)-sin(2.84*x2+0.02)_____________________________\n",
      "Logit:exp(s.02*x1)*sin(4.99*x1+4.99)>>>>..>2>.)>>>1>1>>111>1>>>>>)\n",
      "Target:tensor([34, 47, 42,  3, 11,  9, 11, 13,  5, 47, 13,  4,  8, 45, 37, 40,  3, 13,\n",
      "         9, 19, 15,  5, 47, 13,  6, 11,  9, 11, 13,  4, 23, 33, 33, 33, 33, 33,\n",
      "        33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,\n",
      "        33, 33, 33, 33, 33, 33], device='cuda:0')\n",
      "Target:exp(0.02*x2)-sin(2.84*x2+0.02)>_____________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 5227: train loss 0.75586. lr 3.953364e-04:  79%|███████████████▉    | 5228/6583 [1:32:09<24:11,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:tensor([22, 45, 37, 40,  3, 11,  9, 13,  5, 34, 47, 42,  3,  8, 12,  9, 13, 20,\n",
      "         5, 47, 12,  4,  4, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,\n",
      "        33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,\n",
      "        33, 33, 33, 33, 33, 33], device='cuda:0')\n",
      "Logit:tensor([45, 37, 40,  3, 12,  9, 11,  5, 34, 47, 42,  3, 47, 12,  9, 12, 15,  5,\n",
      "        47, 12,  4,  4, 23, 23, 13, 12,  8, 12,  9, 12,  9, 15, 23,  5, 23, 23,\n",
      "        23, 23, 23, 15, 15, 47, 47, 47,  4, 15, 11, 11, 15, 15, 15, 23, 11, 23,\n",
      "        47, 23, 23, 23, 23, 47], device='cuda:0')\n",
      "Input:<sin(0.2*exp(-1.29*x1))_____________________________________\n",
      "Logit:sin(1.0*exp(x1.14*x1))>>21-1.1.4>*>>>>>44xxx)400444>0>x>>>>x\n",
      "Target:tensor([45, 37, 40,  3, 11,  9, 13,  5, 34, 47, 42,  3,  8, 12,  9, 13, 20,  5,\n",
      "        47, 12,  4,  4, 23, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,\n",
      "        33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,\n",
      "        33, 33, 33, 33, 33, 33], device='cuda:0')\n",
      "Target:sin(0.2*exp(-1.29*x1))>_____________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 6582: train loss 0.74567. lr 3.000239e-04: 100%|████████████████████| 6583/6583 [1:55:59<00:00,  1.06s/it]\n",
      "05/20/2021 16:19:38 - INFO - mingpt.trainer -   test loss: 0.921243\n",
      "05/20/2021 16:19:38 - INFO - mingpt.trainer -   saving ./SavedModels/bestModel/checkpoint.pt\n",
      "epoch 2 iter 591: train loss 0.70428. lr 2.577837e-04:   9%|█▉                    | 592/6583 [10:25<1:46:28,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:tensor([22, 47, 12,  6, 34, 47, 42,  3, 11,  9, 18, 16,  5, 47, 14,  4,  8, 13,\n",
      "         9, 18, 17, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,\n",
      "        33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,\n",
      "        33, 33, 33, 33, 33, 33], device='cuda:0')\n",
      "Logit:tensor([47, 12,  6, 13, 47, 42,  3, 47,  9, 20, 18,  5, 47, 14,  4,  8, 13,  9,\n",
      "        12, 12, 23, 11,  8, 11,  8,  8, 15,  8,  8, 11, 11,  8, 11,  8,  8,  8,\n",
      "        12,  8, 23, 47,  8, 47, 13, 47, 15,  8, 47, 15,  8, 47, 23, 23, 47,  8,\n",
      "        47, 23, 47,  8, 47, 47], device='cuda:0')\n",
      "Input:<x1+exp(0.75*x3)-2.76_______________________________________\n",
      "Logit:x1+2xp(x.97*x3)-2.11>0-0--4--00-0---1->x-x2x4-x4-x>>x-x>x-xx\n",
      "Target:tensor([47, 12,  6, 34, 47, 42,  3, 11,  9, 18, 16,  5, 47, 14,  4,  8, 13,  9,\n",
      "        18, 17, 23, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,\n",
      "        33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,\n",
      "        33, 33, 33, 33, 33, 33], device='cuda:0')\n",
      "Target:x1+exp(0.75*x3)-2.76>_______________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 2 iter 1720: train loss 0.69324. lr 1.802512e-04:  26%|█████▏              | 1721/6583 [30:16<1:25:20,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:tensor([22,  8, 45, 37, 40,  3, 13,  9, 17, 15,  5, 47, 12,  5,  5, 13,  8, 14,\n",
      "         9, 12, 19,  5, 47, 12,  4, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,\n",
      "        33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,\n",
      "        33, 33, 33, 33, 33, 33], device='cuda:0')\n",
      "Logit:tensor([45, 45, 37, 40,  3, 45,  9, 11,  5,  5, 47, 13,  5, 47, 13,  6, 12,  9,\n",
      "        11, 14,  5, 47, 12,  4, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23,\n",
      "        15, 23, 23, 23, 23, 23, 23, 15, 23, 15, 13, 23, 23, 23, 23, 23, 23, 23,\n",
      "        23, 23, 23, 23, 23, 23], device='cuda:0')\n",
      "Input:<-sin(2.64*x1**2-3.18*x1)___________________________________\n",
      "Logit:ssin(s.0**x2*x2+1.03*x1)>>>>>>>>>>>>4>>>>>>4>42>>>>>>>>>>>>>\n",
      "Target:tensor([ 8, 45, 37, 40,  3, 13,  9, 17, 15,  5, 47, 12,  5,  5, 13,  8, 14,  9,\n",
      "        12, 19,  5, 47, 12,  4, 23, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,\n",
      "        33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,\n",
      "        33, 33, 33, 33, 33, 33], device='cuda:0')\n",
      "Target:-sin(2.64*x1**2-3.18*x1)>___________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 2 iter 3123: train loss 0.63775. lr 9.651987e-05:  47%|█████████▍          | 3124/6583 [55:26<1:00:56,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:tensor([22, 39, 41, 36,  3, 14,  9, 13, 20,  8, 12,  9, 15, 20,  5, 47, 13,  4,\n",
      "        33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,\n",
      "        33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,\n",
      "        33, 33, 33, 33, 33, 33], device='cuda:0')\n",
      "Logit:tensor([39, 41, 36,  3, 47,  9, 11, 12,  8, 12,  9, 14, 12,  5, 47, 13,  4, 23,\n",
      "        39, 39, 11, 11, 11, 23, 23, 23, 23, 23, 11, 23, 23, 23, 23, 23, 11, 11,\n",
      "        23, 23, 23, 23, 23, 23, 15, 23, 23, 15, 23, 23, 15, 23, 23, 23, 23, 23,\n",
      "        23, 39, 23, 23, 23, 23], device='cuda:0')\n",
      "Input:<log(3.29-1.49*x2)__________________________________________\n",
      "Logit:log(x.01-1.31*x2)>ll000>>>>>0>>>>>00>>>>>>4>>4>>4>>>>>>l>>>>\n",
      "Target:tensor([39, 41, 36,  3, 14,  9, 13, 20,  8, 12,  9, 15, 20,  5, 47, 13,  4, 23,\n",
      "        33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,\n",
      "        33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,\n",
      "        33, 33, 33, 33, 33, 33], device='cuda:0')\n",
      "Target:log(3.29-1.49*x2)>__________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 2 iter 3221: train loss 0.68984. lr 9.142071e-05:  49%|██████████▊           | 3222/6583 [57:11<58:49,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:tensor([22, 12,  9, 11, 15,  5, 47, 12,  8, 13,  9, 18, 16,  5, 47, 13,  5, 47,\n",
      "        14,  8, 18,  9, 15, 13,  5, 47, 13,  8, 11,  9, 13, 19,  5, 47, 14,  8,\n",
      "        11,  9, 14, 12, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,\n",
      "        33, 33, 33, 33, 33, 33], device='cuda:0')\n",
      "Logit:tensor([ 8, 12, 17,  5,  5, 47, 12,  5, 12,  9, 14, 19,  5, 47, 13,  5, 47, 14,\n",
      "         8, 12,  9, 13, 19,  5, 47, 13,  8, 12,  9, 17, 19,  5, 47, 14,  8, 12,\n",
      "         9, 12, 19, 23, 17,  9, 11,  9,  9, 44,  9,  9,  9,  9,  9, 46, 44,  3,\n",
      "        47,  3, 46,  3,  9, 46], device='cuda:0')\n",
      "Input:<1.04*x1-2.75*x2*x3-7.42*x2-0.28*x3-0.31____________________\n",
      "Logit:-16**x1*1.38*x2*x3-1.28*x2-1.68*x3-1.18>6.0..r.....tr(x(t(.t\n",
      "Target:tensor([12,  9, 11, 15,  5, 47, 12,  8, 13,  9, 18, 16,  5, 47, 13,  5, 47, 14,\n",
      "         8, 18,  9, 15, 13,  5, 47, 13,  8, 11,  9, 13, 19,  5, 47, 14,  8, 11,\n",
      "         9, 14, 12, 23, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,\n",
      "        33, 33, 33, 33, 33, 33], device='cuda:0')\n",
      "Target:1.04*x1-2.75*x2*x3-7.42*x2-0.28*x3-0.31>____________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 2 iter 3745: train loss 0.64165. lr 6.615701e-05:  57%|███████████▍        | 3746/6583 [1:06:34<51:06,  1.08s/it]"
     ]
    }
   ],
   "source": [
    "from mingpt.trainer import Trainer, TrainerConfig\n",
    "\n",
    "# initialize a trainer instance and kick off training\n",
    "tconf = TrainerConfig(max_epochs=50, batch_size=batchSize, learning_rate=6e-4,\n",
    "                      lr_decay=True, warmup_tokens=512*20, final_tokens=2*len(train_dataset)*blockSize,\n",
    "                      num_workers=0, ckpt_path='./SavedModels/bestModel/checkpoint.pt')\n",
    "trainer = Trainer(model, train_dataset, test_dataset, tconf)\n",
    "\n",
    "try:\n",
    "    trainer.train()\n",
    "except KeyboardInterrupt:\n",
    "    print('KeyboardInterrupt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('./SavedModels/bestModel/checkpoint.pt'))\n",
    "# model = model.eval().to(trainer.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a safe wrapper for numpy math functions\n",
    "from numpy import *\n",
    "import numpy as np\n",
    "\n",
    "def divide(x, y):\n",
    "  x = np.nan_to_num(x)\n",
    "  y = np.nan_to_num(y)\n",
    "  return np.divide(x,y+1e-5)\n",
    "\n",
    "def sqrt(x):\n",
    "  x = np.nan_to_num(x)\n",
    "  return np.sqrt(np.abs(x)) \n",
    "\n",
    "# Mean square error\n",
    "def mse(y, y_hat):\n",
    "    y_hat = np.reshape(y_hat, [1, -1])[0]\n",
    "    y_gold = np.reshape(y, [1, -1])[0]\n",
    "    our_sum = 0\n",
    "    for i in range(len(y_gold)):\n",
    "        our_sum += (y_hat[i] - y_gold[i]) ** 2\n",
    "\n",
    "    return our_sum / len(y_gold)\n",
    "\n",
    "# Mean square error\n",
    "def relativeErr(y, y_hat):\n",
    "    y_hat = np.reshape(y_hat, [1, -1])[0]\n",
    "    y_gold = np.reshape(y, [1, -1])[0]\n",
    "    our_sum = 0\n",
    "    for i in range(len(y_gold)):\n",
    "        if y_gold[i] < 1: \n",
    "            # use regular MSE\n",
    "            our_sum += (y_hat[i] - y_gold[i]) ** 2\n",
    "        else:\n",
    "            # use relative MSE\n",
    "            our_sum += ((y_hat[i] - y_gold[i])/y_gold[i]) ** 2\n",
    "\n",
    "    return our_sum / len(y_gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fName = '{}_SymbolicGPT_{}_{}_{}.txt'.format(dataInfo, \n",
    "                                             'GPT_PT_Summation', \n",
    "                                             'Padding',\n",
    "                                             blockSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# alright, let's sample some character-level symbolic GPT\n",
    "from mingpt.utils import sample\n",
    "#from gp_model import Genetic_Model\n",
    "#from mlp_model import MLP_Model\n",
    "    \n",
    "loader = torch.utils.data.DataLoader(\n",
    "                                test_dataset, \n",
    "                                shuffle=False, \n",
    "                                pin_memory=True,\n",
    "                                batch_size=1,\n",
    "                                num_workers=0)\n",
    "\n",
    "testRange = [3.1,6.0]\n",
    "numTestPoints = 10\n",
    "#test = np.linspace(3.1,6.0,numTestPoints)\n",
    "\n",
    "# gpm = Genetic_Model(n_jobs=-1)\n",
    "# mlp = MLP_Model()\n",
    "\n",
    "resultDict = {}\n",
    "with open(fName, 'w', encoding=\"utf-8\") as o:\n",
    "    modelName = 'SymbolicGPT'\n",
    "    resultDict[fName] = {modelName:[]}\n",
    "    \n",
    "    for i, batch in enumerate(loader):\n",
    "        inputs,outputs,points = batch\n",
    "        \n",
    "        print('Test Case {}.'.format(i))\n",
    "        o.write('Test Case {}/{}.\\n'.format(i,len(textTest)))\n",
    "        \n",
    "        t = json.loads(textTest[i])\n",
    "        \n",
    "        inputs = inputs[:,0:1].to(trainer.device)\n",
    "        points = points.to(trainer.device)\n",
    "        outputsHat = sample(model, inputs, blockSize, points=points,\n",
    "                      temperature=1.0, sample=True, \n",
    "                      top_k=10)[0]\n",
    "            \n",
    "        # filter out predicted\n",
    "        target = ''.join([train_dataset.itos[int(i)] for i in outputs[0]])\n",
    "        predicted = ''.join([train_dataset.itos[int(i)] for i in outputsHat])\n",
    "        \n",
    "#         if i == 31:\n",
    "#             raise\n",
    "#         else:\n",
    "#             continue\n",
    "        \n",
    "        #print(target, predicted)\n",
    "        #raise\n",
    "\n",
    "        target = target.strip(train_dataset.paddingToken).split('>')\n",
    "        target = target[0] if len(target[0])>=1 else target[1]\n",
    "        target = target.strip('<').strip(\">\")\n",
    "        predicted = predicted.strip(train_dataset.paddingToken).split('>')\n",
    "        predicted = predicted[0] if len(predicted[0])>=1 else predicted[1]\n",
    "        predicted = predicted.strip('<').strip(\">\")\n",
    "       \n",
    "        o.write('{}\\n'.format(target))\n",
    "        \n",
    "        print('Target:{}\\nPredicted:{}'.format(target, predicted))\n",
    "        \n",
    "        Ys = [] #t['YT']\n",
    "        Yhats = []\n",
    "        for xs in t['XT']:\n",
    "            try:\n",
    "                eqTmp = target + '' # copy eq\n",
    "                eqTmp = eqTmp.replace(' ','')\n",
    "                eqTmp = eqTmp.replace('\\n','')\n",
    "                for i,x in enumerate(xs):\n",
    "                    # replace xi with the value in the eq\n",
    "                    eqTmp = eqTmp.replace('x{}'.format(i+1), str(x))\n",
    "                    if ',' in eqTmp:\n",
    "                        assert 'There is a , in the equation!'\n",
    "                YEval = eval(eqTmp)\n",
    "                YEval = 0 if np.isnan(YEval) else YEval\n",
    "                YEval = 100 if np.isinf(YEval) else YEval\n",
    "            except:\n",
    "                YEval = 100 #TODO: Maybe I have to punish the model for each wrong template not for each point\n",
    "            Ys.append(YEval)\n",
    "            try:\n",
    "                eqTmp = predicted + '' # copy eq\n",
    "                eqTmp = eqTmp.replace(' ','')\n",
    "                eqTmp = eqTmp.replace('\\n','')\n",
    "                for i,x in enumerate(xs):\n",
    "                    # replace xi with the value in the eq\n",
    "                    eqTmp = eqTmp.replace('x{}'.format(i+1), str(x))\n",
    "                    if ',' in eqTmp:\n",
    "                        assert 'There is a , in the equation!'\n",
    "                Yhat = eval(eqTmp)\n",
    "                Yhat = 0 if np.isnan(Yhat) else Yhat\n",
    "                Yhat = 100 if np.isinf(Yhat) else Yhat\n",
    "            except:\n",
    "                Yhat = 100\n",
    "            Yhats.append(Yhat)\n",
    "        mseErr = relativeErr(Ys,Yhats)\n",
    "        \n",
    "        if type(mseErr) is np.complex128:\n",
    "            mseErr = abs(mseErr.real)\n",
    "            \n",
    "        resultDict[fName][modelName].append(mseErr)\n",
    "        \n",
    "        o.write('{}:{}\\n{}\\n\\n'.format(modelName, \n",
    "                               mseErr,\n",
    "                               predicted))\n",
    "        \n",
    "        print('MSE:{}\\n'.format(mseErr))\n",
    "print('Avg MSE:{}'.format(np.mean(resultDict[fName][modelName])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the error frequency for model comparison\n",
    "from matplotlib import pyplot as plt\n",
    "num_eqns = len(resultDict[fName]['SymbolicGPT'])\n",
    "num_vars = pconf.numberofVars\n",
    "\n",
    "models = list(resultDict[fName].keys())\n",
    "lists_of_error_scores = [resultDict[fName][key] for key in models]\n",
    "linestyles = [\"-\",\"dashdot\",\"dotted\",\"--\"]\n",
    "\n",
    "eps = 0.00001\n",
    "y, x, _ = plt.hist([np.log([max(min(x+eps, 1e5),1e-5) for x in e]) for e in lists_of_error_scores],\n",
    "                   label=models,\n",
    "                   cumulative=True, \n",
    "                   histtype=\"step\", \n",
    "                   bins=2000, \n",
    "                   density=\"true\")\n",
    "y = np.expand_dims(y,0)\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for idx, m in enumerate(models): \n",
    "    plt.plot(x[:-1], \n",
    "           y[idx] * 100, \n",
    "           linestyle=linestyles[idx], \n",
    "           label=m)\n",
    "\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.title(\"{} equations of {} variables\".format(num_eqns, num_vars))\n",
    "plt.xlabel(\"Log of Mean Square Error\")\n",
    "plt.ylabel(\"Normalized Cumulative Frequency\")\n",
    "\n",
    "name = '{}.png'.format(fName)\n",
    "plt.savefig(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
