{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logging\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make deterministic\n",
    "from mingpt.utils import set_seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "numEpochs = 2 # number of epochs to train the GPT+PT model\n",
    "embeddingSize=512 # the hidden dimension of the representation of both GPT and PT\n",
    "numPoints=200 # number of points that we are going to receive to make a prediction about f given x and y, if you don't know then use the maximum\n",
    "numVars=2 # the dimenstion of input points x, if you don't know then use the maximum\n",
    "numYs=1 # the dimension of output points y = f(x), if you don't know then use the maximum\n",
    "blockSize = 100 # spatial extent of the model for its context\n",
    "batchSize = 128\n",
    "dataInfo = 'XYE_{}Var_{}Points'.format(numVars, numPoints)\n",
    "target = 'Skeleton' #'EQ'\n",
    "dataFolder = '2Var_RandSupport_FixedLength_0to3_3.1to6_200Points'\n",
    "addr = './SavedModels/bestModel/' # where to save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, block_size, chars, target='EQ'):\n",
    "        data_size, vocab_size = len(data), len(chars)\n",
    "        print('data has %d examples, %d unique.' % (data_size, vocab_size))\n",
    "        \n",
    "        self.stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "        self.itos = { i:ch for i,ch in enumerate(chars) }\n",
    "        \n",
    "        # padding token\n",
    "        self.paddingToken = '_'\n",
    "        self.paddingID = self.stoi[self.paddingToken]\n",
    "        self.stoi[self.paddingToken] = self.paddingID\n",
    "        self.itos[self.paddingID] = self.paddingToken\n",
    "        self.threshold = [-1000,1000]\n",
    "        \n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.data = data # it should be a list of examples\n",
    "        self.target = target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)-1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # grab an example from the data\n",
    "        chunk = self.data[idx] # sequence of tokens including x, y, eq, etc.\n",
    "        \n",
    "        try:\n",
    "            chunk = json.loads(chunk) # convert the sequence tokens to a dictionary\n",
    "        except:\n",
    "            print(\"Couldn't convert to json: {}\".format(chunk))\n",
    "            \n",
    "        # encode every character in the equation to an integer\n",
    "        # < is SOS, > is EOS\n",
    "        dix = [self.stoi[s] for s in '<'+chunk[self.target]+'>']\n",
    "        inputs = dix[:-1]\n",
    "        outputs = dix[1:]\n",
    "        \n",
    "        # add the padding to the equations\n",
    "        paddingSize = max(self.block_size-len(inputs),0)\n",
    "        paddingList = [self.paddingID]*paddingSize\n",
    "        inputs += paddingList\n",
    "        outputs += paddingList \n",
    "        \n",
    "        # make sure it is not more than what should be\n",
    "        inputs = inputs[:self.block_size]\n",
    "        outputs = outputs[:self.block_size]\n",
    "        \n",
    "        # extract points from the input sequence\n",
    "        points = torch.zeros(numVars+numYs, numPoints)\n",
    "        for idx, xy in enumerate(zip(chunk['X'], chunk['Y'])):\n",
    "            x = xy[0] + [0]*(max(numVars-len(xy[0]),0)) # padding\n",
    "            y = [xy[1]] if type(xy[1])== float else xy[1]\n",
    "            y = y + [0]*(max(numYs-len(y),0)) # padding\n",
    "            p = x+y # because it is only one point \n",
    "            p = torch.tensor(p)\n",
    "            #replace nan and inf\n",
    "            p = torch.nan_to_num(p, nan=0.0, \n",
    "                                 posinf=self.threshold[1], \n",
    "                                 neginf=self.threshold[0])\n",
    "            p[p>self.threshold[1]] = self.threshold[1] # clip the upper bound\n",
    "            p[p<self.threshold[0]] = self.threshold[0] # clip the lower bound\n",
    "            points[:,idx] = p\n",
    "        \n",
    "        \"\"\"\n",
    "        arrange data and targets so that the first i elements of x\n",
    "        will be asked to predict the i-th element of y. Notice that\n",
    "        the eventual language model will actually make block_size\n",
    "        individual predictions at the same time based on this data,\n",
    "        so we are being clever and amortizing the cost of the forward\n",
    "        pass of the network. So for example if block_size is 4, then\n",
    "        we could e.g. sample a chunk of text \"hello\", the integers in\n",
    "        x will correspond to \"hell\" and in y will be \"ello\". This will\n",
    "        then actually \"multitask\" 4 separate examples at the same time\n",
    "        in the language model:\n",
    "        - given just \"h\", please predict \"e\" as next\n",
    "        - given \"he\" please predict \"l\" next\n",
    "        - given \"hel\" predict \"l\" next\n",
    "        - given \"hell\" predict \"o\" next\n",
    "        \n",
    "        In addition, because the DataLoader will create batches of examples,\n",
    "        every forward/backward pass during traning will simultaneously train\n",
    "        a LOT of predictions, amortizing a lot of computation. In particular,\n",
    "        for a batched input of integers X (B, T) where B is batch size and\n",
    "        T is block_size and Y (B, T), the network will during training be\n",
    "        simultaneously training to make B*T predictions, all at once! Of course,\n",
    "        at test time we can paralellize across batch B, but unlike during training\n",
    "        we cannot parallelize across the time dimension T - we have to run\n",
    "        a forward pass of the network to recover the next single character of the \n",
    "        sequence along each batch dimension, and repeatedly always feed in a next\n",
    "        character to get the next one.\n",
    "        \n",
    "        So yes there is a big asymmetry between train/test time of autoregressive\n",
    "        models. During training we can go B*T at a time with every forward pass,\n",
    "        but during test time we can only go B at a time, T times, with T forward \n",
    "        passes.\n",
    "        \"\"\"\n",
    "        inputs = torch.tensor(inputs, dtype=torch.long)\n",
    "        outputs = torch.tensor(outputs, dtype=torch.long)\n",
    "        return inputs, outputs, points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "def processDataFiles(files):\n",
    "    text = ''\"\"\n",
    "    for f in tqdm(files):\n",
    "        with open(f, 'r') as h: \n",
    "            lines = h.read() # don't worry we won't run out of file handles\n",
    "            if lines[-1]==-1:\n",
    "                lines = lines[:-1]\n",
    "            text += lines #json.loads(line)       \n",
    "            \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:01<00:00,  5.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 110000 examples, 48 unique.\n"
     ]
    }
   ],
   "source": [
    "#path = 'D:\\Datasets\\Symbolic Dataset\\Datasets\\Mesh_Simple_GPT2_Sorted\\TrainDatasetFixed\\*.json'\n",
    "path = 'D:/Datasets/Symbolic Dataset/Datasets/{}/Train/*.json'.format(dataFolder)\n",
    "files = glob.glob(path)\n",
    "text = processDataFiles(files)\n",
    "chars = sorted(list(set(text))+['_','T','<','>']) # extract unique characters from the text before converting the text to a list\n",
    "# T is for the test data\n",
    "text = text.split('\\n') # convert the raw text to a set of examples\n",
    "text = text[:-1] if len(text[-1]) == 0 else text\n",
    "train_dataset = CharDataset(text, blockSize, chars, target=target) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:tensor([21, 43, 36, 39,  3, 12,  5, 23,  5, 45, 12,  6, 12,  5, 23,  4, 32, 32,\n",
      "        32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,\n",
      "        32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,\n",
      "        32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,\n",
      "        32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,\n",
      "        32, 32, 32, 32, 32, 32, 32, 32, 32, 32])\n",
      "id:15795\n",
      "inputs:<sin(2*C*x2+2*C)____________________________________________________________________________________\n",
      "outputs:sin(2*C*x2+2*C)>____________________________________________________________________________________\n",
      "points:tensor([[ 1.3200,  0.8300,  1.4200,  1.6500,  2.8100,  0.0600,  2.3100,  2.1000,\n",
      "          2.2800,  0.3000,  1.0200,  2.0800,  0.3800,  1.2700,  0.6700,  1.0000,\n",
      "          1.8900,  1.4800,  1.7200,  1.6200,  0.0000,  1.0800,  1.2200,  0.6600,\n",
      "          1.9600,  1.8200,  1.7500,  1.4000,  1.4500,  0.1500,  0.0500,  0.4500,\n",
      "          1.4000,  2.6300,  1.9000,  0.8500,  1.6700,  2.5200,  0.6800,  0.3700,\n",
      "          0.6200,  2.9300,  1.1500,  2.2100,  1.9700,  1.7600,  2.5600,  2.1400,\n",
      "          2.6800,  1.1600,  0.8800,  2.5000,  2.7700,  2.7000,  1.1000,  0.2900,\n",
      "          1.3400,  1.9300,  1.3600,  1.4900,  0.9800,  0.5600,  2.6600,  2.0200,\n",
      "          2.5900,  2.3900,  2.5100,  0.4700,  2.7300,  0.7400,  2.5800,  0.1100,\n",
      "          0.7600,  0.5700,  0.6400,  2.6100,  2.0400,  1.1800,  1.7700,  1.3200,\n",
      "          1.2900,  1.0500,  0.2200,  2.6400,  1.1700,  2.0800,  2.8900,  0.0300,\n",
      "          0.7800,  2.1600,  0.1900,  1.4700,  0.6100,  1.2000,  2.5300,  1.4100,\n",
      "          0.3300,  1.2400,  2.9000,  1.2500,  2.1800,  1.3800,  0.6500,  1.2700,\n",
      "          0.1700,  2.1900,  1.4100,  1.9700,  1.4700,  0.4600,  0.5100,  1.7900,\n",
      "          1.1900,  0.7500,  0.7500,  0.7600,  0.5200,  0.5200,  0.5300,  0.1700,\n",
      "          0.7700,  1.5700,  0.7200,  2.8200,  1.7200,  0.5600,  1.7300,  2.6600,\n",
      "          1.5200,  0.7600,  2.2700,  0.6200,  0.8800,  1.5600,  0.0300,  0.8300,\n",
      "          0.6700,  1.7800,  1.3400,  2.3900,  1.2700,  0.7700,  2.7900,  2.9400,\n",
      "          2.8800,  2.9400,  1.1000,  0.4500,  1.0300,  1.0300,  2.9600,  0.6400,\n",
      "          0.6100,  2.7200,  1.6000,  2.0500,  1.0800,  2.5000,  1.4800,  1.6700,\n",
      "          0.4800,  2.9000,  1.7200,  2.8900,  1.3700,  1.6500,  1.4300,  2.8700,\n",
      "          0.6500,  2.6300,  0.4200,  0.9300,  0.0600,  2.0400,  2.4100,  0.3100,\n",
      "          0.1200,  2.8300,  1.8800,  2.0900,  2.8200,  0.6000,  0.3100,  0.5700,\n",
      "          0.3900,  0.6500,  0.4200,  2.4900,  1.6100,  1.2000,  1.6800,  0.2100,\n",
      "          2.6100,  1.1100,  1.7000,  2.5000,  2.0700,  0.2600,  1.3200,  1.2600],\n",
      "        [ 2.7300,  0.3200,  1.2500,  0.7800,  0.5700,  0.6700,  2.6900,  0.8300,\n",
      "          1.7100,  0.8300,  1.9500,  1.3400,  0.7400,  0.6700,  0.4700,  1.9200,\n",
      "          1.8400,  0.7300,  2.9300,  2.4100,  0.3600,  1.5400,  1.3000,  0.2900,\n",
      "          1.7900,  0.4700,  2.6500,  1.2100,  1.0600,  0.2700,  2.1200,  2.4000,\n",
      "          0.3000,  0.3100,  2.2700,  1.9800,  1.6000,  0.8900,  1.9900,  0.1800,\n",
      "          0.4500,  1.7200,  1.1000,  0.7600,  1.4800,  0.5400,  1.0800,  0.9200,\n",
      "          0.9900,  2.5900,  2.1800,  0.7400,  1.0900,  1.2900,  0.5400,  2.7600,\n",
      "          2.7300,  1.1300,  2.5900,  2.7600,  0.9100,  2.5200,  1.0400,  0.6200,\n",
      "          0.9800,  2.7700,  2.3400,  0.5600,  1.8900,  0.3700,  2.0800,  1.2200,\n",
      "          2.4300,  1.6500,  0.6900,  1.2600,  0.6700,  0.5600,  0.2500,  1.4700,\n",
      "          1.9800,  2.4300,  1.9800,  0.1200,  0.6400,  1.4600,  2.5200,  1.4200,\n",
      "          0.5200,  0.3600,  2.1500,  2.7500,  2.0200,  1.7400,  2.6300,  1.7700,\n",
      "          2.5600,  1.0100,  0.6800,  2.8900,  0.9500,  2.0200,  0.3100,  2.2800,\n",
      "          1.0700,  1.8600,  0.9600,  2.6400,  2.7600,  2.0900,  0.4000,  2.1300,\n",
      "          1.1300,  2.5600,  2.7700,  1.2000,  0.3600,  2.5500,  2.9900,  2.9900,\n",
      "          0.7800,  0.9000,  2.0700,  0.1400,  0.6500,  1.8800,  2.7500,  1.8500,\n",
      "          2.4200,  1.7000,  1.9800,  0.5000,  0.1700,  0.9900,  0.4500,  1.5600,\n",
      "          2.5000,  2.4300,  0.4000,  1.4300,  2.4000,  1.6100,  2.4200,  1.1800,\n",
      "          2.8600,  2.5000,  2.5500,  1.1400,  1.2900,  2.5800,  1.1000,  0.4800,\n",
      "          0.5000,  1.6800,  0.1600,  0.1100,  1.6400,  1.3800,  2.0100,  1.4100,\n",
      "          0.6100,  2.5500,  2.8500,  1.7200,  2.2900,  2.4900,  1.1200,  0.2200,\n",
      "          1.2700,  1.0500,  2.5900,  2.6800,  0.0100,  0.9700,  0.2700,  0.2500,\n",
      "          0.7600,  0.8700,  1.2100,  2.6000,  0.3400,  1.4600,  1.0300,  2.0000,\n",
      "          0.9200,  1.5300,  0.2000,  1.2800,  0.1000,  2.1500,  0.6700,  1.9300,\n",
      "          2.7000,  1.4700,  2.5900,  1.9600,  0.1100,  2.4500,  0.0900,  2.5800],\n",
      "        [-0.6600, -0.3000,  0.9600,  0.4700,  0.1200,  0.2900, -0.6100,  0.5400,\n",
      "          0.8600,  0.5400,  0.5900,  0.9900,  0.4100,  0.2900, -0.0500,  0.6300,\n",
      "          0.7300,  0.3900, -0.8800, -0.1700, -0.2400,  0.9700,  0.9800, -0.3500,\n",
      "          0.7900, -0.0500, -0.5500,  0.9400,  0.8300, -0.3900,  0.3300, -0.1500,\n",
      "         -0.3400, -0.3200,  0.0700,  0.5400,  0.9400,  0.6300,  0.5300, -0.5200,\n",
      "         -0.0800,  0.8500,  0.8600,  0.4400,  0.9900,  0.0700,  0.8500,  0.6700,\n",
      "          0.7500, -0.4600,  0.2300,  0.4100,  0.8500,  0.9800,  0.0700, -0.7000,\n",
      "         -0.6600,  0.8900, -0.4600, -0.7000,  0.6500, -0.3500,  0.8100,  0.2100,\n",
      "          0.7400, -0.7100, -0.0500,  0.1100,  0.6700, -0.2200,  0.3900,  0.9500,\n",
      "         -0.2000,  0.9100,  0.3300,  0.9700,  0.2900,  0.1100, -0.4200,  0.9900,\n",
      "          0.5400, -0.2000,  0.5400, -0.6100,  0.2400,  1.0000, -0.3500,  1.0000,\n",
      "          0.0400, -0.2400,  0.2800, -0.6900,  0.4800,  0.8400, -0.5200,  0.8100,\n",
      "         -0.4200,  0.7700,  0.3100, -0.8400,  0.7100,  0.4800, -0.3200,  0.0600,\n",
      "          0.8400,  0.7100,  0.7200, -0.5400, -0.7000,  0.3700, -0.1700,  0.3100,\n",
      "          0.8900, -0.4200, -0.7100,  0.9400, -0.2400, -0.4000, -0.9200, -0.9200,\n",
      "          0.4700,  0.6400,  0.4100, -0.5800,  0.2600,  0.6800, -0.6900,  0.7200,\n",
      "         -0.1900,  0.8700,  0.5400,  0.0000, -0.5400,  0.7500, -0.0800,  0.9600,\n",
      "         -0.3200, -0.2000, -0.1700,  1.0000, -0.1500,  0.9400, -0.1900,  0.9300,\n",
      "         -0.8100, -0.3200, -0.4000,  0.9000,  0.9800, -0.4500,  0.8600, -0.0300,\n",
      "          0.0000,  0.8900, -0.5500, -0.6200,  0.9200,  1.0000,  0.5000,  1.0000,\n",
      "          0.1900, -0.4000, -0.8000,  0.8500,  0.0400, -0.3000,  0.8800, -0.4600,\n",
      "          0.9700,  0.8200, -0.4600, -0.6000, -0.7500,  0.7300, -0.3900, -0.4200,\n",
      "          0.4400,  0.6000,  0.9400, -0.4800, -0.2700,  1.0000,  0.8000,  0.5100,\n",
      "          0.6700,  0.9800, -0.4900,  0.9800, -0.6400,  0.2800,  0.2900,  0.6100,\n",
      "         -0.6200,  0.9900, -0.4600,  0.5700, -0.6200, -0.2400, -0.6500, -0.4500]])\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(train_dataset.__len__())\n",
    "inputs, outputs, points = train_dataset.__getitem__(idx)\n",
    "print('inputs:{}'.format(inputs))\n",
    "inputs = ''.join([train_dataset.itos[int(i)] for i in inputs])\n",
    "outputs = ''.join([train_dataset.itos[int(i)] for i in outputs])\n",
    "print('id:{}\\ninputs:{}\\noutputs:{}\\npoints:{}'.format(idx,inputs,outputs,points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 41.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1001 examples, 48 unique.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#path = 'D:\\Datasets\\Symbolic Dataset\\Datasets\\Mesh_Simple_GPT2_Sorted\\TestDataset\\*.json'\n",
    "path = 'D:/Datasets/Symbolic Dataset/Datasets/{}/Val/*.json'.format(dataFolder)\n",
    "files = glob.glob(path)\n",
    "textVal = processDataFiles([files[0]])\n",
    "textVal = textVal.split('\\n') # convert the raw text to a set of examples\n",
    "val_dataset = CharDataset(textVal, blockSize, chars, target=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0100) tensor(2.9800)\n",
      "id:860\n",
      "inputs:<sqrt(abs(C*x2+C))__________________________________________________________________________________\n",
      "outputs:sqrt(abs(C*x2+C))>__________________________________________________________________________________\n",
      "points:tensor([[2.5900, 2.4500, 2.0200, 2.0500, 1.6900, 2.6400, 0.2800, 1.7900, 1.3300,\n",
      "         1.0000, 2.1700, 2.4700, 2.5800, 2.0600, 2.6100, 0.5400, 0.4200, 2.2500,\n",
      "         2.4600, 0.2700, 1.1300, 0.1700, 1.4700, 0.4300, 0.3300, 0.0800, 0.4500,\n",
      "         2.6000, 2.2800, 1.6200, 1.7300, 0.8400, 1.6200, 2.2900, 0.9600, 0.6400,\n",
      "         1.7800, 1.1200, 0.6400, 0.7900, 2.8800, 1.9200, 1.8200, 0.6100, 0.3500,\n",
      "         1.9000, 0.4100, 0.3700, 2.2000, 2.4700, 1.8000, 1.6200, 1.6900, 1.3200,\n",
      "         0.5000, 1.3100, 2.3000, 0.6500, 2.9600, 2.9800, 2.7800, 0.7500, 2.1800,\n",
      "         0.1700, 2.0000, 0.3400, 1.1600, 1.8500, 1.4000, 0.5500, 2.6200, 2.1500,\n",
      "         1.3600, 2.6700, 2.1200, 1.5000, 1.5400, 2.1200, 0.8300, 0.2100, 0.5800,\n",
      "         0.2300, 1.5500, 2.1800, 0.9800, 0.7300, 1.1200, 1.1700, 0.1700, 0.4900,\n",
      "         0.8600, 1.3900, 0.4100, 0.9400, 1.8000, 1.2800, 0.8300, 0.6900, 0.0300,\n",
      "         0.3800, 2.3800, 2.8400, 1.6300, 2.6500, 0.9500, 2.0500, 2.4200, 2.0400,\n",
      "         1.0800, 1.1700, 1.5000, 2.9800, 2.6000, 0.8200, 0.0900, 2.7100, 0.0200,\n",
      "         2.9800, 0.3000, 1.5700, 0.0500, 2.8400, 0.5100, 0.9500, 0.7200, 2.8200,\n",
      "         0.6300, 1.3300, 1.1700, 0.9400, 0.3500, 0.6200, 0.0700, 2.2900, 1.2800,\n",
      "         2.2600, 2.8100, 1.8600, 0.3300, 0.2400, 0.9600, 2.5600, 2.2400, 2.5700,\n",
      "         2.7000, 0.1100, 0.2200, 0.0200, 0.7700, 0.1100, 1.5700, 2.4700, 0.5000,\n",
      "         2.2500, 2.0900, 0.5700, 0.4200, 1.2800, 0.2800, 1.1400, 2.6700, 2.9600,\n",
      "         1.8400, 2.1200, 1.0400, 2.0800, 2.7800, 0.2800, 2.3500, 0.3500, 1.4200,\n",
      "         1.8900, 2.9200, 0.5600, 0.3900, 1.5900, 0.8500, 1.8700, 0.4900, 1.5800,\n",
      "         1.1100, 2.0500, 2.5900, 2.7700, 0.0700, 0.3400, 1.6500, 2.8300, 0.3000,\n",
      "         2.5900, 1.8500, 0.3500, 0.8100, 1.6100, 0.1800, 1.1300, 0.0100, 0.4200,\n",
      "         1.0700, 1.6900],\n",
      "        [1.9900, 1.9100, 0.6700, 1.4400, 2.3200, 2.6400, 1.6600, 0.7900, 0.4500,\n",
      "         1.2500, 1.4800, 2.2400, 0.2500, 2.3100, 2.7500, 1.2600, 2.0100, 2.0000,\n",
      "         0.7700, 1.0100, 0.9000, 1.0100, 1.5200, 0.9800, 2.3100, 2.9700, 2.1300,\n",
      "         2.4300, 1.3400, 0.6800, 0.7000, 0.4700, 0.5800, 0.2000, 2.5200, 2.0700,\n",
      "         0.4600, 1.5700, 0.9300, 0.6600, 1.2100, 0.6300, 1.7100, 2.8900, 1.4800,\n",
      "         2.4800, 0.2100, 1.6700, 1.0600, 0.5100, 0.5700, 1.1100, 0.6800, 2.3400,\n",
      "         0.3900, 1.9800, 1.3400, 1.9500, 0.8400, 2.2900, 2.8600, 2.3200, 0.6800,\n",
      "         1.5600, 2.6500, 1.1200, 2.5700, 2.5000, 1.0100, 0.5200, 1.6100, 1.0800,\n",
      "         2.3900, 2.8600, 1.2100, 0.3200, 2.1900, 2.7500, 1.9700, 1.6000, 2.6900,\n",
      "         0.6900, 1.4800, 2.8400, 2.8100, 0.5900, 2.2900, 1.7400, 2.7300, 0.2300,\n",
      "         0.6300, 0.9200, 1.0600, 1.1500, 0.1700, 1.9200, 1.8000, 0.9000, 2.8100,\n",
      "         2.2400, 0.5500, 1.5800, 2.3700, 0.3300, 1.4200, 1.1400, 2.7200, 0.2600,\n",
      "         0.9600, 1.9400, 0.6200, 1.7900, 1.5100, 1.9000, 0.6400, 2.2100, 1.9600,\n",
      "         2.1100, 2.9300, 1.2400, 0.7500, 1.1300, 0.0400, 2.3000, 0.3300, 0.8300,\n",
      "         1.7400, 2.0500, 2.1100, 1.4500, 2.4200, 0.5900, 2.2900, 2.6400, 2.7500,\n",
      "         0.3500, 2.5600, 2.0900, 0.6400, 0.2400, 2.2600, 0.1300, 2.0700, 1.5900,\n",
      "         0.6900, 2.6000, 2.5200, 1.4200, 2.6100, 2.9700, 1.0600, 1.2500, 0.3500,\n",
      "         2.7000, 1.9500, 1.7100, 2.1600, 1.9200, 2.3600, 0.3200, 2.1300, 0.1300,\n",
      "         0.1800, 0.4100, 2.4800, 2.4100, 1.8800, 1.1300, 1.1900, 1.7200, 0.4600,\n",
      "         2.8400, 2.6600, 2.4000, 0.5500, 1.4100, 1.4300, 1.1200, 1.7200, 1.9600,\n",
      "         0.5100, 0.2900, 0.1600, 1.0100, 0.3300, 1.8400, 1.3000, 2.6800, 1.7600,\n",
      "         2.0100, 1.2800, 0.0900, 1.1300, 0.6100, 1.7200, 0.1500, 2.9800, 1.9000,\n",
      "         2.4300, 0.9800],\n",
      "        [0.3000, 0.3500, 0.7900, 0.5600, 0.2100, 0.4200, 0.4800, 0.7600, 0.8500,\n",
      "         0.6300, 0.5500, 0.1100, 0.9000, 0.2000, 0.4700, 0.6200, 0.2900, 0.2900,\n",
      "         0.7700, 0.7000, 0.7300, 0.7000, 0.5300, 0.7100, 0.2000, 0.5600, 0.1800,\n",
      "         0.3000, 0.6000, 0.7900, 0.7900, 0.8400, 0.8200, 0.9100, 0.3600, 0.2400,\n",
      "         0.8500, 0.5100, 0.7200, 0.8000, 0.6400, 0.8000, 0.4500, 0.5300, 0.5500,\n",
      "         0.3300, 0.9100, 0.4700, 0.6900, 0.8300, 0.8200, 0.6700, 0.7900, 0.2300,\n",
      "         0.8600, 0.3100, 0.6000, 0.3300, 0.7500, 0.1800, 0.5200, 0.2100, 0.7900,\n",
      "         0.5200, 0.4200, 0.6700, 0.3800, 0.3400, 0.7000, 0.8300, 0.5000, 0.6800,\n",
      "         0.2700, 0.5200, 0.6400, 0.8800, 0.0900, 0.4700, 0.3100, 0.5000, 0.4400,\n",
      "         0.7900, 0.5500, 0.5100, 0.5000, 0.8100, 0.1800, 0.4400, 0.4600, 0.9000,\n",
      "         0.8000, 0.7300, 0.6900, 0.6600, 0.9100, 0.3500, 0.4100, 0.7300, 0.5000,\n",
      "         0.1100, 0.8200, 0.5100, 0.2600, 0.8800, 0.5700, 0.6600, 0.4600, 0.8900,\n",
      "         0.7200, 0.3300, 0.8100, 0.4200, 0.5400, 0.3600, 0.8000, 0.0200, 0.3200,\n",
      "         0.2000, 0.5400, 0.6300, 0.7700, 0.6700, 0.9400, 0.1900, 0.8800, 0.7500,\n",
      "         0.4400, 0.2600, 0.2000, 0.5600, 0.2900, 0.8100, 0.1800, 0.4200, 0.4700,\n",
      "         0.8700, 0.3800, 0.2200, 0.8000, 0.9000, 0.1400, 0.9200, 0.2400, 0.5000,\n",
      "         0.7900, 0.4000, 0.3600, 0.5700, 0.4000, 0.5600, 0.6900, 0.6300, 0.8700,\n",
      "         0.4500, 0.3300, 0.4500, 0.1400, 0.3500, 0.2500, 0.8800, 0.1800, 0.9200,\n",
      "         0.9100, 0.8600, 0.3300, 0.2900, 0.3700, 0.6700, 0.6500, 0.4500, 0.8500,\n",
      "         0.5100, 0.4300, 0.2800, 0.8200, 0.5700, 0.5700, 0.6700, 0.4500, 0.3200,\n",
      "         0.8300, 0.8900, 0.9200, 0.7000, 0.8800, 0.3900, 0.6100, 0.4400, 0.4300,\n",
      "         0.2900, 0.6200, 0.9300, 0.6700, 0.8100, 0.4500, 0.9200, 0.5600, 0.3600,\n",
      "         0.3000, 0.7100]])\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(val_dataset.__len__())\n",
    "inputs, outputs, points = val_dataset.__getitem__(idx)\n",
    "print(points.min(), points.max())\n",
    "inputs = ''.join([train_dataset.itos[int(i)] for i in inputs])\n",
    "outputs = ''.join([train_dataset.itos[int(i)] for i in outputs])\n",
    "print('id:{}\\ninputs:{}\\noutputs:{}\\npoints:{}'.format(idx,inputs,outputs,points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 41.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1001 examples, 48 unique.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#path = 'D:\\Datasets\\Symbolic Dataset\\Datasets\\Mesh_Simple_GPT2_Sorted\\TestDataset\\*.json'\n",
    "path = 'D:/Datasets/Symbolic Dataset/Datasets/{}/Test/*.json'.format(dataFolder)\n",
    "files = glob.glob(path)\n",
    "textTest = processDataFiles([files[0]])\n",
    "textTest = textTest.split('\\n') # convert the raw text to a set of examples\n",
    "# test_dataset_target = CharDataset(textTest, blockSize, chars, target=target)\n",
    "test_dataset = CharDataset(textTest, blockSize, chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.) tensor(2.9800)\n",
      "id:270\n",
      "inputs:<-sin(0.7*x1)_______________________________________________________________________________________\n",
      "outputs:-sin(0.7*x1)>_______________________________________________________________________________________\n",
      "points:tensor([[ 1.0500,  1.0800,  0.0600,  1.8300,  2.0500,  0.5100,  2.5500,  2.6800,\n",
      "          0.0600,  0.9100,  1.6500,  2.9800,  1.1000,  0.9800,  2.3100,  2.2500,\n",
      "          2.2800,  0.6400,  1.2300,  2.9600,  0.8300,  2.1900,  1.2500,  0.5600,\n",
      "          0.1100,  1.1100,  1.4400,  2.7600,  2.2500,  1.1700,  2.6300,  1.3700,\n",
      "          2.0000,  0.7200,  1.8700,  1.4300,  1.2700,  1.0000,  1.2800,  1.2700,\n",
      "          1.7400,  1.6000,  0.9400,  2.1400,  2.8100,  2.9700,  1.2600,  0.9000,\n",
      "          2.6700,  0.2600,  0.6900,  0.6700,  2.6900,  1.7300,  0.1900,  2.7500,\n",
      "          1.6100,  1.7200,  1.3900,  2.7800,  2.7000,  0.4300,  2.1200,  2.2600,\n",
      "          0.6500,  1.4200,  2.4000,  2.0900,  1.1800,  0.7400,  1.7400,  1.1200,\n",
      "          0.4700,  2.6000,  0.4900,  2.9100,  0.2100,  0.2000,  2.8100,  1.7600,\n",
      "          2.5500,  1.3400,  0.2500,  2.3500,  1.3900,  0.4600,  1.0600,  1.3100,\n",
      "          1.5600,  1.2800,  1.6300,  2.6500,  2.1600,  0.0900,  0.9100,  1.8900,\n",
      "          1.9800,  2.0700,  1.6900,  0.4000,  1.2800,  1.6600,  2.0600,  1.1000,\n",
      "          1.6800,  2.6700,  0.8400,  1.3600,  1.5600,  1.2700,  0.7300,  1.1300,\n",
      "          0.4000,  2.0700,  1.3900,  1.8700,  0.1100,  2.3100,  0.7200,  1.4500,\n",
      "          0.9000,  1.8600,  2.8100,  0.0600,  2.8800,  0.4400,  2.8500,  2.3600,\n",
      "          1.0000,  1.7300,  1.8800,  1.0200,  1.9700,  0.9700,  2.6900,  1.7900,\n",
      "          2.3100,  1.5500,  1.9500,  0.7500,  1.3600,  1.0400,  1.5800,  2.6200,\n",
      "          1.8900,  0.7100,  2.3300,  2.5600,  0.5000,  1.6400,  2.1000,  1.2700,\n",
      "          2.8000,  1.1700,  2.0600,  2.3300,  1.0100,  1.2300,  2.9300,  0.3500,\n",
      "          2.7600,  2.8300,  1.7700,  1.6500,  1.8700,  2.3600,  0.9500,  2.7500,\n",
      "          1.6700,  1.6300,  0.6300,  2.2000,  1.8800,  1.0400,  1.1700,  1.2700,\n",
      "          1.1800,  0.2600,  1.3600,  0.5200,  2.1200,  2.7300,  1.1800,  2.4000,\n",
      "          0.8600,  2.3400,  2.9800,  2.8900,  1.9900,  1.3200,  1.8700,  0.4400,\n",
      "          0.9600,  1.9900,  0.9400,  0.7400,  1.4000,  0.6700,  0.9900,  1.8800],\n",
      "        [ 2.0100,  2.1900,  0.9500,  0.5700,  2.6300,  1.1500,  2.1500,  0.0900,\n",
      "          2.7500,  0.1600,  1.3200,  2.1200,  1.5500,  1.4000,  1.4800,  1.5900,\n",
      "          0.6900,  0.7000,  1.6600,  1.2400,  0.0700,  0.1400,  2.8900,  0.7300,\n",
      "          2.3500,  0.2500,  2.7000,  1.0500,  2.7200,  0.4600,  2.8300,  0.4400,\n",
      "          2.4400,  0.1500,  1.8200,  1.6700,  0.9000,  1.1600,  0.2100,  0.7900,\n",
      "          1.1400,  1.5700,  2.0500,  1.7200,  2.6500,  0.7600,  1.2600,  0.5800,\n",
      "          0.7800,  2.6500,  1.4600,  1.7500,  1.4600,  2.0200,  0.2900,  1.8300,\n",
      "          2.9500,  1.7000,  2.1900,  0.5200,  1.8600,  0.9900,  1.3500,  0.4000,\n",
      "          0.7900,  1.3300,  0.0100,  0.5200,  2.9300,  0.0400,  1.6700,  0.5700,\n",
      "          1.0600,  0.9500,  0.1700,  2.5000,  2.9300,  0.3200,  2.9100,  0.6000,\n",
      "          0.4700,  1.7600,  0.8100,  1.3600,  1.9100,  0.1600,  1.0200,  0.6100,\n",
      "          2.9700,  2.4600,  1.7800,  1.1200,  1.8800,  2.9100,  0.9300,  2.9600,\n",
      "          0.5100,  0.9800,  0.2000,  0.8300,  1.2500,  2.7600,  1.1000,  2.6700,\n",
      "          0.7500,  2.9500,  1.9600,  2.4400,  1.4800,  2.5700,  1.3700,  2.2800,\n",
      "          2.5000,  0.6500,  0.1800,  1.4300,  2.9200,  1.3500,  1.0300,  0.7400,\n",
      "          2.3700,  0.7300,  1.7300,  2.1700,  0.5200,  2.5700,  1.9500,  2.5000,\n",
      "          1.0700,  2.9500,  1.6300,  0.1500,  2.8400,  0.0900,  0.9000,  2.4000,\n",
      "          1.9100,  1.4600,  0.3600,  1.8000,  1.9600,  1.5600,  1.0900,  2.3400,\n",
      "          0.5900,  2.6300,  0.4000,  0.0900,  2.1600,  1.4400,  1.5300,  0.5900,\n",
      "          2.6500,  1.3100,  1.5700,  1.4900,  1.6300,  1.0400,  2.5200,  0.0600,\n",
      "          0.2100,  2.1600,  0.0400,  0.4600,  1.6200,  0.5000,  1.7900,  0.9000,\n",
      "          1.7300,  2.5200,  0.7300,  1.5000,  1.2400,  0.6400,  2.9300,  2.7500,\n",
      "          2.3300,  2.6100,  2.8300,  1.7300,  1.5900,  0.0200,  0.2900,  0.4900,\n",
      "          2.5100,  1.2900,  1.0900,  2.6000,  2.1600,  2.5800,  0.7100,  1.7600,\n",
      "          2.5900,  0.9300,  0.0300,  1.4900,  1.2000,  1.7100,  2.6500,  1.4700],\n",
      "        [-0.6700, -0.6900, -0.0400, -0.9600, -0.9900, -0.3500, -0.9800, -0.9500,\n",
      "         -0.0400, -0.6000, -0.9200, -0.8700, -0.7000, -0.6300, -1.0000, -1.0000,\n",
      "         -1.0000, -0.4300, -0.7600, -0.8800, -0.5500, -1.0000, -0.7700, -0.3800,\n",
      "         -0.0800, -0.7000, -0.8500, -0.9300, -1.0000, -0.7300, -0.9600, -0.8200,\n",
      "         -0.9900, -0.4800, -0.9700, -0.8400, -0.7800, -0.6500, -0.7800, -0.7800,\n",
      "         -0.9400, -0.9000, -0.6100, -1.0000, -0.9200, -0.8700, -0.7700, -0.5900,\n",
      "         -0.9500, -0.1800, -0.4700, -0.4500, -0.9500, -0.9400, -0.1300, -0.9400,\n",
      "         -0.9000, -0.9300, -0.8300, -0.9300, -0.9500, -0.3000, -1.0000, -1.0000,\n",
      "         -0.4400, -0.8400, -0.9900, -0.9900, -0.7400, -0.5000, -0.9400, -0.7100,\n",
      "         -0.3200, -0.9700, -0.3400, -0.8900, -0.1500, -0.1400, -0.9200, -0.9400,\n",
      "         -0.9800, -0.8100, -0.1700, -1.0000, -0.8300, -0.3200, -0.6800, -0.7900,\n",
      "         -0.8900, -0.7800, -0.9100, -0.9600, -1.0000, -0.0600, -0.6000, -0.9700,\n",
      "         -0.9800, -0.9900, -0.9300, -0.2800, -0.7800, -0.9200, -0.9900, -0.7000,\n",
      "         -0.9200, -0.9500, -0.5600, -0.8200, -0.8900, -0.7800, -0.4900, -0.7100,\n",
      "         -0.2800, -0.9900, -0.8300, -0.9700, -0.0800, -1.0000, -0.4800, -0.8500,\n",
      "         -0.5900, -0.9600, -0.9200, -0.0400, -0.9000, -0.3000, -0.9100, -1.0000,\n",
      "         -0.6500, -0.9400, -0.9700, -0.6600, -0.9800, -0.6300, -0.9500, -0.9500,\n",
      "         -1.0000, -0.8900, -0.9800, -0.5000, -0.8200, -0.6700, -0.8900, -0.9600,\n",
      "         -0.9700, -0.4800, -1.0000, -0.9700, -0.3400, -0.9100, -1.0000, -0.7800,\n",
      "         -0.9200, -0.7300, -0.9900, -1.0000, -0.6500, -0.7600, -0.8900, -0.2400,\n",
      "         -0.9300, -0.9200, -0.9500, -0.9200, -0.9700, -1.0000, -0.6200, -0.9400,\n",
      "         -0.9200, -0.9100, -0.4300, -1.0000, -0.9700, -0.6700, -0.7300, -0.7800,\n",
      "         -0.7400, -0.1800, -0.8200, -0.3600, -1.0000, -0.9400, -0.7400, -0.9900,\n",
      "         -0.5700, -1.0000, -0.8700, -0.9000, -0.9800, -0.8000, -0.9700, -0.3000,\n",
      "         -0.6200, -0.9800, -0.6100, -0.5000, -0.8300, -0.4500, -0.6400, -0.9700]])\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(test_dataset.__len__())\n",
    "inputs, outputs, points = test_dataset.__getitem__(idx)\n",
    "print(points.min(), points.max())\n",
    "inputs = ''.join([train_dataset.itos[int(i)] for i in inputs])\n",
    "outputs = ''.join([train_dataset.itos[int(i)] for i in outputs])\n",
    "print('id:{}\\ninputs:{}\\noutputs:{}\\npoints:{}'.format(idx,inputs,outputs,points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/22/2021 15:47:43 - INFO - mingpt.model -   number of parameters: 3.058023e+07\n"
     ]
    }
   ],
   "source": [
    "from mingpt.model import GPT, GPTConfig, PointNetConfig\n",
    "pconf = PointNetConfig(embeddingSize=embeddingSize, \n",
    "                       numberofPoints=numPoints, \n",
    "                       numberofVars=numVars, \n",
    "                       numberofYs=numYs)\n",
    "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size,\n",
    "                  n_layer=8, n_head=8, n_embd=embeddingSize, padding_idx=train_dataset.paddingID)\n",
    "model = GPT(mconf, pconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fName = '{}_SymbolicGPT_{}_{}_{}.txt'.format(dataInfo, \n",
    "                                             'GPT_PT_Summation', \n",
    "                                             'Padding',\n",
    "                                             blockSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder already exists!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 123: train loss 0.68934. lr 5.924184e-04:  14%|███▌                     | 124/860 [03:22<19:52,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:tensor([21, 43, 41, 42, 44,  3, 33, 34, 43,  3, 23,  5, 45, 11,  6, 23,  4,  4,\n",
      "        32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,\n",
      "        32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,\n",
      "        32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,\n",
      "        32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,\n",
      "        32, 32, 32, 32, 32, 32, 32, 32, 32, 32], device='cuda:0')\n",
      "Logit:tensor([43, 41, 42, 44,  3, 33, 34, 43,  3, 43,  5, 45, 11,  6, 23,  4,  4,  4,\n",
      "        43, 39, 39, 43, 43,  5, 43, 39, 43, 43, 43,  5,  3, 43, 43,  5, 39, 39,\n",
      "        43, 43, 41, 43,  4, 39, 39,  3, 43,  6, 43,  5, 43, 43, 43, 41, 39, 43,\n",
      "        43,  5,  5, 43,  3, 43, 39, 39, 43, 43, 42, 41,  3,  3, 41, 43, 39, 43,\n",
      "        43, 45, 45, 41,  6, 39,  4,  6, 45, 43, 45,  5, 11, 43, 43, 41, 42,  6,\n",
      "         3,  3, 34, 43,  3, 43, 43, 43,  3,  3], device='cuda:0')\n",
      "Input:<sqrt(abs(C*x1+C))__________________________________________________________________________________\n",
      "Logit:sqrt(abs(s*x1+C)))snnss*snsss*(ss*nnssqs)nn(s+s*sssqnss**s(snnssrq((qsnssxxq+n)+xsx*1ssqr+((bs(sss((\n",
      "Target:tensor([43, 41, 42, 44,  3, 33, 34, 43,  3, 23,  5, 45, 11,  6, 23,  4,  4, 22,\n",
      "        32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,\n",
      "        32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,\n",
      "        32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,\n",
      "        32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,\n",
      "        32, 32, 32, 32, 32, 32, 32, 32, 32, 32], device='cuda:0')\n",
      "Target:sqrt(abs(C*x1+C))>__________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 196: train loss 0.42254. lr 5.808965e-04:  23%|█████▋                   | 197/860 [05:21<18:07,  1.64s/it]"
     ]
    }
   ],
   "source": [
    "from mingpt.trainer import Trainer, TrainerConfig\n",
    "import os\n",
    "\n",
    "try: \n",
    "    os.mkdir(addr)\n",
    "except:\n",
    "    print('Folder already exists!')\n",
    "    \n",
    "ckptPath = '{}/{}.pt'.format(addr,fName.split('.txt')[0])\n",
    "# initialize a trainer instance and kick off training\n",
    "tconf = TrainerConfig(max_epochs=numEpochs, batch_size=batchSize, learning_rate=6e-4,\n",
    "                      lr_decay=True, warmup_tokens=512*20, final_tokens=2*len(train_dataset)*blockSize,\n",
    "                      num_workers=0, ckpt_path=ckptPath)\n",
    "trainer = Trainer(model, train_dataset, val_dataset, tconf)\n",
    "\n",
    "try:\n",
    "    trainer.train()\n",
    "except KeyboardInterrupt:\n",
    "    print('KeyboardInterrupt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best model\n",
    "model.load_state_dict(torch.load(ckptPath))\n",
    "model = model.eval().to(trainer.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a safe wrapper for numpy math functions\n",
    "from numpy import *\n",
    "import numpy as np\n",
    "\n",
    "def divide(x, y):\n",
    "  x = np.nan_to_num(x)\n",
    "  y = np.nan_to_num(y)\n",
    "  return np.divide(x,y+1e-5)\n",
    "\n",
    "def sqrt(x):\n",
    "  x = np.nan_to_num(x)\n",
    "  return np.sqrt(np.abs(x)) \n",
    "\n",
    "# Mean square error\n",
    "def mse(y, y_hat):\n",
    "    y_hat = np.reshape(y_hat, [1, -1])[0]\n",
    "    y_gold = np.reshape(y, [1, -1])[0]\n",
    "    our_sum = 0\n",
    "    for i in range(len(y_gold)):\n",
    "        our_sum += (y_hat[i] - y_gold[i]) ** 2\n",
    "\n",
    "    return our_sum / len(y_gold)\n",
    "\n",
    "# Mean square error\n",
    "def relativeErr(y, y_hat):\n",
    "    y_hat = np.reshape(y_hat, [1, -1])[0]\n",
    "    y_gold = np.reshape(y, [1, -1])[0]\n",
    "    our_sum = 0\n",
    "    for i in range(len(y_gold)):\n",
    "        if y_gold[i] < 1: \n",
    "            # use regular MSE\n",
    "            our_sum += (y_hat[i] - y_gold[i]) ** 2\n",
    "        else:\n",
    "            # use relative MSE\n",
    "            our_sum += ((y_hat[i] - y_gold[i])/y_gold[i]) ** 2\n",
    "\n",
    "    return our_sum / len(y_gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class pointsDataset(Dataset):\n",
    "\n",
    "#     def __init__(self, data):\n",
    "#         # data should be a list of x,y pairs\n",
    "#         self.x = data[0] # it should be a list\n",
    "#         self.y = data[1] # it should be a list\n",
    "#         self.threshold = [-1000,1000]\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.y)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         # grab an example from the data\n",
    "#         x = self.x[idx] \n",
    "#         y = self.y[idx] \n",
    "        \n",
    "#         p = x+[y]\n",
    "        \n",
    "#         p = torch.tensor(p, dtype=torch.float)\n",
    "#         p = torch.nan_to_num(p, nan=0.0, \n",
    "#                              posinf=self.threshold[1], \n",
    "#                              neginf=self.threshold[0])\n",
    "#         p[p>self.threshold[1]] = self.threshold[1] # clip the upper bound\n",
    "#         p[p<self.threshold[0]] = self.threshold[0] # clip the lower bound\n",
    "        \n",
    "#         return p\n",
    "    \n",
    "# # train a mlp to find the constants\n",
    "# data = pointsDataset((t['X'],t['Y']))\n",
    "# loader = torch.utils.data.DataLoader(\n",
    "#                                 data, \n",
    "#                                 shuffle=False, \n",
    "#                                 pin_memory=True,\n",
    "#                                 batch_size=batchSize,\n",
    "#                                 num_workers=0)\n",
    "# class MLP(nn.Module):\n",
    "#     def __init__(self, inputSize, outputSize):\n",
    "#         super().__init__()\n",
    "#         self.layers = nn.Sequential(\n",
    "#             nn.Linear(inputSize, 100),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(100, outputSize)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         c = self.layers(x)\n",
    "#         return c\n",
    "    \n",
    "#     def loss(self, constants, eq, X, Y):\n",
    "#         # constants is the output of the model\n",
    "#         err = torch.zeros(len(constants)) # batch_size\n",
    "        \n",
    "#         # sample number of points\n",
    "#         indexes = []\n",
    "#         numberSamples = 10\n",
    "#         while len(indexes) != numberSamples:\n",
    "#             randNum = np.random.randint(len(batch))\n",
    "#             indexes.append(randNum)\n",
    "#         X = X[indexes,:]\n",
    "#         Y = Y[indexes,:]\n",
    "        \n",
    "#         # replace the constants with their predicted values\n",
    "#         for idx, const in enumerate(constants):\n",
    "#             eq = eq.replace('C','{}').format(*const.tolist())\n",
    "        \n",
    "#         # calculate the error for a limited number of points, approximate the error\n",
    "#         for x,y in zip(X,Y):\n",
    "#             # replace variables with their values\n",
    "#             for i,e in enumerate(x):\n",
    "#                 eqTemp = eq.replace('x{}'.format(i+1), str(e.item()))\n",
    "                \n",
    "#             # calculate the error\n",
    "#             yHat = eval(eqTemp)\n",
    "#             err[idx] += (y.item()-yHat)**2\n",
    "#         err[idx] /= numberSamples\n",
    "            \n",
    "#         print(err.shape, constants.shape)\n",
    "#         return err\n",
    "    \n",
    "# c = [0 for i,x in enumerate(predicted) if x=='C']\n",
    "# network = MLP(numVars+numYs, len(c))\n",
    "# cHat = network(batch)\n",
    "# err = network.loss(cHat, predicted, batch[:,:numVars], batch[:,-numYs:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# alright, let's sample some character-level symbolic GPT\n",
    "from mingpt.utils import sample\n",
    "from gp_model import Genetic_Model\n",
    "from mlp_model import MLP_Model\n",
    "from scipy.optimize import least_squares, newton\n",
    "\n",
    "    \n",
    "loader = torch.utils.data.DataLoader(\n",
    "                                test_dataset, \n",
    "                                shuffle=False, \n",
    "                                pin_memory=True,\n",
    "                                batch_size=1,\n",
    "                                num_workers=0)\n",
    "\n",
    "testRange = [3.1,6.0]\n",
    "numTestPoints = 10\n",
    "#test = np.linspace(3.1,6.0,numTestPoints)\n",
    "\n",
    "gpm = Genetic_Model(n_jobs=-1)\n",
    "mlp = MLP_Model()\n",
    "\n",
    "resultDict = {}\n",
    "try:\n",
    "    with open(fName, 'w', encoding=\"utf-8\") as o:\n",
    "        modelName = 'SymbolicGPT'\n",
    "        resultDict[fName] = {modelName:[],\n",
    "                             'GP':[],\n",
    "                             'MLP':[]}\n",
    "\n",
    "        for i, batch in enumerate(loader):\n",
    "                \n",
    "            inputs,outputs,points = batch\n",
    "\n",
    "            print('Test Case {}.'.format(i))\n",
    "            o.write('Test Case {}/{}.\\n'.format(i,len(textTest)-1))\n",
    "\n",
    "            t = json.loads(textTest[i])\n",
    "\n",
    "            inputs = inputs[:,0:1].to(trainer.device)\n",
    "            points = points.to(trainer.device)\n",
    "            outputsHat = sample(model, inputs, blockSize, points=points,\n",
    "                          temperature=1.0, sample=True, \n",
    "                          top_k=40)[0]\n",
    "\n",
    "            # filter out predicted\n",
    "            target = ''.join([train_dataset.itos[int(i)] for i in outputs[0]])\n",
    "            predicted = ''.join([train_dataset.itos[int(i)] for i in outputsHat])\n",
    "\n",
    "            target = target.strip(train_dataset.paddingToken).split('>')\n",
    "            target = target[0] if len(target[0])>=1 else target[1]\n",
    "            target = target.strip('<').strip(\">\")\n",
    "            predicted = predicted.strip(train_dataset.paddingToken).split('>')\n",
    "            predicted = predicted[0] if len(predicted[0])>=1 else predicted[1]\n",
    "            predicted = predicted.strip('<').strip(\">\")\n",
    "            \n",
    "            print('Target:{}\\nSkeleton:{}'.format(target, predicted))\n",
    "            \n",
    "            o.write('{}\\n'.format(target))\n",
    "            o.write('{}:\\n'.format(modelName))\n",
    "            o.write('{}\\n'.format(predicted))\n",
    "\n",
    "            # train a regressor to find the constants (too slow)\n",
    "            c = [1 for i,x in enumerate(predicted) if x=='C']            \n",
    "            def lossFunc(constants, eq, X, Y):\n",
    "                err = 0\n",
    "                eq = eq.replace('C','{}').format(*constants)\n",
    "\n",
    "                for x,y in zip(X,Y):\n",
    "                    eqTemp = eq + ''\n",
    "                    for i,e in enumerate(x):\n",
    "                        eqTemp = eqTemp.replace('x{}'.format(i+1), str(e))\n",
    "                    try:\n",
    "                        yHat = eval(eqTemp)\n",
    "                    except:\n",
    "                        yHat = 100\n",
    "                    err += (y-yHat)**2\n",
    "                err /= len(Y)\n",
    "                return err\n",
    "            \n",
    "            try:\n",
    "                if len(c) == 0:\n",
    "                    pass # do nothing\n",
    "                else:\n",
    "                    cHat = least_squares(lossFunc, c, ftol=1e-3,\n",
    "                                         args=(predicted, t['X'], t['Y']))\n",
    "#                     cHat= newton(lossFunc, c, maxiter=100,\n",
    "#                                  args=(predicted, t['X'], t['Y']))\n",
    "                    predicted = predicted.replace('C','{}').format(*cHat.x)\n",
    "            except:\n",
    "                print('Wrong Equation:{}'.format(predicted))\n",
    "                raise\n",
    "                predicted = 0\n",
    "\n",
    "            # TODO: let's enjoy GPU\n",
    "\n",
    "            print('Skeleton+LS:{}'.format(predicted))\n",
    "\n",
    "            Ys = [] #t['YT']\n",
    "            Yhats = []\n",
    "            for xs in t['XT']:\n",
    "                try:\n",
    "                    eqTmp = target + '' # copy eq\n",
    "                    eqTmp = eqTmp.replace(' ','')\n",
    "                    eqTmp = eqTmp.replace('\\n','')\n",
    "                    for i,x in enumerate(xs):\n",
    "                        # replace xi with the value in the eq\n",
    "                        eqTmp = eqTmp.replace('x{}'.format(i+1), str(x))\n",
    "                        if ',' in eqTmp:\n",
    "                            assert 'There is a , in the equation!'\n",
    "                    YEval = eval(eqTmp)\n",
    "                    YEval = 0 if np.isnan(YEval) else YEval\n",
    "                    YEval = 100 if np.isinf(YEval) else YEval\n",
    "                except:\n",
    "                    YEval = 100 #TODO: Maybe I have to punish the model for each wrong template not for each point\n",
    "                Ys.append(YEval)\n",
    "                try:\n",
    "                    eqTmp = predicted + '' # copy eq\n",
    "                    eqTmp = eqTmp.replace(' ','')\n",
    "                    eqTmp = eqTmp.replace('\\n','')\n",
    "                    for i,x in enumerate(xs):\n",
    "                        # replace xi with the value in the eq\n",
    "                        eqTmp = eqTmp.replace('x{}'.format(i+1), str(x))\n",
    "                        if ',' in eqTmp:\n",
    "                            assert 'There is a , in the equation!'\n",
    "                    Yhat = eval(eqTmp)\n",
    "                    Yhat = 0 if np.isnan(Yhat) else Yhat\n",
    "                    Yhat = 100 if np.isinf(Yhat) else Yhat\n",
    "                except:\n",
    "                    Yhat = 100\n",
    "                Yhats.append(Yhat)\n",
    "            err = relativeErr(Ys,Yhats)\n",
    "\n",
    "            if type(err) is np.complex128 or np.complex:\n",
    "                err = abs(err.real)\n",
    "\n",
    "            resultDict[fName][modelName].append(err)\n",
    "\n",
    "            o.write('{}\\n{}\\n\\n'.format( \n",
    "                                    predicted,\n",
    "                                    err\n",
    "                                    ))\n",
    "\n",
    "            print('Err:{}'.format(err))\n",
    "            \n",
    "            # Calculate error for baselines\n",
    "\n",
    "    #         # tokenize to get input x, input y, and true eqn\n",
    "    #         train_data_x = t[\"X\"]\n",
    "    #         train_data_y = t[\"Y\"]\n",
    "    #         test_data_x = t[\"XT\"]\n",
    "    #         test_data_y = t[\"YT\"]\n",
    "\n",
    "    #         # train MLP model\n",
    "    #         mlp.reset()\n",
    "    #         model_eqn, _, best_err = mlp.repeat_train(\n",
    "    #                                                 train_data_x, \n",
    "    #                                                 train_data_y,\n",
    "    #                                                 test_x=test_data_x, \n",
    "    #                                                 test_y=test_data_y,                                     \n",
    "    #                                                 verbose=False)\n",
    "    #         test_y_hat = mlp.predict(test_data_x)\n",
    "    #         err = relativeErr(test_data_y,test_y_hat)\n",
    "    #         print(\"{}: {}\".format(mlp.name, model_eqn)[:550])\n",
    "    #         print(\"Err: {:.5f}\".format(err))\n",
    "    #         resultDict[fName]['MLP'].append(err)\n",
    "    #         o.write('\\n{}: {}\\n{}'.format('MLP', \n",
    "    #                                    err,\n",
    "    #                                    model_eqn))\n",
    "\n",
    "    #         # train GP model\n",
    "    #         gpm.reset()\n",
    "    #         model_eqn, _, best_err = gpm.repeat_train(train_data_x, train_data_y,\n",
    "    #                                                 test_x=test_data_x, test_y=test_data_y,\n",
    "    #                                                 verbose=False)\n",
    "    #         print(\"{}: {}\".format(gpm.name, model_eqn)[:550])\n",
    "    #         test_y_hat = gpm.predict(test_data_x)\n",
    "    #         err = relativeErr(test_data_y,test_y_hat)\n",
    "    #         print(\"Err: {:.5f}\".format(err))\n",
    "    #         resultDict[fName]['GP'].append(err)\n",
    "    #         o.write('\\n{}: {}\\n{}'.format('GP', \n",
    "    #                                    err,\n",
    "    #                                    model_eqn))\n",
    "            print('') # just an empty line\n",
    "    print('Avg Err:{}'.format(np.mean(resultDict[fName][modelName])))\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print('KeyboardInterrupt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the error frequency for model comparison\n",
    "from matplotlib import pyplot as plt\n",
    "num_eqns = len(resultDict[fName]['SymbolicGPT'])\n",
    "num_vars = pconf.numberofVars\n",
    "\n",
    "models = list(key for key in resultDict[fName].keys() if len(resultDict[fName][key])==num_eqns)\n",
    "lists_of_error_scores = [resultDict[fName][key] for key in models if len(resultDict[fName][key])==num_eqns]\n",
    "linestyles = [\"-\",\"dashdot\",\"dotted\",\"--\"]\n",
    "\n",
    "eps = 0.00001\n",
    "y, x, _ = plt.hist([np.log([max(min(x+eps, 1e5),1e-5) for x in e]) for e in lists_of_error_scores],\n",
    "                   label=models,\n",
    "                   cumulative=True, \n",
    "                   histtype=\"step\", \n",
    "                   bins=2000, \n",
    "                   density=True,\n",
    "                   log=False)\n",
    "y = np.expand_dims(y,0)\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for idx, m in enumerate(models): \n",
    "    plt.plot(x[:-1], \n",
    "           y[idx] * 100, \n",
    "           linestyle=linestyles[idx], \n",
    "           label=m)\n",
    "\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.title(\"{} equations of {} variables\".format(num_eqns, num_vars))\n",
    "plt.xlabel(\"Log of Relative Mean Square Error\")\n",
    "plt.ylabel(\"Normalized Cumulative Frequency\")\n",
    "\n",
    "name = '{}.png'.format(fName.split('.txt')[0])\n",
    "plt.savefig(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
