{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logging\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make deterministic\n",
    "from mingpt.utils import set_seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "numEpochs = 50 # number of epochs to train the GPT+PT model\n",
    "embeddingSize = 512 # the hidden dimension of the representation of both GPT and PT\n",
    "numPoints=20 # number of points that we are going to receive to make a prediction about f given x and y, if you don't know then use the maximum\n",
    "numVars=2 # the dimenstion of input points x, if you don't know then use the maximum\n",
    "numYs=1 # the dimension of output points y = f(x), if you don't know then use the maximum\n",
    "blockSize = 100 # spatial extent of the model for its context\n",
    "batchSize = 128 # batch size of training data\n",
    "dataDir = 'D:/Datasets/Symbolic Dataset/Datasets'\n",
    "dataInfo = 'XYE_{}Var_{}Points_{}EmbeddingSize'.format(numVars, numPoints, embeddingSize)\n",
    "target = 'Skeleton' #'Skeleton' #'EQ'\n",
    "dataFolder = '1-2Var_RandSupport_FixedLength_-1to4_4.1to8.0_benchmark_nguyenALL'\n",
    "addr = './SavedModels/bestModel/' # where to save model\n",
    "maxNumFiles = 30\n",
    "bestLoss = 0.142760 # NONE # if there is any model to load as pre-trained one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, block_size, chars, target='EQ'):\n",
    "        data_size, vocab_size = len(data), len(chars)\n",
    "        print('data has %d examples, %d unique.' % (data_size, vocab_size))\n",
    "        \n",
    "        self.stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "        self.itos = { i:ch for i,ch in enumerate(chars) }\n",
    "        \n",
    "        # padding token\n",
    "        self.paddingToken = '_'\n",
    "        self.paddingID = self.stoi[self.paddingToken]\n",
    "        self.stoi[self.paddingToken] = self.paddingID\n",
    "        self.itos[self.paddingID] = self.paddingToken\n",
    "        self.threshold = [-1000,1000]\n",
    "        \n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.data = data # it should be a list of examples\n",
    "        self.target = target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)-1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # grab an example from the data\n",
    "        chunk = self.data[idx] # sequence of tokens including x, y, eq, etc.\n",
    "        \n",
    "        try:\n",
    "            chunk = json.loads(chunk) # convert the sequence tokens to a dictionary\n",
    "        except:\n",
    "            print(\"Couldn't convert to json: {}\".format(chunk))\n",
    "            \n",
    "        # encode every character in the equation to an integer\n",
    "        # < is SOS, > is EOS\n",
    "        dix = [self.stoi[s] for s in '<'+chunk[self.target]+'>']\n",
    "        inputs = dix[:-1]\n",
    "        outputs = dix[1:]\n",
    "        \n",
    "        # add the padding to the equations\n",
    "        paddingSize = max(self.block_size-len(inputs),0)\n",
    "        paddingList = [self.paddingID]*paddingSize\n",
    "        inputs += paddingList\n",
    "        outputs += paddingList \n",
    "        \n",
    "        # make sure it is not more than what should be\n",
    "        inputs = inputs[:self.block_size]\n",
    "        outputs = outputs[:self.block_size]\n",
    "        \n",
    "        # extract points from the input sequence\n",
    "        points = torch.zeros(numVars+numYs, numPoints)\n",
    "        for idx, xy in enumerate(zip(chunk['X'], chunk['Y'])):\n",
    "            x = xy[0] + [0]*(max(numVars-len(xy[0]),0)) # padding\n",
    "            y = [xy[1]] if type(xy[1])== float else xy[1]\n",
    "            y = y + [0]*(max(numYs-len(y),0)) # padding\n",
    "            p = x+y # because it is only one point \n",
    "            p = torch.tensor(p)\n",
    "            #replace nan and inf\n",
    "            p = torch.nan_to_num(p, nan=0.0, \n",
    "                                 posinf=self.threshold[1], \n",
    "                                 neginf=self.threshold[0])\n",
    "            p[p>self.threshold[1]] = self.threshold[1] # clip the upper bound\n",
    "            p[p<self.threshold[0]] = self.threshold[0] # clip the lower bound\n",
    "            points[:,idx] = p\n",
    "        \n",
    "        \"\"\"\n",
    "        arrange data and targets so that the first i elements of x\n",
    "        will be asked to predict the i-th element of y. Notice that\n",
    "        the eventual language model will actually make block_size\n",
    "        individual predictions at the same time based on this data,\n",
    "        so we are being clever and amortizing the cost of the forward\n",
    "        pass of the network. So for example if block_size is 4, then\n",
    "        we could e.g. sample a chunk of text \"hello\", the integers in\n",
    "        x will correspond to \"hell\" and in y will be \"ello\". This will\n",
    "        then actually \"multitask\" 4 separate examples at the same time\n",
    "        in the language model:\n",
    "        - given just \"h\", please predict \"e\" as next\n",
    "        - given \"he\" please predict \"l\" next\n",
    "        - given \"hel\" predict \"l\" next\n",
    "        - given \"hell\" predict \"o\" next\n",
    "        \n",
    "        In addition, because the DataLoader will create batches of examples,\n",
    "        every forward/backward pass during traning will simultaneously train\n",
    "        a LOT of predictions, amortizing a lot of computation. In particular,\n",
    "        for a batched input of integers X (B, T) where B is batch size and\n",
    "        T is block_size and Y (B, T), the network will during training be\n",
    "        simultaneously training to make B*T predictions, all at once! Of course,\n",
    "        at test time we can paralellize across batch B, but unlike during training\n",
    "        we cannot parallelize across the time dimension T - we have to run\n",
    "        a forward pass of the network to recover the next single character of the \n",
    "        sequence along each batch dimension, and repeatedly always feed in a next\n",
    "        character to get the next one.\n",
    "        \n",
    "        So yes there is a big asymmetry between train/test time of autoregressive\n",
    "        models. During training we can go B*T at a time with every forward pass,\n",
    "        but during test time we can only go B at a time, T times, with T forward \n",
    "        passes.\n",
    "        \"\"\"\n",
    "        inputs = torch.tensor(inputs, dtype=torch.long)\n",
    "        outputs = torch.tensor(outputs, dtype=torch.long)\n",
    "        return inputs, outputs, points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "def processDataFiles(files):\n",
    "    text = ''\"\"\n",
    "    for f in tqdm(files):\n",
    "        with open(f, 'r') as h: \n",
    "            lines = h.read() # don't worry we won't run out of file handles\n",
    "            if lines[-1]==-1:\n",
    "                lines = lines[:-1]\n",
    "            text += lines #json.loads(line)       \n",
    "            \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 11.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 100000 examples, 54 unique.\n"
     ]
    }
   ],
   "source": [
    "path = '{}/{}/Train/*.json'.format(dataDir, dataFolder)\n",
    "files = glob.glob(path)[:maxNumFiles]\n",
    "text = processDataFiles(files)\n",
    "chars = sorted(list(set(text))+['_','T','<','>']) # extract unique characters from the text before converting the text to a list\n",
    "# T is for the test data\n",
    "text = text.split('\\n') # convert the raw text to a set of examples\n",
    "text = text[:-1] if len(text[-1]) == 0 else text\n",
    "random.shuffle(text) # shuffle the dataset, it's important for combined number of variables\n",
    "train_dataset = CharDataset(text, blockSize, chars, target=target) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:tensor([22, 48, 46, 47, 49,  3, 35, 36, 48,  3, 12,  9, 11,  5, 50, 12,  4,  4,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34])\n",
      "id:15795\n",
      "inputs:<sqrt(abs(1.0*x1))__________________________________________________________________________________\n",
      "outputs:sqrt(abs(1.0*x1))>__________________________________________________________________________________\n",
      "points:tensor([[ 1.8535,  0.9868,  0.4910,  3.5687, -0.9755,  1.0919,  2.2073,  0.6608,\n",
      "          2.9987,  0.7460,  0.7118,  2.7532,  1.1871,  3.3545,  1.1658,  3.9936,\n",
      "          3.4541,  3.1069, -0.5287,  3.4882],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 1.3614,  0.9934,  0.7007,  1.8891,  0.9877,  1.0449,  1.4857,  0.8129,\n",
      "          1.7317,  0.8637,  0.8437,  1.6593,  1.0895,  1.8315,  1.0797,  1.9984,\n",
      "          1.8585,  1.7626,  0.7271,  1.8677]])\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(train_dataset.__len__())\n",
    "inputs, outputs, points = train_dataset.__getitem__(idx)\n",
    "print('inputs:{}'.format(inputs))\n",
    "inputs = ''.join([train_dataset.itos[int(i)] for i in inputs])\n",
    "outputs = ''.join([train_dataset.itos[int(i)] for i in outputs])\n",
    "print('id:{}\\ninputs:{}\\noutputs:{}\\npoints:{}'.format(idx,inputs,outputs,points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 142.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1001 examples, 54 unique.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "path = '{}/{}/Val/*.json'.format(dataDir,dataFolder)\n",
    "files = glob.glob(path)\n",
    "textVal = processDataFiles([files[0]])\n",
    "textVal = textVal.split('\\n') # convert the raw text to a set of examples\n",
    "val_dataset = CharDataset(textVal, blockSize, chars, target=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-7.0015) tensor(221.4538)\n",
      "id:860\n",
      "inputs:<2.58*exp(1.0*x1)*log(abs(2.0*x1+0.59))_____________________________________________________________\n",
      "outputs:2.58*exp(1.0*x1)*log(abs(2.0*x1+0.59))>_____________________________________________________________\n",
      "points:tensor([[-4.9690e-01,  7.6170e-01,  1.9532e+00,  2.9528e+00,  1.3324e+00,\n",
      "          2.5312e+00, -2.3000e-03,  1.7192e+00,  2.2507e+00,  2.0109e+00,\n",
      "          8.2210e-01,  3.3020e+00,  3.6761e+00, -3.0590e-01,  2.0001e+00,\n",
      "          2.7783e+00,  2.8090e+00,  3.7190e+00,  9.7400e-01, -2.2710e-01],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [-1.4105e+00,  4.1260e+00,  2.7330e+01,  9.2453e+01,  1.1528e+01,\n",
      "          5.6138e+01, -1.3926e+00,  2.0045e+01,  3.9846e+01,  2.9442e+01,\n",
      "          4.7096e+00,  1.3825e+02,  2.1106e+02, -7.0015e+00,  2.9036e+01,\n",
      "          7.5356e+01,  7.8131e+01,  2.2145e+02,  6.3544e+00, -4.1543e+00]])\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(val_dataset.__len__())\n",
    "inputs, outputs, points = val_dataset.__getitem__(idx)\n",
    "print(points.min(), points.max())\n",
    "inputs = ''.join([train_dataset.itos[int(i)] for i in inputs])\n",
    "outputs = ''.join([train_dataset.itos[int(i)] for i in outputs])\n",
    "print('id:{}\\ninputs:{}\\noutputs:{}\\npoints:{}'.format(idx,inputs,outputs,points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 129.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 2001 examples, 54 unique.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "path = '{}/{}/Test/*.json'.format(dataDir,dataFolder)\n",
    "files = glob.glob(path)\n",
    "textTest = processDataFiles(files)\n",
    "textTest = textTest.split('\\n') # convert the raw text to a set of examples\n",
    "# test_dataset_target = CharDataset(textTest, blockSize, chars, target=target)\n",
    "test_dataset = CharDataset(textTest, blockSize, chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.9410) tensor(3.9571)\n",
      "id:1294\n",
      "inputs:<0.4508*x1*x2*sin(1.0*x2)___________________________________________________________________________\n",
      "outputs:0.4508*x1*x2*sin(1.0*x2)>___________________________________________________________________________\n",
      "points:tensor([[ 1.7275,  3.0639,  3.7677,  0.1758,  3.6706,  2.7179,  0.9605, -0.8395,\n",
      "          2.3760, -0.8771, -0.8363,  2.5351,  0.9444,  1.8181,  3.9571,  0.5580,\n",
      "          0.4032,  0.3944,  3.4121,  3.0647],\n",
      "        [ 2.6912, -0.2168,  0.2600,  3.0117, -0.8038, -0.0626,  1.5906,  0.3948,\n",
      "          1.6447,  2.4807,  3.2615, -0.4916, -0.6020, -0.9410,  2.7888,  0.2043,\n",
      "          0.7180,  1.1541,  2.8316,  0.8246],\n",
      "        [ 0.9067,  0.0640,  0.1128,  0.0307,  0.9517,  0.0048,  0.6843, -0.0571,\n",
      "          1.7460, -0.5983,  0.1462,  0.2636,  0.1442,  0.6194,  1.7083,  0.0104,\n",
      "          0.0853,  0.1865,  1.3204,  0.8313]])\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(test_dataset.__len__())\n",
    "inputs, outputs, points = test_dataset.__getitem__(idx)\n",
    "print(points.min(), points.max())\n",
    "inputs = ''.join([train_dataset.itos[int(i)] for i in inputs])\n",
    "outputs = ''.join([train_dataset.itos[int(i)] for i in outputs])\n",
    "print('id:{}\\ninputs:{}\\noutputs:{}\\npoints:{}'.format(idx,inputs,outputs,points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/25/2021 13:16:03 - INFO - mingpt.model -   number of parameters: 3.058637e+07\n"
     ]
    }
   ],
   "source": [
    "from mingpt.model import GPT, GPTConfig, PointNetConfig\n",
    "pconf = PointNetConfig(embeddingSize=embeddingSize, \n",
    "                       numberofPoints=numPoints, \n",
    "                       numberofVars=numVars, \n",
    "                       numberofYs=numYs)\n",
    "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size,\n",
    "                  n_layer=8, n_head=8, n_embd=embeddingSize, padding_idx=train_dataset.paddingID)\n",
    "model = GPT(mconf, pconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "fName = '{}_SymbolicGPT_{}_{}_{}_MINIMIZE.txt'.format(dataInfo, \n",
    "                                             'GPT_PT_Summation', \n",
    "                                             'Padding',\n",
    "                                             blockSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./SavedModels/bestModel//XYE_2Var_20Points_512EmbeddingSize_SymbolicGPT_GPT_PT_Summation_Padding_100_MINIMIZE.pt\n",
      "Model has been loaded!\n"
     ]
    }
   ],
   "source": [
    "ckptPath = '{}/{}.pt'.format(addr,fName.split('.txt')[0])\n",
    "print(ckptPath)\n",
    "if bestLoss != None:\n",
    "    # load the best model\n",
    "    print('Model has been loaded!')\n",
    "    model.load_state_dict(torch.load(ckptPath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder already exists!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 781: train loss 0.58234. lr 3.002414e-04: 100%|█████████████████████████| 782/782 [05:35<00:00,  2.33it/s]\n",
      "05/25/2021 13:21:55 - INFO - mingpt.trainer -   test loss: 0.669639\n",
      "epoch 2 iter 781: train loss 0.54302. lr 6.000000e-05: 100%|█████████████████████████| 782/782 [05:40<00:00,  2.29it/s]\n",
      "05/25/2021 13:27:38 - INFO - mingpt.trainer -   test loss: 0.678913\n",
      "epoch 3 iter 751: train loss 0.53864. lr 2.826035e-04:  96%|████████████████████████ | 752/782 [05:24<00:12,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:tensor([22, 11,  9, 14,  5, 50, 12,  5, 48, 46, 47, 49,  3, 35, 36, 48,  3, 12,\n",
      "         9, 11,  5, 50, 12,  4,  4,  5, 37, 50, 45,  3,  8, 11,  9, 17,  5, 50,\n",
      "        12,  4,  6, 13,  9, 11,  5, 50, 12,  8, 11,  9, 13, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34], device='cuda:0')\n",
      "Logit:tensor([12,  9, 20, 19, 50, 12,  5,  5, 46, 47, 49,  3, 35, 36, 48,  3, 12,  9,\n",
      "        11,  5, 50, 12,  4,  4,  6, 42, 50, 45,  3,  8, 11,  9, 20, 17, 50, 12,\n",
      "         4,  6, 42,  9, 11, 23, 42, 12,  8, 11,  9, 18, 20, 23, 23, 23, 23, 23,\n",
      "        23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23,  9, 23, 23,  9, 20, 23,\n",
      "        11, 20, 37, 23, 20, 12, 20, 23, 12, 37, 11, 11, 12, 17, 37, 20, 20, 37,\n",
      "        11, 20, 37, 20, 11,  8, 12, 20, 20, 20], device='cuda:0')\n",
      "Input:<0.3*x1*sqrt(abs(1.0*x1))*exp(-0.6*x1)+2.0*x1-0.2___________________________________________________\n",
      "Logit:1.98x1**qrt(abs(1.0*x1))+lxp(-0.96x1)+l.0>l1-0.79>>>>>>>>>>>>>>>>>.>>.9>09e>919>1e0016e99e09e90-1999\n",
      "Target:tensor([11,  9, 14,  5, 50, 12,  5, 48, 46, 47, 49,  3, 35, 36, 48,  3, 12,  9,\n",
      "        11,  5, 50, 12,  4,  4,  5, 37, 50, 45,  3,  8, 11,  9, 17,  5, 50, 12,\n",
      "         4,  6, 13,  9, 11,  5, 50, 12,  8, 11,  9, 13, 23, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34], device='cuda:0')\n",
      "Target:0.3*x1*sqrt(abs(1.0*x1))*exp(-0.6*x1)+2.0*x1-0.2>___________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 3 iter 781: train loss 0.59175. lr 3.002414e-04: 100%|█████████████████████████| 782/782 [05:37<00:00,  2.32it/s]\n",
      "05/25/2021 13:33:17 - INFO - mingpt.trainer -   test loss: 0.656111\n",
      "epoch 4 iter 781: train loss 0.55839. lr 5.999996e-04: 100%|█████████████████████████| 782/782 [05:42<00:00,  2.28it/s]\n",
      "05/25/2021 13:39:01 - INFO - mingpt.trainer -   test loss: 0.647097\n",
      "epoch 5 iter 781: train loss 0.54965. lr 2.992758e-04: 100%|█████████████████████████| 782/782 [05:40<00:00,  2.29it/s]\n",
      "05/25/2021 13:44:44 - INFO - mingpt.trainer -   test loss: 0.666232\n",
      "epoch 6 iter 20: train loss 0.48881. lr 2.866067e-04:   3%|▋                          | 21/782 [00:09<05:24,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:tensor([22, 48, 46, 47, 49,  3, 35, 36, 48,  3, 42, 44, 39,  3, 35, 36, 48,  3,\n",
      "        12,  9, 11,  5, 50, 12,  4,  4,  4,  4, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34], device='cuda:0')\n",
      "Logit:tensor([48, 46, 47, 49,  3, 35, 36, 48,  3, 42, 44, 39,  3, 35, 36, 48,  3, 12,\n",
      "         9, 11,  5, 50, 12,  4,  4,  4,  4, 23, 48, 48, 48, 48, 48, 13, 13, 13,\n",
      "         9, 13, 13,  4,  4,  4,  4, 11, 12, 50,  4, 11, 12, 50, 11, 12, 11, 11,\n",
      "        11,  4,  4,  4, 11, 11, 11, 11,  4, 11, 11, 11, 11,  4,  4, 11, 12, 11,\n",
      "        11, 11,  4, 11, 11, 11, 11, 35, 12, 11, 11, 11, 11, 11, 11, 35, 11, 12,\n",
      "        12, 12, 11, 11, 12, 12, 11, 11, 11, 12], device='cuda:0')\n",
      "Input:<sqrt(abs(log(abs(1.0*x1))))________________________________________________________________________\n",
      "Logit:sqrt(abs(log(abs(1.0*x1))))>sssss222.22))))01x)01x01000)))0000)0000))01000)0000a1000000a011100110001\n",
      "Target:tensor([48, 46, 47, 49,  3, 35, 36, 48,  3, 42, 44, 39,  3, 35, 36, 48,  3, 12,\n",
      "         9, 11,  5, 50, 12,  4,  4,  4,  4, 23, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34], device='cuda:0')\n",
      "Target:sqrt(abs(log(abs(1.0*x1))))>________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 6 iter 216: train loss 0.51754. lr 1.725042e-04:  28%|██████▉                  | 216/782 [01:33<04:14,  2.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:tensor([22,  8, 11,  9, 12, 16,  5, 50, 12,  5, 50, 13,  5, 48, 46, 47, 49,  3,\n",
      "        35, 36, 48,  3,  8, 11,  9, 15,  5, 50, 13,  4,  4,  5, 42, 44, 39,  3,\n",
      "        35, 36, 48,  3, 12,  9, 11,  5, 50, 13,  4,  4,  6, 11,  9, 12, 13,  5,\n",
      "        50, 13, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34], device='cuda:0')\n",
      "Logit:tensor([ 8, 11,  9, 11, 12,  5, 50, 12,  5, 50, 13,  5, 42, 40, 47, 49,  3, 35,\n",
      "        36, 48,  3, 11, 11,  9, 13, 19, 50, 13,  4,  4,  5, 42, 44, 39,  3, 35,\n",
      "        36, 48,  3, 12,  9, 11,  5, 50, 13,  4,  4, 23, 11,  9, 11, 17,  5, 50,\n",
      "        13, 23, 23, 11, 23, 23, 11, 11, 11, 11, 23, 11, 11, 11, 11, 11, 11, 23,\n",
      "        11, 11,  5, 35,  5,  5,  5, 11, 11, 11, 11, 11, 11, 35, 11, 35, 11, 11,\n",
      "        11, 11,  9, 11, 11, 11, 11,  5, 11, 11], device='cuda:0')\n",
      "Input:<-0.15*x1*x2*sqrt(abs(-0.4*x2))*log(abs(1.0*x2))+0.12*x2____________________________________________\n",
      "Logit:-0.01*x1*x2*lirt(abs(00.28x2))*log(abs(1.0*x2))>0.06*x2>>0>>0000>000000>00*a***000000a0a0000.0000*00\n",
      "Target:tensor([ 8, 11,  9, 12, 16,  5, 50, 12,  5, 50, 13,  5, 48, 46, 47, 49,  3, 35,\n",
      "        36, 48,  3,  8, 11,  9, 15,  5, 50, 13,  4,  4,  5, 42, 44, 39,  3, 35,\n",
      "        36, 48,  3, 12,  9, 11,  5, 50, 13,  4,  4,  6, 11,  9, 12, 13,  5, 50,\n",
      "        13, 23, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34], device='cuda:0')\n",
      "Target:-0.15*x1*x2*sqrt(abs(-0.4*x2))*log(abs(1.0*x2))+0.12*x2>____________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 6 iter 781: train loss 0.46738. lr 6.000000e-05: 100%|█████████████████████████| 782/782 [05:38<00:00,  2.31it/s]\n",
      "05/25/2021 13:50:25 - INFO - mingpt.trainer -   test loss: 0.694502\n",
      "epoch 7 iter 703: train loss 0.48801. lr 2.547645e-04:  90%|██████████████████████▌  | 704/782 [05:04<00:35,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:tensor([22, 37, 50, 45,  3, 13,  9, 11,  5, 50, 12,  4, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34], device='cuda:0')\n",
      "Logit:tensor([11, 50, 45,  3, 13,  9, 11,  5, 50, 12,  4, 23, 48, 48, 11, 48,  9, 48,\n",
      "        11, 11, 37, 11, 11, 11, 11, 11, 12, 11, 11, 12, 12, 11, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 37, 12, 12, 12, 12, 11,\n",
      "        12, 12, 12, 11, 12, 12, 11, 11, 12, 37, 12, 11, 12, 12, 11, 12, 12, 37,\n",
      "        12, 11, 12, 12, 12, 11, 11,  9, 12, 11, 12, 11, 12, 12, 12, 12, 12, 11,\n",
      "        12, 12, 12, 37, 12, 12, 12, 11,  9, 11], device='cuda:0')\n",
      "Input:<exp(2.0*x1)________________________________________________________________________________________\n",
      "Logit:0xp(2.0*x1)>ss0s.s00e000001001101111111111111111e11110111011001e1011011e1011100.1010111110111e1110.0\n",
      "Target:tensor([37, 50, 45,  3, 13,  9, 11,  5, 50, 12,  4, 23, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34], device='cuda:0')\n",
      "Target:exp(2.0*x1)>________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 7 iter 781: train loss 0.53263. lr 3.012070e-04: 100%|█████████████████████████| 782/782 [05:38<00:00,  2.31it/s]\n",
      "05/25/2021 13:56:05 - INFO - mingpt.trainer -   test loss: 0.626108\n",
      "epoch 8 iter 781: train loss 0.54351. lr 5.999965e-04: 100%|█████████████████████████| 782/782 [05:39<00:00,  2.30it/s]\n",
      "05/25/2021 14:01:47 - INFO - mingpt.trainer -   test loss: 0.635318\n",
      "epoch 9 iter 781: train loss 0.46786. lr 2.983102e-04: 100%|█████████████████████████| 782/782 [05:39<00:00,  2.30it/s]\n",
      "05/25/2021 14:07:28 - INFO - mingpt.trainer -   test loss: 0.548872\n",
      "epoch 10 iter 706: train loss 0.44086. lr 6.000000e-05:  90%|█████████████████████▋  | 706/782 [05:04<00:33,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:tensor([22, 13,  9, 11,  5, 50, 12,  6, 11,  9, 19, 12, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34], device='cuda:0')\n",
      "Logit:tensor([13,  9, 11,  5, 50, 12,  6, 11,  9, 18, 23, 23, 23, 37, 23, 23, 23, 37,\n",
      "        13, 13, 13, 11, 13, 13, 13,  9, 13, 13, 13, 12, 12, 12, 13, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12, 12,  8, 12, 12,  8, 12, 12, 23, 37, 12,  8, 12,\n",
      "        12, 12, 12, 12, 12, 12,  8, 12, 12, 12, 12, 12, 12,  8, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12, 11, 12, 12, 12,  8, 12,  8, 12, 12, 12, 12,  8,\n",
      "        12, 12, 12, 11, 12, 12, 12,  8,  8, 12], device='cuda:0')\n",
      "Input:<2.0*x1+0.81________________________________________________________________________________________\n",
      "Logit:2.0*x1+0.7>>>e>>>e2220222.22211121111111111-11-11>e1-1111111-111111-11111111110111-1-1111-1110111--1\n",
      "Target:tensor([13,  9, 11,  5, 50, 12,  6, 11,  9, 19, 12, 23, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34], device='cuda:0')\n",
      "Target:2.0*x1+0.81>________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 10 iter 781: train loss 0.63054. lr 6.000000e-05: 100%|████████████████████████| 782/782 [05:36<00:00,  2.33it/s]\n",
      "05/25/2021 14:13:07 - INFO - mingpt.trainer -   test loss: 0.521670\n",
      "epoch 11 iter 781: train loss 0.59192. lr 3.021726e-04: 100%|████████████████████████| 782/782 [05:32<00:00,  2.35it/s]\n",
      "05/25/2021 14:18:41 - INFO - mingpt.trainer -   test loss: 0.590133\n",
      "epoch 12 iter 781: train loss 0.46845. lr 5.999903e-04: 100%|████████████████████████| 782/782 [05:34<00:00,  2.34it/s]\n",
      "05/25/2021 14:24:17 - INFO - mingpt.trainer -   test loss: 0.633948\n",
      "epoch 13 iter 781: train loss 0.51876. lr 2.973446e-04: 100%|████████████████████████| 782/782 [05:32<00:00,  2.35it/s]\n",
      "05/25/2021 14:29:51 - INFO - mingpt.trainer -   test loss: 0.514194\n",
      "epoch 14 iter 689: train loss 0.48257. lr 6.000000e-05:  88%|█████████████████████▏  | 689/782 [04:58<00:39,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:tensor([22,  8, 12,  9, 15, 17, 20, 17,  5, 50, 12,  5, 50, 13,  5, 37, 50, 45,\n",
      "         3, 12,  9, 11,  5, 50, 12,  4,  6, 11,  9, 16,  5, 50, 12,  6, 11,  9,\n",
      "        11, 18,  5, 50, 13,  8, 11,  9, 14, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34], device='cuda:0')\n",
      "Logit:tensor([ 8, 11,  9, 11,  5,  5,  5,  5, 50, 12,  5,  5, 13,  5, 37, 50, 45,  3,\n",
      "        12,  9, 11,  5, 50, 12,  4, 23, 11,  9, 20, 20, 50, 12, 23, 11,  9, 11,\n",
      "        13,  5, 50, 13, 23, 11,  9, 11, 19, 23, 23, 23, 13, 23, 23, 23, 50, 50,\n",
      "        50, 37,  8,  8, 50, 50,  8, 50, 50, 50,  8,  8,  8,  8, 50,  8, 50,  8,\n",
      "        11,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8, 50,  8, 50,  8,\n",
      "         8, 50, 11,  8,  8,  8,  8, 11,  8, 11], device='cuda:0')\n",
      "Input:<-1.4696*x1*x2*exp(1.0*x1)+0.5*x1+0.07*x2-0.3_______________________________________________________\n",
      "Logit:-0.0****x1**2*exp(1.0*x1)>0.99x1>0.02*x2>0.08>>>2>>>xxxe--xx-xxx----x-x-0-------------x-x--x0----0-0\n",
      "Target:tensor([ 8, 12,  9, 15, 17, 20, 17,  5, 50, 12,  5, 50, 13,  5, 37, 50, 45,  3,\n",
      "        12,  9, 11,  5, 50, 12,  4,  6, 11,  9, 16,  5, 50, 12,  6, 11,  9, 11,\n",
      "        18,  5, 50, 13,  8, 11,  9, 14, 23, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34], device='cuda:0')\n",
      "Target:-1.4696*x1*x2*exp(1.0*x1)+0.5*x1+0.07*x2-0.3>_______________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 14 iter 781: train loss 0.45027. lr 6.000000e-05: 100%|████████████████████████| 782/782 [05:37<00:00,  2.32it/s]\n",
      "05/25/2021 14:35:31 - INFO - mingpt.trainer -   test loss: 0.682989\n",
      "epoch 15 iter 781: train loss 0.48382. lr 3.031381e-04: 100%|████████████████████████| 782/782 [05:32<00:00,  2.35it/s]\n",
      "05/25/2021 14:41:05 - INFO - mingpt.trainer -   test loss: 0.477715\n",
      "epoch 16 iter 683: train loss 0.49194. lr 5.949165e-04:  87%|████████████████████▉   | 684/782 [04:50<00:41,  2.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:tensor([22,  8, 11,  9, 14, 14,  5, 50, 12,  5,  5, 14,  8, 11,  9, 16, 20,  5,\n",
      "        50, 12,  6, 37, 50, 45,  3, 12,  9, 11,  5, 50, 12,  4,  5, 42, 44, 39,\n",
      "         3, 35, 36, 48,  3, 12,  9, 11,  5, 50, 12,  6, 11,  9, 18, 16,  4,  4,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34], device='cuda:0')\n",
      "Logit:tensor([12, 11,  9, 13, 13,  5, 50, 12,  5,  5, 13,  6, 11,  9, 15, 15,  5, 50,\n",
      "        12,  5, 11, 50, 45,  3, 12,  9, 11,  5, 50, 12,  4, 23, 42, 44, 39,  3,\n",
      "        35, 36, 48,  3, 12,  9, 11,  5, 50, 12,  6, 11,  9, 16, 17,  4,  4, 23,\n",
      "         4, 11, 11,  9, 12, 12, 12, 12, 12, 12, 12, 23, 12,  5,  5,  5, 23, 12,\n",
      "        12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 11, 12, 12, 12, 11, 11, 12, 11,\n",
      "        12, 11, 12, 37, 11, 11, 12, 11, 12, 12], device='cuda:0')\n",
      "Input:<-0.33*x1**3-0.59*x1+exp(1.0*x1)*log(abs(1.0*x1+0.75))______________________________________________\n",
      "Logit:10.22*x1**2+0.44*x1*0xp(1.0*x1)>log(abs(1.0*x1+0.56))>)00.1111111>1***>1111111111101110010101e001011\n",
      "Target:tensor([ 8, 11,  9, 14, 14,  5, 50, 12,  5,  5, 14,  8, 11,  9, 16, 20,  5, 50,\n",
      "        12,  6, 37, 50, 45,  3, 12,  9, 11,  5, 50, 12,  4,  5, 42, 44, 39,  3,\n",
      "        35, 36, 48,  3, 12,  9, 11,  5, 50, 12,  6, 11,  9, 18, 16,  4,  4, 23,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34], device='cuda:0')\n",
      "Target:-0.33*x1**3-0.59*x1+exp(1.0*x1)*log(abs(1.0*x1+0.75))>______________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 16 iter 781: train loss 0.58726. lr 5.999810e-04: 100%|████████████████████████| 782/782 [05:33<00:00,  2.35it/s]\n",
      "05/25/2021 14:46:40 - INFO - mingpt.trainer -   test loss: 0.638250\n",
      "epoch 17 iter 85: train loss 0.46340. lr 5.949211e-04:  11%|██▊                       | 86/782 [00:38<05:07,  2.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:tensor([22, 48, 46, 47, 49,  3, 35, 36, 48,  3, 12,  9, 11,  5, 50, 12,  4,  4,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34], device='cuda:0')\n",
      "Logit:tensor([48, 46, 47, 49,  3, 35, 36, 48,  3, 12,  9, 11,  5, 50, 12,  4,  4, 23,\n",
      "        48, 48, 48, 48, 48, 48, 48, 48, 48,  5, 48, 48, 48, 48, 48, 48, 48, 48,\n",
      "        48, 37, 48, 48, 12, 48, 11, 48, 37, 48,  5, 12, 50, 11, 37,  5, 12, 50,\n",
      "        11, 37, 12, 12, 12, 12, 48, 12, 12, 12, 48, 12, 12, 12, 12, 35, 35, 50,\n",
      "         8, 37, 12, 12, 48, 12, 12,  8, 12, 12, 48,  8,  8, 48, 48, 12, 48, 12,\n",
      "        35, 12, 35, 48, 48, 11, 48, 48, 48, 48], device='cuda:0')\n",
      "Input:<sqrt(abs(1.0*x1))__________________________________________________________________________________\n",
      "Logit:sqrt(abs(1.0*x1))>sssssssss*sssssssssess1s0ses*1x0e*1x0e1111s111s1111aax-e11s11-11s--ss1s1a1ass0ssss\n",
      "Target:tensor([48, 46, 47, 49,  3, 35, 36, 48,  3, 12,  9, 11,  5, 50, 12,  4,  4, 23,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34], device='cuda:0')\n",
      "Target:sqrt(abs(1.0*x1))>__________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 17 iter 781: train loss 0.41815. lr 2.963791e-04: 100%|████████████████████████| 782/782 [05:35<00:00,  2.33it/s]\n",
      "05/25/2021 14:52:18 - INFO - mingpt.trainer -   test loss: 0.498025\n",
      "epoch 18 iter 781: train loss 0.51555. lr 6.000000e-05: 100%|████████████████████████| 782/782 [05:32<00:00,  2.35it/s]\n",
      "05/25/2021 14:57:52 - INFO - mingpt.trainer -   test loss: 0.596027\n",
      "epoch 19 iter 781: train loss 0.59902. lr 3.041037e-04: 100%|████████████████████████| 782/782 [05:34<00:00,  2.34it/s]\n",
      "05/25/2021 15:03:28 - INFO - mingpt.trainer -   test loss: 0.624981\n",
      "epoch 20 iter 280: train loss 0.47540. lr 4.641457e-04:  36%|████████▌               | 281/782 [02:00<03:38,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:tensor([22, 12,  9, 11,  5, 50, 12,  6, 37, 50, 45,  3, 12,  9, 11,  5, 50, 12,\n",
      "         4, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34], device='cuda:0')\n",
      "Logit:tensor([12,  9, 11,  5, 50, 12,  6, 37, 50, 45,  3, 12,  9, 11,  5, 50, 12,  4,\n",
      "        23, 11,  6, 11, 23, 11, 11, 11, 11, 12, 11, 11, 12, 12, 12,  5,  5, 37,\n",
      "        12, 12,  5,  5, 37, 12, 37, 12, 12, 37, 50, 12, 50, 12, 12, 37, 12, 11,\n",
      "        12, 11, 11,  5, 12, 12, 11, 11, 11, 11, 12, 12, 11,  8, 11,  4, 11, 11,\n",
      "        11, 11,  4, 11, 12, 11,  4, 12, 11, 11, 11, 11, 11, 11, 11, 12, 11,  4,\n",
      "         4, 12, 11, 11, 11, 11, 11, 11, 11,  4], device='cuda:0')\n",
      "Input:<1.0*x1+exp(1.0*x1)_________________________________________________________________________________\n",
      "Logit:1.0*x1+exp(1.0*x1)>0+0>0000100111**e11**e1e11ex1x11e10100*110000110-0)0000)010)1000000010))10000000)\n",
      "Target:tensor([12,  9, 11,  5, 50, 12,  6, 37, 50, 45,  3, 12,  9, 11,  5, 50, 12,  4,\n",
      "        23, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34], device='cuda:0')\n",
      "Target:1.0*x1+exp(1.0*x1)>_________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 20 iter 402: train loss 0.47646. lr 5.202385e-04:  52%|████████████▎           | 403/782 [02:53<02:40,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:tensor([22, 11,  9, 14, 19,  5, 50, 12,  5,  5, 13,  6, 12,  9, 11, 19,  5, 50,\n",
      "        12, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34], device='cuda:0')\n",
      "Logit:tensor([12,  9, 19, 19,  5, 50, 12,  5,  5, 13,  6, 11,  9, 11,  5,  5, 50, 12,\n",
      "        23, 11, 11, 23, 13, 11, 37,  9,  5,  5, 11, 11, 50,  5,  9, 50,  5, 50,\n",
      "        50, 11, 50, 12, 50, 50, 50, 11, 50, 50, 11, 11, 50,  9, 50, 50, 50, 50,\n",
      "        37, 11,  9,  9, 50,  9,  9, 50, 50, 50,  9,  5, 11, 50,  5, 50, 50, 50,\n",
      "        50, 50,  9, 50, 11, 50, 50, 11, 11,  9, 50, 11, 11, 50, 11, 11, 11, 11,\n",
      "        11, 11, 11, 11, 11, 11, 11, 11, 11, 11], device='cuda:0')\n",
      "Input:<0.38*x1**2+1.08*x1_________________________________________________________________________________\n",
      "Logit:1.88*x1**2+0.0**x1>00>20e.**00x*.x*xx0x1xxx0xx00x.xxxxe0..x..xxx.*0x*xxxxx.x0xx00.x00x00000000000000\n",
      "Target:tensor([11,  9, 14, 19,  5, 50, 12,  5,  5, 13,  6, 12,  9, 11, 19,  5, 50, 12,\n",
      "        23, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34], device='cuda:0')\n",
      "Target:0.38*x1**2+1.08*x1>_________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 20 iter 781: train loss 0.54267. lr 5.999685e-04: 100%|████████████████████████| 782/782 [05:34<00:00,  2.34it/s]\n",
      "05/25/2021 15:09:05 - INFO - mingpt.trainer -   test loss: 0.650361\n",
      "epoch 21 iter 781: train loss 0.51494. lr 2.954136e-04: 100%|████████████████████████| 782/782 [05:33<00:00,  2.35it/s]\n",
      "05/25/2021 15:14:40 - INFO - mingpt.trainer -   test loss: 0.560184\n",
      "epoch 22 iter 680: train loss 0.40326. lr 6.000000e-05:  87%|████████████████████▊   | 680/782 [04:49<00:43,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:tensor([22, 12,  9, 11,  5, 50, 12,  5,  5, 13,  5, 48, 40, 43,  3, 12,  9, 11,\n",
      "         5, 50, 12,  8, 11,  9, 11, 18,  4,  6, 11,  9, 17, 17,  5, 50, 12, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34], device='cuda:0')\n",
      "Logit:tensor([12,  9, 11,  5, 50, 12,  5,  5, 13,  5, 48, 40, 43,  3, 12,  9, 11,  5,\n",
      "        50, 12,  4, 11,  9, 11, 13,  4,  6, 11,  9, 15, 17,  5, 50, 12, 23, 23,\n",
      "        23, 11, 23, 23, 23, 12, 11, 23, 37, 37, 11, 11, 50, 11, 11, 11, 11, 23,\n",
      "        50, 23, 23, 23, 23, 11, 11, 11, 11, 23, 23, 23, 23, 11, 23, 11, 23, 11,\n",
      "        11, 11, 11, 11, 43, 11, 23,  4, 11, 47, 11, 47, 23, 11, 11, 47,  4, 23,\n",
      "        11, 11, 11, 11, 23, 11, 11, 11,  9, 23], device='cuda:0')\n",
      "Input:<1.0*x1**2*sin(1.0*x1-0.07)+0.66*x1_________________________________________________________________\n",
      "Logit:1.0*x1**2*sin(1.0*x1)0.02)+0.46*x1>>>0>>>10>ee00x0000>x>>>>0000>>>>0>0>00000n0>)0r0r>00r)>0000>000.>\n",
      "Target:tensor([12,  9, 11,  5, 50, 12,  5,  5, 13,  5, 48, 40, 43,  3, 12,  9, 11,  5,\n",
      "        50, 12,  8, 11,  9, 11, 18,  4,  6, 11,  9, 17, 17,  5, 50, 12, 23, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34], device='cuda:0')\n",
      "Target:1.0*x1**2*sin(1.0*x1-0.07)+0.66*x1>_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 22 iter 781: train loss 0.46703. lr 6.000000e-05: 100%|████████████████████████| 782/782 [05:32<00:00,  2.35it/s]\n",
      "05/25/2021 15:20:14 - INFO - mingpt.trainer -   test loss: 0.542413\n",
      "epoch 23 iter 781: train loss 0.45964. lr 3.050692e-04: 100%|████████████████████████| 782/782 [05:33<00:00,  2.35it/s]\n",
      "05/25/2021 15:25:49 - INFO - mingpt.trainer -   test loss: 0.487644\n",
      "epoch 24 iter 599: train loss 0.47593. lr 5.821297e-04:  77%|██████████████████▍     | 599/782 [04:15<01:18,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:tensor([22, 48, 46, 47, 49,  3, 35, 36, 48,  3, 11,  9, 13, 18,  5, 50, 12,  5,\n",
      "        50, 13,  8, 11,  9, 13, 16,  5, 50, 12,  4,  4,  5, 48, 40, 43,  3, 12,\n",
      "         9, 11,  5, 50, 13,  6, 11,  9, 15,  4, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34], device='cuda:0')\n",
      "Logit:tensor([48, 40, 47, 49,  3, 35, 36, 48,  3,  8,  9, 13, 18,  5, 50, 12,  5, 50,\n",
      "        13,  4, 11,  9, 11, 13,  5, 50, 12,  4,  4,  5, 48, 40, 43,  3, 12,  9,\n",
      "        11,  5, 50, 13,  4, 11,  9, 12, 13, 23,  5, 23, 23, 23, 23, 23, 11, 11,\n",
      "        13, 11, 23, 12, 11,  8, 11, 12, 50, 37, 12, 23,  8,  8, 11, 11, 11, 11,\n",
      "        12, 50,  8, 11, 11, 11,  8,  8, 11, 11, 12, 11,  8,  8, 23,  8,  8,  8,\n",
      "         8,  5,  8, 11, 11,  8,  8,  8, 11, 11], device='cuda:0')\n",
      "Input:<sqrt(abs(0.27*x1*x2-0.25*x1))*sin(1.0*x2+0.4)______________________________________________________\n",
      "Logit:sirt(abs(-.27*x1*x2)0.02*x1))*sin(1.0*x2)0.12>*>>>>>0020>10-01xe1>--00001x-000--0010-->----*-00---00\n",
      "Target:tensor([48, 46, 47, 49,  3, 35, 36, 48,  3, 11,  9, 13, 18,  5, 50, 12,  5, 50,\n",
      "        13,  8, 11,  9, 13, 16,  5, 50, 12,  4,  4,  5, 48, 40, 43,  3, 12,  9,\n",
      "        11,  5, 50, 13,  6, 11,  9, 15,  4, 23, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34], device='cuda:0')\n",
      "Target:sqrt(abs(0.27*x1*x2-0.25*x1))*sin(1.0*x2+0.4)>______________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 24 iter 781: train loss 0.64696. lr 5.999530e-04: 100%|████████████████████████| 782/782 [05:32<00:00,  2.35it/s]\n",
      "05/25/2021 15:31:24 - INFO - mingpt.trainer -   test loss: 0.490838\n",
      "epoch 25 iter 781: train loss 0.47276. lr 2.944481e-04: 100%|████████████████████████| 782/782 [05:34<00:00,  2.34it/s]\n",
      "05/25/2021 15:37:00 - INFO - mingpt.trainer -   test loss: 0.592881\n",
      "epoch 26 iter 781: train loss 0.50367. lr 6.000000e-05: 100%|████████████████████████| 782/782 [05:33<00:00,  2.35it/s]\n",
      "05/25/2021 15:42:35 - INFO - mingpt.trainer -   test loss: 0.711604\n",
      "epoch 27 iter 83: train loss 0.43661. lr 6.000000e-05:  11%|██▊                       | 83/782 [00:35<04:54,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:tensor([22, 42, 44, 39,  3, 35, 36, 48,  3, 12,  9, 11,  5, 50, 12,  8, 11,  9,\n",
      "        12, 18,  4,  4,  5, 48, 40, 43,  3, 12,  9, 11,  5, 50, 13,  8, 11,  9,\n",
      "        15,  4, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34], device='cuda:0')\n",
      "Logit:tensor([48, 44, 39,  3, 35, 36, 48,  3, 12,  9, 11,  5, 50, 12,  4, 11,  9, 14,\n",
      "        14,  4,  4,  5, 48, 40, 43,  3, 12,  9, 11,  5, 50, 13,  4, 11,  9, 13,\n",
      "         4, 23, 48, 48, 23, 48, 23, 48, 48, 23, 37, 37, 37, 12, 37, 37, 37, 37,\n",
      "        37, 37,  3, 37, 12, 13, 11, 11, 50, 11, 11, 12, 11, 11, 50, 14, 50, 11,\n",
      "        11, 50, 11, 50, 11, 11, 50, 50, 11, 11, 50, 11, 11, 11, 11, 40, 40, 47,\n",
      "        11, 23, 11, 11, 11, 11, 11, 11, 50, 11], device='cuda:0')\n",
      "Input:<log(abs(1.0*x1-0.17))*sin(1.0*x2-0.4)______________________________________________________________\n",
      "Logit:sog(abs(1.0*x1)0.33))*sin(1.0*x2)0.2)>ss>s>ss>eee1eeeeee(e1200x00100x3x00x0x00xx00x0000iir0>000000x0\n",
      "Target:tensor([42, 44, 39,  3, 35, 36, 48,  3, 12,  9, 11,  5, 50, 12,  8, 11,  9, 12,\n",
      "        18,  4,  4,  5, 48, 40, 43,  3, 12,  9, 11,  5, 50, 13,  8, 11,  9, 15,\n",
      "         4, 23, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34], device='cuda:0')\n",
      "Target:log(abs(1.0*x1-0.17))*sin(1.0*x2-0.4)>______________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 27 iter 781: train loss 0.86911. lr 3.060346e-04: 100%|████████████████████████| 782/782 [05:33<00:00,  2.34it/s]\n",
      "05/25/2021 15:48:10 - INFO - mingpt.trainer -   test loss: 0.735893\n",
      "epoch 28 iter 781: train loss 0.47839. lr 5.999343e-04: 100%|████████████████████████| 782/782 [05:33<00:00,  2.35it/s]\n",
      "05/25/2021 15:53:46 - INFO - mingpt.trainer -   test loss: 0.510050\n",
      "epoch 29 iter 193: train loss 0.48542. lr 5.749943e-04:  25%|█████▉                  | 194/782 [01:22<04:10,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:tensor([22, 48, 46, 47, 49,  3, 35, 36, 48,  3, 11,  9, 17,  5, 50, 12,  5,  5,\n",
      "        14,  6, 11,  9, 17, 18,  5, 50, 12,  8, 11,  9, 12, 15,  4,  4, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34], device='cuda:0')\n",
      "Logit:tensor([48, 46, 47, 49,  3, 35, 36, 48,  3, 11,  9, 15, 12, 50, 12,  5,  5, 14,\n",
      "         6, 11,  9, 13, 12,  5, 50, 12,  4, 11,  9, 11,  4,  4,  4, 23, 48, 48,\n",
      "        48, 48, 13, 11, 13, 13, 11, 11, 13, 13, 13, 13, 13, 13, 13, 37, 37, 37,\n",
      "        11, 37,  3, 37, 37, 50, 37, 37, 11, 37, 37, 11,  3, 37, 37,  8, 50, 37,\n",
      "        50, 37, 11, 50, 11, 50, 50, 50, 50, 11, 11, 11, 11,  9, 11,  9, 11, 11,\n",
      "        11, 11, 11, 48, 35, 48, 11, 11, 11, 11], device='cuda:0')\n",
      "Input:<sqrt(abs(0.6*x1**3+0.67*x1-0.14))__________________________________________________________________\n",
      "Logit:sqrt(abs(0.41x1**3+0.21*x1)0.0)))>ssss2022002222222eee0e(eexee0ee0(ee-xexe0x0xxxx0000.0.00000sas0000\n",
      "Target:tensor([48, 46, 47, 49,  3, 35, 36, 48,  3, 11,  9, 17,  5, 50, 12,  5,  5, 14,\n",
      "         6, 11,  9, 17, 18,  5, 50, 12,  8, 11,  9, 12, 15,  4,  4, 23, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34], device='cuda:0')\n",
      "Target:sqrt(abs(0.6*x1**3+0.67*x1-0.14))>__________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 29 iter 409: train loss 0.43079. lr 4.989610e-04:  52%|████████████▌           | 410/782 [02:54<02:38,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:tensor([22, 48, 46, 47, 49,  3, 35, 36, 48,  3,  8, 11,  9, 13, 17,  5, 50, 12,\n",
      "         5,  5, 13,  8, 11,  9, 15, 20,  5, 50, 12,  4,  4,  5, 48, 40, 43,  3,\n",
      "        12,  9, 11,  5, 50, 12,  4, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34], device='cuda:0')\n",
      "Logit:tensor([48, 46, 47, 49,  3, 35, 36, 48,  3, 11, 11,  9, 16, 20,  5, 50, 12,  5,\n",
      "         5, 13,  8, 11,  9, 14, 12,  5, 50, 12,  4,  4,  5, 48, 40, 43,  3, 12,\n",
      "         9, 11,  5, 50, 12,  4, 23, 23, 48, 23, 48, 13,  3, 23,  3,  3, 50, 13,\n",
      "         3,  3,  3,  3,  3,  9,  3,  3,  3,  3,  3,  9, 23,  3, 23, 23,  3, 23,\n",
      "        23,  3,  9,  9,  3, 23, 23, 23, 50, 23,  3,  3, 23, 23, 43,  9,  9,  9,\n",
      "        23,  3,  9, 23,  3,  3,  3,  9,  9,  3], device='cuda:0')\n",
      "Input:<sqrt(abs(-0.26*x1**2-0.49*x1))*sin(1.0*x1)_________________________________________________________\n",
      "Logit:sqrt(abs(00.59*x1**2-0.31*x1))*sin(1.0*x1)>>s>s2(>((x2(((((.(((((.>(>>(>>(..(>>>x>((>>n...>(.>(((..(\n",
      "Target:tensor([48, 46, 47, 49,  3, 35, 36, 48,  3,  8, 11,  9, 13, 17,  5, 50, 12,  5,\n",
      "         5, 13,  8, 11,  9, 15, 20,  5, 50, 12,  4,  4,  5, 48, 40, 43,  3, 12,\n",
      "         9, 11,  5, 50, 12,  4, 23, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34], device='cuda:0')\n",
      "Target:sqrt(abs(-0.26*x1**2-0.49*x1))*sin(1.0*x1)>_________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 29 iter 781: train loss 0.49863. lr 2.934827e-04: 100%|████████████████████████| 782/782 [05:33<00:00,  2.35it/s]\n",
      "05/25/2021 15:59:21 - INFO - mingpt.trainer -   test loss: 0.571359\n",
      "epoch 30 iter 140: train loss 0.38226. lr 2.098058e-04:  18%|████▎                   | 141/782 [00:59<04:33,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:tensor([22, 11,  9, 16, 17,  5, 50, 12,  5, 50, 13, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34], device='cuda:0')\n",
      "Logit:tensor([11,  9, 16, 14,  5, 50, 12,  5, 50, 13, 23, 48, 11, 46, 46, 46,  9, 11,\n",
      "        11, 11, 11,  8, 11, 11, 11,  8,  9, 11,  8,  8,  8,  9,  8,  9,  9, 11,\n",
      "         8, 50, 50,  9,  8,  9,  9,  9,  5, 43,  3, 50, 50,  3,  3,  9,  3,  3,\n",
      "         3,  9,  3,  3,  4, 50,  3,  3,  3,  3, 36,  3,  3,  3,  4,  3,  3, 50,\n",
      "         3,  3,  3, 43,  3,  3,  3, 49,  3, 36,  3,  3,  3, 50, 43,  3,  3,  9,\n",
      "         3,  3,  3,  3, 50,  3,  3,  3,  3,  3], device='cuda:0')\n",
      "Input:<0.56*x1*x2_________________________________________________________________________________________\n",
      "Logit:0.53*x1*x2>s0qqq.0000-000-.0---.-..0-xx.-...*n(xx((.(((.(()x((((b((()((x(((n(((t(b(((xn((.((((x(((((\n",
      "Target:tensor([11,  9, 16, 17,  5, 50, 12,  5, 50, 13, 23, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34], device='cuda:0')\n",
      "Target:0.56*x1*x2>_________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 30 iter 373: train loss 0.42437. lr 9.028022e-05:  48%|███████████▍            | 374/782 [02:39<02:55,  2.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:tensor([22, 11,  9, 15, 19,  5, 50, 12,  5,  5, 13,  5, 48, 40, 43,  3, 11,  9,\n",
      "        12, 20,  5, 50, 12,  4, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34], device='cuda:0')\n",
      "Logit:tensor([11,  9, 11, 12,  5, 50, 12,  5,  5, 13,  5, 48, 46, 43,  3, 11,  9, 12,\n",
      "        15,  5, 50, 12,  4, 23, 11, 11, 48, 11, 11, 11,  3, 11,  8,  8, 11, 11,\n",
      "         8, 11, 11, 11, 11, 11,  8, 11,  9,  3,  8,  8, 50, 11, 37,  3, 50, 36,\n",
      "         3, 50, 49,  4,  3, 50,  4,  4, 50,  4,  3, 43,  4,  3, 43,  4,  5, 50,\n",
      "        43,  3,  9, 43,  3,  3,  3, 43,  3,  3, 47,  3,  3, 36,  3, 43, 43,  3,\n",
      "         3,  3,  4, 43, 36, 36,  3, 43, 36,  3], device='cuda:0')\n",
      "Input:<0.48*x1**2*sin(0.19*x1)____________________________________________________________________________\n",
      "Logit:0.01*x1**2*sqn(0.14*x1)>00s000(0--00-00000-0.(--x0e(xb(xt)(x))x)(n)(n)*xn(.n(((n((r((b(nn((()nbb(nb(\n",
      "Target:tensor([11,  9, 15, 19,  5, 50, 12,  5,  5, 13,  5, 48, 40, 43,  3, 11,  9, 12,\n",
      "        20,  5, 50, 12,  4, 23, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34], device='cuda:0')\n",
      "Target:0.48*x1**2*sin(0.19*x1)>____________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 30 iter 513: train loss 0.39976. lr 6.000000e-05:  66%|███████████████▋        | 513/782 [03:38<01:52,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:tensor([22, 48, 40, 43,  3, 37, 50, 45,  3,  8, 11,  9, 17, 12,  5, 50, 13,  4,\n",
      "         5, 37, 50, 45,  3, 12,  9, 11,  5, 50, 12,  5, 50, 13,  4,  4, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34], device='cuda:0')\n",
      "Logit:tensor([48, 40, 43,  3, 12, 50, 45,  3, 12, 11,  9, 13, 16,  5, 50, 13,  4,  5,\n",
      "        37, 50, 45,  3, 12,  9, 11,  5, 50, 12,  5, 50, 13,  4,  4, 23, 48, 48,\n",
      "        48, 48, 23,  3, 10, 23, 23, 47, 11, 47, 50,  3, 12, 50, 12, 11, 50, 50,\n",
      "         9, 11, 11, 50,  9, 50, 17,  9, 11, 50, 50,  3,  9, 11,  9, 50,  9, 50,\n",
      "         9,  3,  3, 43, 40, 50,  3,  3, 43, 47,  3,  3,  3,  9,  3,  3, 11,  3,\n",
      "        35,  9,  3, 11,  3,  9, 43,  9,  3,  3], device='cuda:0')\n",
      "Input:<sin(exp(-0.61*x2)*exp(1.0*x1*x2))__________________________________________________________________\n",
      "Logit:sin(1xp(10.25*x2)*exp(1.0*x1*x2))>ssss>(/>>r0rx(1x10xx.00x.x6.0xx(.0.x.x.((nix((nr(((.((0(a.(0(.n.((\n",
      "Target:tensor([48, 40, 43,  3, 37, 50, 45,  3,  8, 11,  9, 17, 12,  5, 50, 13,  4,  5,\n",
      "        37, 50, 45,  3, 12,  9, 11,  5, 50, 12,  5, 50, 13,  4,  4, 23, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34], device='cuda:0')\n",
      "Target:sin(exp(-0.61*x2)*exp(1.0*x1*x2))>__________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 30 iter 781: train loss 0.74002. lr 6.000000e-05: 100%|████████████████████████| 782/782 [05:32<00:00,  2.35it/s]\n",
      "05/25/2021 16:04:55 - INFO - mingpt.trainer -   test loss: 0.483530\n",
      "epoch 31 iter 505: train loss 0.39030. lr 1.482466e-04:  65%|███████████████▌        | 506/782 [03:35<01:58,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:tensor([22, 12,  9, 11,  5, 50, 12,  5, 48, 46, 47, 49,  3, 35, 36, 48,  3, 12,\n",
      "         9, 11,  5, 50, 12,  4,  4,  5, 37, 50, 45,  3, 48, 40, 43,  3, 12,  9,\n",
      "        11,  5, 50, 12,  4,  4, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34], device='cuda:0')\n",
      "Logit:tensor([12,  9, 11,  5, 50, 12,  5,  5, 46, 47, 49,  3, 35, 36, 48,  3, 12,  9,\n",
      "        11,  5, 50, 12,  4,  4,  5, 37, 50, 45,  3, 48, 40, 43,  3, 12,  9, 11,\n",
      "         5, 50, 12,  4,  4, 23, 11, 11, 23, 23, 13,  4,  3, 23, 11,  3,  3, 46,\n",
      "        12, 43,  3, 12, 43, 43, 43,  3, 43,  3, 43, 43, 17, 50, 43, 47, 17, 46,\n",
      "        43,  3, 43,  3,  3, 43, 43, 43, 43, 43, 43,  3, 49,  9, 43, 43,  3,  3,\n",
      "         3, 49, 43, 43, 43, 43, 43,  3, 49, 47], device='cuda:0')\n",
      "Input:<1.0*x1*sqrt(abs(1.0*x1))*exp(sin(1.0*x1))__________________________________________________________\n",
      "Logit:1.0*x1**qrt(abs(1.0*x1))*exp(sin(1.0*x1))>00>>2)(>0((q1n(1nnn(n(nn6xnr6qn(n((nnnnnn(t.nn(((tnnnnn(tr\n",
      "Target:tensor([12,  9, 11,  5, 50, 12,  5, 48, 46, 47, 49,  3, 35, 36, 48,  3, 12,  9,\n",
      "        11,  5, 50, 12,  4,  4,  5, 37, 50, 45,  3, 48, 40, 43,  3, 12,  9, 11,\n",
      "         5, 50, 12,  4,  4, 23, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34], device='cuda:0')\n",
      "Target:1.0*x1*sqrt(abs(1.0*x1))*exp(sin(1.0*x1))>__________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 31 iter 634: train loss 0.40229. lr 2.197332e-04:  81%|███████████████████▍    | 635/782 [04:30<01:02,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:tensor([22,  8, 11,  9, 15, 18, 12, 18,  5, 50, 12,  5,  5, 14,  5, 42, 44, 39,\n",
      "         3, 35, 36, 48,  3, 12,  9, 11,  5, 50, 12,  4,  4, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34], device='cuda:0')\n",
      "Logit:tensor([ 8, 11,  9, 12, 12,  5, 17,  5, 50, 12,  5,  5, 14,  5, 42, 44, 39,  3,\n",
      "        35, 36, 48,  3, 12,  9, 11,  5, 50, 12,  4,  4, 23, 11, 11, 11, 23, 11,\n",
      "        11, 11, 11, 11, 11, 11, 11, 11, 11,  8, 11,  8, 11, 11, 11, 11, 11, 11,\n",
      "        11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 49,  3,  3, 11,  8, 11, 11,\n",
      "        11, 49,  8, 47, 47,  4, 47,  3, 11, 11, 50,  3, 49, 11, 39,  3, 11,  8,\n",
      "        11, 11, 11,  3, 11, 11, 46, 47, 11, 11], device='cuda:0')\n",
      "Input:<-0.4717*x1**3*log(abs(1.0*x1))_____________________________________________________________________\n",
      "Logit:-0.11*6*x1**3*log(abs(1.0*x1))>000>0000000000-0-00000000000000000t((0-000t-rr)r(00x(t0g(0-000(00qr00\n",
      "Target:tensor([ 8, 11,  9, 15, 18, 12, 18,  5, 50, 12,  5,  5, 14,  5, 42, 44, 39,  3,\n",
      "        35, 36, 48,  3, 12,  9, 11,  5, 50, 12,  4,  4, 23, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34], device='cuda:0')\n",
      "Target:-0.4717*x1**3*log(abs(1.0*x1))>_____________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 31 iter 646: train loss 0.44556. lr 2.267339e-04:  83%|███████████████████▊    | 646/782 [04:35<00:58,  2.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:tensor([22,  8, 48, 40, 43,  3, 11,  9, 20, 18,  5, 50, 12,  5,  5, 13,  5, 48,\n",
      "        40, 43,  3, 12,  9, 11,  5, 50, 12,  4,  4, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34], device='cuda:0')\n",
      "Logit:tensor([ 8, 48, 40, 43,  3, 11,  9, 17, 12,  5, 50, 12,  5,  5, 14,  5, 48, 40,\n",
      "        43,  3, 12,  9, 11,  5, 50, 12,  4,  4, 23, 12, 12, 12, 12, 12, 11, 12,\n",
      "        12, 11, 11, 11, 50, 11, 12, 11, 12, 12, 11, 12, 11, 12, 12, 11, 12, 11,\n",
      "        11, 11,  9, 12, 11, 50,  9, 12,  5, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "        50, 43,  3, 50,  3, 50, 50,  3, 47,  3, 50,  3, 50, 43,  3, 43,  3,  3,\n",
      "         3,  9,  3,  3, 43,  3,  9,  3,  3, 46], device='cuda:0')\n",
      "Input:<-sin(0.97*x1**2*sin(1.0*x1))_______________________________________________________________________\n",
      "Logit:-sin(0.61*x1**3*sin(1.0*x1))>11111011000x010110101101000.10x.1*xxxxxxxxxxn(x(xx(r(x(xn(n(((.((n(.((q\n",
      "Target:tensor([ 8, 48, 40, 43,  3, 11,  9, 20, 18,  5, 50, 12,  5,  5, 13,  5, 48, 40,\n",
      "        43,  3, 12,  9, 11,  5, 50, 12,  4,  4, 23, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34], device='cuda:0')\n",
      "Target:-sin(0.97*x1**2*sin(1.0*x1))>_______________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 31 iter 781: train loss 0.58616. lr 3.070000e-04: 100%|████████████████████████| 782/782 [05:32<00:00,  2.35it/s]\n",
      "05/25/2021 16:10:30 - INFO - mingpt.trainer -   test loss: 0.595725\n",
      "epoch 32 iter 781: train loss 0.49961. lr 5.999126e-04: 100%|████████████████████████| 782/782 [05:33<00:00,  2.35it/s]\n",
      "05/25/2021 16:16:05 - INFO - mingpt.trainer -   test loss: 0.526268\n",
      "epoch 33 iter 371: train loss 0.46316. lr 5.148544e-04:  47%|███████████▍            | 371/782 [02:38<02:53,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:tensor([22, 11,  9, 12, 17,  5, 50, 13,  5, 42, 44, 39,  3, 35, 36, 48,  3, 12,\n",
      "         9, 11,  5, 50, 12,  4,  4,  5, 42, 44, 39,  3, 35, 36, 48,  3, 11,  9,\n",
      "        12,  5, 50, 12,  5,  5, 13,  6, 11,  9, 11, 15,  5, 50, 12,  4,  4,  6,\n",
      "        11,  9, 13, 13, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34], device='cuda:0')\n",
      "Logit:tensor([ 8,  9, 12, 15,  5, 50, 13,  5, 42, 44, 39,  3, 35, 36, 48,  3, 12,  9,\n",
      "        11,  5, 50, 12,  4,  4,  8, 42, 44, 39,  3, 35, 36, 48,  3, 12,  9, 16,\n",
      "        15, 50, 12,  5,  5, 13,  6, 11,  9, 11, 13,  5, 50, 12,  4,  4, 23, 11,\n",
      "         9, 11, 12, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 50, 23, 23, 23, 23,\n",
      "        23, 23,  8, 23, 11, 23, 23, 23, 50, 23, 23, 23, 23, 23, 23, 23, 23, 23,\n",
      "        11,  8, 23,  8, 50,  8, 50,  8, 23, 11], device='cuda:0')\n",
      "Input:<0.16*x2*log(abs(1.0*x1))*log(abs(0.1*x1**2+0.04*x1))+0.22__________________________________________\n",
      "Logit:-.14*x2*log(abs(1.0*x1))-log(abs(1.54x1**2+0.02*x1))>0.01>>>>>>>>>>x>>>>>>->0>>>x>>>>>>>>>0->-x-x->0\n",
      "Target:tensor([11,  9, 12, 17,  5, 50, 13,  5, 42, 44, 39,  3, 35, 36, 48,  3, 12,  9,\n",
      "        11,  5, 50, 12,  4,  4,  5, 42, 44, 39,  3, 35, 36, 48,  3, 11,  9, 12,\n",
      "         5, 50, 12,  5,  5, 13,  6, 11,  9, 11, 15,  5, 50, 12,  4,  4,  6, 11,\n",
      "         9, 13, 13, 23, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34], device='cuda:0')\n",
      "Target:0.16*x2*log(abs(1.0*x1))*log(abs(0.1*x1**2+0.04*x1))+0.22>__________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 33 iter 781: train loss 0.42522. lr 2.925174e-04: 100%|████████████████████████| 782/782 [05:32<00:00,  2.35it/s]\n",
      "05/25/2021 16:21:39 - INFO - mingpt.trainer -   test loss: 0.462702\n",
      "epoch 34 iter 781: train loss 0.42051. lr 6.000000e-05: 100%|████████████████████████| 782/782 [05:35<00:00,  2.33it/s]\n",
      "05/25/2021 16:27:16 - INFO - mingpt.trainer -   test loss: 0.420951\n",
      "epoch 35 iter 781: train loss 0.41815. lr 3.079653e-04: 100%|████████████████████████| 782/782 [05:31<00:00,  2.36it/s]\n",
      "05/25/2021 16:32:50 - INFO - mingpt.trainer -   test loss: 0.440126\n",
      "epoch 36 iter 781: train loss 0.64579. lr 5.998877e-04: 100%|████████████████████████| 782/782 [05:31<00:00,  2.36it/s]\n",
      "05/25/2021 16:38:24 - INFO - mingpt.trainer -   test loss: 0.525728\n",
      "epoch 37 iter 308: train loss 0.43206. lr 5.390130e-04:  39%|█████████▍              | 308/782 [02:11<03:22,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:tensor([22, 11, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34], device='cuda:0')\n",
      "Logit:tensor([11, 23, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
      "        11, 11, 11,  5, 11, 11, 11, 11, 11,  5, 11,  5, 12, 11, 12, 37, 11,  5,\n",
      "         8,  5, 11,  8,  8, 12,  8, 11,  8, 11, 12,  8, 42, 11,  8, 42, 11, 11,\n",
      "        12, 11, 11, 11, 11, 11, 11, 11, 11,  8, 11, 11,  8, 11, 11, 11,  8, 11,\n",
      "        12, 11, 11, 11, 11, 12, 11,  8, 11, 11, 11,  8, 11, 11, 11, 11, 11, 11,\n",
      "        11, 11, 11, 11, 11, 11,  8, 11, 11, 11], device='cuda:0')\n",
      "Input:<0__________________________________________________________________________________________________\n",
      "Logit:0>0000000000000000000*00000*0*101e0*-*0--1-0-01-l0-l00100000000-00-000-01000010-000-000000000000-000\n",
      "Target:tensor([11, 23, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34], device='cuda:0')\n",
      "Target:0>__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 37 iter 781: train loss 0.71303. lr 2.915521e-04: 100%|████████████████████████| 782/782 [05:31<00:00,  2.36it/s]\n",
      "05/25/2021 16:43:57 - INFO - mingpt.trainer -   test loss: 0.488530\n",
      "epoch 38 iter 236: train loss 0.39516. lr 1.548764e-04:  30%|███████▏                | 236/782 [01:40<03:54,  2.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:tensor([22,  8, 11,  9, 11, 14,  5, 50, 12,  5,  5, 15, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34], device='cuda:0')\n",
      "Logit:tensor([ 8, 11,  9, 11, 14,  5, 50, 12,  5,  5, 15, 23, 11, 48, 11, 11, 11, 50,\n",
      "        11, 11, 11, 11, 11, 11, 37, 11, 11, 11, 48, 50, 11, 11, 11, 13, 11, 11,\n",
      "        11, 11, 48, 11, 11, 37, 11, 11,  3, 11, 11, 11, 11, 11, 50, 11, 50, 11,\n",
      "        11, 50, 11, 11, 11, 50, 11, 23, 11,  4,  4,  4,  8,  4,  4, 11, 11, 39,\n",
      "        11, 11,  9,  4, 11, 39,  4,  4, 11, 11,  4, 11,  4, 11, 11,  4, 44, 11,\n",
      "         4, 11,  4,  4,  8,  8,  8,  4, 11, 40], device='cuda:0')\n",
      "Input:<-0.03*x1**4________________________________________________________________________________________\n",
      "Logit:-0.03*x1**4>0s000x000000e000sx00020000s00e00(00000x0x00x000x0>0)))-))00g00.)0g))00)0)00)o0)0))---)0i\n",
      "Target:tensor([ 8, 11,  9, 11, 14,  5, 50, 12,  5,  5, 15, 23, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34], device='cuda:0')\n",
      "Target:-0.03*x1**4>________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 38 iter 781: train loss 0.43877. lr 6.000000e-05: 100%|████████████████████████| 782/782 [05:32<00:00,  2.35it/s]\n",
      "05/25/2021 16:49:32 - INFO - mingpt.trainer -   test loss: 0.421321\n",
      "epoch 39 iter 781: train loss 0.38238. lr 3.089305e-04: 100%|████████████████████████| 782/782 [05:32<00:00,  2.35it/s]\n",
      "05/25/2021 16:55:05 - INFO - mingpt.trainer -   test loss: 0.451768\n",
      "epoch 40 iter 781: train loss 0.49812. lr 5.998598e-04: 100%|████████████████████████| 782/782 [05:32<00:00,  2.35it/s]\n",
      "05/25/2021 17:00:40 - INFO - mingpt.trainer -   test loss: 0.496957\n",
      "epoch 41 iter 614: train loss 0.42328. lr 3.895276e-04:  79%|██████████████████▊     | 615/782 [04:22<01:11,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:tensor([22, 11,  9, 13, 13,  5, 50, 12,  5, 48, 46, 47, 49,  3, 35, 36, 48,  3,\n",
      "         8, 11,  9, 15, 19,  5, 50, 12,  4,  4, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34], device='cuda:0')\n",
      "Logit:tensor([11,  9, 13, 16,  5, 50, 12,  5, 48, 46, 47, 49,  3, 35, 36, 48,  3, 12,\n",
      "        11,  9, 17, 15,  5, 50, 12,  4,  4, 23, 48, 48, 37, 23, 37, 14, 37, 37,\n",
      "         5, 37, 37, 37, 37,  5,  5,  5,  5, 14,  5, 37, 50,  3, 50,  5, 50,  3,\n",
      "         5,  9,  3,  5,  9, 14, 50,  3, 14,  3,  5, 50,  3,  5,  3, 11,  4,  4,\n",
      "         4, 36,  4, 23,  3,  4,  4,  4, 36, 23,  4,  3,  3,  4,  4,  4,  4, 49,\n",
      "         4,  3,  3,  4, 46, 47,  4, 11, 49,  3], device='cuda:0')\n",
      "Input:<0.22*x1*sqrt(abs(-0.48*x1))________________________________________________________________________\n",
      "Logit:0.25*x1*sqrt(abs(10.64*x1))>sse>e3ee*eeee****3*ex(x*x(*.(*.3x(3(*x(*(0)))b)>()))b>)(())))t)(()qr)0t(\n",
      "Target:tensor([11,  9, 13, 13,  5, 50, 12,  5, 48, 46, 47, 49,  3, 35, 36, 48,  3,  8,\n",
      "        11,  9, 15, 19,  5, 50, 12,  4,  4, 23, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34], device='cuda:0')\n",
      "Target:0.22*x1*sqrt(abs(-0.48*x1))>________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 41 iter 781: train loss 0.45679. lr 2.905869e-04: 100%|████████████████████████| 782/782 [05:32<00:00,  2.35it/s]\n",
      "05/25/2021 17:06:14 - INFO - mingpt.trainer -   test loss: 0.614549\n",
      "epoch 42 iter 137: train loss 0.38788. lr 2.087703e-04:  18%|████▏                   | 138/782 [00:59<04:40,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:tensor([22, 11,  9, 12, 19,  5, 50, 12, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34], device='cuda:0')\n",
      "Logit:tensor([ 8,  9, 12, 15,  8, 50, 12, 23, 11, 48, 11, 11, 13, 11, 48, 50, 48, 12,\n",
      "        13, 48, 37, 12, 13, 12, 37, 12, 12, 12, 37, 12, 12, 35,  8, 12,  8, 48,\n",
      "        35, 50, 50, 12, 35, 46, 12, 50, 35,  8,  8, 35, 35, 50,  4, 39, 50,  4,\n",
      "         8, 50, 35, 50, 40, 35, 35,  3, 35, 35, 35,  4, 35,  4,  3,  3, 35, 44,\n",
      "        44, 44,  4,  3, 35,  4,  3, 44,  3,  4, 39, 44,  3, 35, 35,  4, 35, 49,\n",
      "        44, 44,  4, 35, 44,  4,  4, 47, 46, 35], device='cuda:0')\n",
      "Input:<0.18*x1____________________________________________________________________________________________\n",
      "Logit:-.14-x1>0s0020sxs12se121e111e11a-1-saxx1aq1xa--aax)gx)-xaxiaa(aaa)a)((aooo)(a)(o()go(aa)atoo)ao))rqa\n",
      "Target:tensor([11,  9, 12, 19,  5, 50, 12, 23, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34], device='cuda:0')\n",
      "Target:0.18*x1>____________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 42 iter 697: train loss 0.35151. lr 6.000000e-05:  89%|█████████████████████▍  | 698/782 [04:58<00:36,  2.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:tensor([22, 12,  9, 11,  5, 50, 12,  8, 11,  9, 12, 14, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34], device='cuda:0')\n",
      "Logit:tensor([12,  9, 11,  5, 50, 12,  8, 11,  9, 12, 15, 23, 13,  5, 23,  5, 50, 37,\n",
      "         8, 50, 50, 50, 11, 11, 11, 46, 11, 50, 13,  5, 37, 11, 11, 37,  8, 11,\n",
      "        11, 11, 11,  5, 46, 11, 11, 50, 46, 11, 37, 11, 13, 11, 12, 13, 37, 11,\n",
      "        20, 11, 44,  8,  9, 37,  3, 40, 39, 43, 40,  3, 43, 39, 43, 43, 44,  3,\n",
      "         3,  3, 43, 43, 43, 36, 43,  7, 43,  3, 50, 36,  3,  3, 43,  4, 43, 43,\n",
      "        49, 43, 49, 26, 43, 36,  4, 36,  3, 46], device='cuda:0')\n",
      "Input:<1.0*x1-0.13________________________________________________________________________________________\n",
      "Logit:1.0*x1-0.14>2*>*xe-xxx000q0x2*e00e-0000*q00xq0e02012e090o-.e(igni(ngnno(((nnnbn,n(xb((n)nntntInb)b(q\n",
      "Target:tensor([12,  9, 11,  5, 50, 12,  8, 11,  9, 12, 14, 23, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34], device='cuda:0')\n",
      "Target:1.0*x1-0.13>________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 42 iter 781: train loss 0.40568. lr 6.000000e-05: 100%|████████████████████████| 782/782 [05:34<00:00,  2.34it/s]\n",
      "05/25/2021 17:11:50 - INFO - mingpt.trainer -   test loss: 0.453766\n",
      "epoch 43 iter 629: train loss 0.36615. lr 2.196169e-04:  81%|███████████████████▎    | 630/782 [04:32<01:07,  2.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:tensor([22, 48, 46, 47, 49,  3, 35, 36, 48,  3, 13,  9, 11,  5, 50, 12,  5,  5,\n",
      "        13,  6, 11,  9, 20,  4,  4, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34], device='cuda:0')\n",
      "Logit:tensor([48, 46, 47, 49,  3, 35, 36, 48,  3, 12,  9, 11,  5, 50, 12,  5,  5, 13,\n",
      "         6, 11,  9, 16, 12,  4, 23, 48, 48, 48,  5, 48, 48, 48, 48, 48,  5, 48,\n",
      "        48, 48, 48, 37, 37, 11, 48, 37,  8, 37, 50, 12,  8,  5,  5,  5, 37, 11,\n",
      "        11, 11, 14, 16, 11, 11, 14, 11, 11,  9, 11, 11,  5, 23, 11, 23,  5,  4,\n",
      "        11, 20,  9, 23, 11, 11, 23,  4, 11, 11, 23, 14, 14, 23, 23, 23, 11,  4,\n",
      "        14,  5, 36, 23, 11,  4, 11, 11, 11, 11], device='cuda:0')\n",
      "Input:<sqrt(abs(2.0*x1**2+0.9))___________________________________________________________________________\n",
      "Logit:sqrt(abs(1.0*x1**2+0.51)>sss*sssss*ssssee0se-ex1-***e0003500300.00*>0>*)09.>00>)00>33>>>0)3*b>0)0000\n",
      "Target:tensor([48, 46, 47, 49,  3, 35, 36, 48,  3, 13,  9, 11,  5, 50, 12,  5,  5, 13,\n",
      "         6, 11,  9, 20,  4,  4, 23, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34], device='cuda:0')\n",
      "Target:sqrt(abs(2.0*x1**2+0.9))>___________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 43 iter 754: train loss 0.38727. lr 2.940606e-04:  97%|███████████████████████▏| 755/782 [05:27<00:12,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:tensor([22, 11,  9, 12, 15, 19, 15,  5, 50, 12,  5,  5, 13,  5, 50, 13,  5, 37,\n",
      "        50, 45,  3, 12,  9, 11,  5, 50, 13,  4,  8, 11,  9, 15, 14,  5, 50, 12,\n",
      "         5, 50, 13,  5, 48, 46, 47, 49,  3, 35, 36, 48,  3, 12,  9, 11,  5, 50,\n",
      "        12,  4,  4,  8, 11,  9, 17, 14,  5, 50, 13, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34], device='cuda:0')\n",
      "Logit:tensor([12,  9, 17, 13,  5,  5,  5, 50, 12,  5, 50, 13,  5, 50, 13,  5, 37, 50,\n",
      "        45,  3, 12,  9, 11,  5, 50, 13,  4,  6, 11,  9, 20, 15,  5, 50, 12,  5,\n",
      "        50, 13,  8,  5, 46, 47, 49,  3, 35, 36, 48,  3, 12,  9, 11,  5, 50, 12,\n",
      "         4,  4,  8, 11,  9, 20, 20,  5, 50, 13, 23, 11, 11, 11, 11, 11, 11, 11,\n",
      "        11, 11, 49,  8, 35,  3, 49, 11, 49, 11, 44, 49, 49, 49, 49, 49, 11, 49,\n",
      "        35, 44, 11, 49, 11, 49, 49, 23,  4, 49], device='cuda:0')\n",
      "Input:<0.1484*x1**2*x2*exp(1.0*x2)-0.43*x1*x2*sqrt(abs(1.0*x1))-0.63*x2___________________________________\n",
      "Logit:1.62***x1*x2*x2*exp(1.0*x2)+0.94*x1*x2-*qrt(abs(1.0*x1))-0.99*x2>000000000t-a(t0t0ottttt0tao0t0tt>)t\n",
      "Target:tensor([11,  9, 12, 15, 19, 15,  5, 50, 12,  5,  5, 13,  5, 50, 13,  5, 37, 50,\n",
      "        45,  3, 12,  9, 11,  5, 50, 13,  4,  8, 11,  9, 15, 14,  5, 50, 12,  5,\n",
      "        50, 13,  5, 48, 46, 47, 49,  3, 35, 36, 48,  3, 12,  9, 11,  5, 50, 12,\n",
      "         4,  4,  8, 11,  9, 17, 14,  5, 50, 13, 23, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34], device='cuda:0')\n",
      "Target:0.1484*x1**2*x2*exp(1.0*x2)-0.43*x1*x2*sqrt(abs(1.0*x1))-0.63*x2>___________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 43 iter 781: train loss 0.39772. lr 3.098956e-04: 100%|████████████████████████| 782/782 [05:39<00:00,  2.30it/s]\n",
      "05/25/2021 17:17:32 - INFO - mingpt.trainer -   test loss: 0.460700\n",
      "epoch 44 iter 497: train loss 0.42468. lr 5.579285e-04:  64%|███████████████▎        | 498/782 [03:34<02:02,  2.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from mingpt.trainer import Trainer, TrainerConfig\n",
    "import os\n",
    "\n",
    "try: \n",
    "    os.mkdir(addr)\n",
    "except:\n",
    "    print('Folder already exists!')\n",
    "    \n",
    "# initialize a trainer instance and kick off training\n",
    "tconf = TrainerConfig(max_epochs=numEpochs, batch_size=batchSize, learning_rate=6e-4,\n",
    "                      lr_decay=True, warmup_tokens=512*20, final_tokens=2*len(train_dataset)*blockSize,\n",
    "                      num_workers=0, ckpt_path=ckptPath)\n",
    "trainer = Trainer(model, train_dataset, val_dataset, tconf, bestLoss)\n",
    "\n",
    "try:\n",
    "    trainer.train()\n",
    "except KeyboardInterrupt:\n",
    "    print('KeyboardInterrupt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best model\n",
    "model.load_state_dict(torch.load(ckptPath))\n",
    "model = model.eval().to(trainer.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a safe wrapper for numpy math functions\n",
    "from numpy import *\n",
    "import numpy as np\n",
    "\n",
    "def divide(x, y):\n",
    "  x = np.nan_to_num(x)\n",
    "  y = np.nan_to_num(y)\n",
    "  return np.divide(x,y+1e-5)\n",
    "\n",
    "def sqrt(x):\n",
    "  x = np.nan_to_num(x)\n",
    "  return np.sqrt(np.abs(x)) \n",
    "\n",
    "# Mean square error\n",
    "def mse(y, y_hat):\n",
    "    y_hat = np.reshape(y_hat, [1, -1])[0]\n",
    "    y_gold = np.reshape(y, [1, -1])[0]\n",
    "    our_sum = 0\n",
    "    for i in range(len(y_gold)):\n",
    "        our_sum += (y_hat[i] - y_gold[i]) ** 2\n",
    "\n",
    "    return our_sum / len(y_gold)\n",
    "\n",
    "# Mean square error\n",
    "def relativeErr(y, y_hat):\n",
    "    y_hat = np.reshape(y_hat, [1, -1])[0]\n",
    "    y_gold = np.reshape(y, [1, -1])[0]\n",
    "    our_sum = 0\n",
    "    for i in range(len(y_gold)):\n",
    "        if y_gold[i] < 1: \n",
    "            # use regular MSE\n",
    "            our_sum += (y_hat[i] - y_gold[i]) ** 2\n",
    "        else:\n",
    "            # use relative MSE\n",
    "            our_sum += ((y_hat[i] - y_gold[i])/y_gold[i]) ** 2\n",
    "\n",
    "    return our_sum / len(y_gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class pointsDataset(Dataset):\n",
    "\n",
    "#     def __init__(self, data):\n",
    "#         # data should be a list of x,y pairs\n",
    "#         self.x = data[0] # it should be a list\n",
    "#         self.y = data[1] # it should be a list\n",
    "#         self.threshold = [-1000,1000]\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.y)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         # grab an example from the data\n",
    "#         x = self.x[idx] \n",
    "#         y = self.y[idx] \n",
    "        \n",
    "#         p = x+[y]\n",
    "        \n",
    "#         p = torch.tensor(p, dtype=torch.float)\n",
    "#         p = torch.nan_to_num(p, nan=0.0, \n",
    "#                              posinf=self.threshold[1], \n",
    "#                              neginf=self.threshold[0])\n",
    "#         p[p>self.threshold[1]] = self.threshold[1] # clip the upper bound\n",
    "#         p[p<self.threshold[0]] = self.threshold[0] # clip the lower bound\n",
    "        \n",
    "#         return p\n",
    "    \n",
    "# # train a mlp to find the constants\n",
    "# data = pointsDataset((t['X'],t['Y']))\n",
    "# loader = torch.utils.data.DataLoader(\n",
    "#                                 data, \n",
    "#                                 shuffle=False, \n",
    "#                                 pin_memory=True,\n",
    "#                                 batch_size=batchSize,\n",
    "#                                 num_workers=0)\n",
    "# class MLP(nn.Module):\n",
    "#     def __init__(self, inputSize, outputSize):\n",
    "#         super().__init__()\n",
    "#         self.layers = nn.Sequential(\n",
    "#             nn.Linear(inputSize, 100),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(100, outputSize)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         c = self.layers(x)\n",
    "#         return c\n",
    "    \n",
    "#     def loss(self, constants, eq, X, Y):\n",
    "#         # constants is the output of the model\n",
    "#         err = torch.zeros(len(constants)) # batch_size\n",
    "        \n",
    "#         # sample number of points\n",
    "#         indexes = []\n",
    "#         numberSamples = 10\n",
    "#         while len(indexes) != numberSamples:\n",
    "#             randNum = np.random.randint(len(batch))\n",
    "#             indexes.append(randNum)\n",
    "#         X = X[indexes,:]\n",
    "#         Y = Y[indexes,:]\n",
    "        \n",
    "#         # replace the constants with their predicted values\n",
    "#         for idx, const in enumerate(constants):\n",
    "#             eq = eq.replace('C','{}').format(*const.tolist())\n",
    "        \n",
    "#         # calculate the error for a limited number of points, approximate the error\n",
    "#         for x,y in zip(X,Y):\n",
    "#             # replace variables with their values\n",
    "#             for i,e in enumerate(x):\n",
    "#                 eqTemp = eq.replace('x{}'.format(i+1), str(e.item()))\n",
    "                \n",
    "#             # calculate the error\n",
    "#             yHat = eval(eqTemp)\n",
    "#             err[idx] += (y.item()-yHat)**2\n",
    "#         err[idx] /= numberSamples\n",
    "            \n",
    "#         print(err.shape, constants.shape)\n",
    "#         return err\n",
    "    \n",
    "# c = [0 for i,x in enumerate(predicted) if x=='C']\n",
    "# network = MLP(numVars+numYs, len(c))\n",
    "# cHat = network(batch)\n",
    "# err = network.loss(cHat, predicted, batch[:,:numVars], batch[:,-numYs:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Case 0.\n",
      "Target:0.57*x1**2+0.77*x1*sin(0.13*x1)\n",
      "Skeleton:1.0*x1**2*log(abs(sin(1.0*x1)))\n",
      "Skeleton+LS:1.0*x1**2*log(abs(sin(1.0*x1)))\n",
      "Err:4.393822433284861\n",
      "\n",
      "Test Case 1.\n",
      "Target:1.0*x1-0.38\n",
      "Skeleton:1.0*x1\n",
      "Skeleton+LS:1.0*x1\n",
      "Err:0.005549168864357468\n",
      "\n",
      "Test Case 2.\n",
      "Target:1.39*sqrt(abs(-0.66*x1))*exp(0.24*x1)*exp(-sin(0.53*x1))\n",
      "Skeleton:sqrt(abs(1.0*x1))*sqrt(abs(log(abs(1.0*x1))))\n",
      "Skeleton+LS:sqrt(abs(1.0*x1))*sqrt(abs(log(abs(1.0*x1))))\n",
      "Err:0.48509825267098716\n",
      "\n",
      "Test Case 3.\n",
      "Target:1.0*x1**2*exp(0.55*exp(1.0*x1))+0.43\n",
      "Skeleton:sqrt(abs(1.0*x1))*exp(2.0*x1)\n",
      "Skeleton+LS:sqrt(abs(1.0*x1))*exp(2.0*x1)\n",
      "Err:6069844655.303213\n",
      "\n",
      "Test Case 4.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vpcom\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:1: RuntimeWarning: overflow encountered in exp\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target:-0.29*x1*sin(0.05*x1**2+0.02*x1)\n",
      "Skeleton:log(abs(1.0*x1))*sin(1.0*x1)\n",
      "Skeleton+LS:log(abs(1.0*x1))*sin(1.0*x1)\n",
      "Err:2.2397296074106556\n",
      "\n",
      "Test Case 5.\n",
      "Target:1.0*x1**2-0.48*x1\n",
      "Skeleton:1.0*x1**2*sqrt(abs(log(abs(1.0*x1))))\n",
      "Skeleton+LS:1.0*x1**2*sqrt(abs(log(abs(1.0*x1))))\n",
      "Err:0.2275445713555079\n",
      "\n",
      "Test Case 6.\n",
      "Target:exp(sqrt(abs(1.0*x1)))\n",
      "Skeleton:exp(sqrt(abs(1.0*x1)))\n",
      "Skeleton+LS:exp(sqrt(abs(1.0*x1)))\n",
      "Err:0.0\n",
      "\n",
      "Test Case 7.\n",
      "Target:-0.25*x1**3-0.5338*x1**2-0.12\n",
      "Skeleton:exp(1.0*x1)*log(abs(sin(1.0*x1)))\n",
      "Skeleton+LS:exp(1.0*x1)*log(abs(sin(1.0*x1)))\n",
      "Err:279242.98701740766\n",
      "\n",
      "Test Case 8.\n",
      "Target:1.0*x1**2-0.06*x1\n",
      "Skeleton:1.0*x1**2\n",
      "Skeleton+LS:1.0*x1**2\n",
      "Err:0.00011361472303482163\n",
      "\n",
      "Test Case 9.\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# alright, let's sample some character-level symbolic GPT\n",
    "from mingpt.utils import sample\n",
    "from gp_model import Genetic_Model\n",
    "from mlp_model import MLP_Model\n",
    "from scipy.optimize import least_squares, newton, minimize\n",
    "\n",
    "    \n",
    "loader = torch.utils.data.DataLoader(\n",
    "                                test_dataset, \n",
    "                                shuffle=False, \n",
    "                                pin_memory=True,\n",
    "                                batch_size=1,\n",
    "                                num_workers=0)\n",
    "\n",
    "testRange = [3.1,6.0]\n",
    "numTestPoints = 10\n",
    "#test = np.linspace(3.1,6.0,numTestPoints)\n",
    "\n",
    "gpm = Genetic_Model(n_jobs=-1)\n",
    "mlp = MLP_Model()\n",
    "\n",
    "resultDict = {}\n",
    "try:\n",
    "    with open(fName, 'w', encoding=\"utf-8\") as o:\n",
    "        modelName = 'SymbolicGPT'\n",
    "        resultDict[fName] = {modelName:[],\n",
    "                             'GP':[],\n",
    "                             'MLP':[]}\n",
    "\n",
    "        for i, batch in enumerate(loader):\n",
    "                \n",
    "            inputs,outputs,points = batch\n",
    "\n",
    "            print('Test Case {}.'.format(i))\n",
    "            o.write('Test Case {}/{}.\\n'.format(i,len(textTest)-1))\n",
    "\n",
    "            t = json.loads(textTest[i])\n",
    "\n",
    "            inputs = inputs[:,0:1].to(trainer.device)\n",
    "            points = points.to(trainer.device)\n",
    "            outputsHat = sample(model, inputs, blockSize, points=points,\n",
    "                          temperature=1.0, sample=True, \n",
    "                          top_k=40)[0]\n",
    "\n",
    "            # filter out predicted\n",
    "            target = ''.join([train_dataset.itos[int(i)] for i in outputs[0]])\n",
    "            predicted = ''.join([train_dataset.itos[int(i)] for i in outputsHat])\n",
    "\n",
    "            target = target.strip(train_dataset.paddingToken).split('>')\n",
    "            target = target[0] if len(target[0])>=1 else target[1]\n",
    "            target = target.strip('<').strip(\">\")\n",
    "            predicted = predicted.strip(train_dataset.paddingToken).split('>')\n",
    "            predicted = predicted[0] if len(predicted[0])>=1 else predicted[1]\n",
    "            predicted = predicted.strip('<').strip(\">\")\n",
    "            \n",
    "            print('Target:{}\\nSkeleton:{}'.format(target, predicted))\n",
    "            \n",
    "            o.write('{}\\n'.format(target))\n",
    "            o.write('{}:\\n'.format(modelName))\n",
    "            o.write('{}\\n'.format(predicted))\n",
    "\n",
    "            # train a regressor to find the constants (too slow)\n",
    "            c = [1 for i,x in enumerate(predicted) if x=='C']            \n",
    "            def lossFunc(constants, eq, X, Y):\n",
    "                err = 0\n",
    "                eq = eq.replace('C','{}').format(*constants)\n",
    "\n",
    "                for x,y in zip(X,Y):\n",
    "                    eqTemp = eq + ''\n",
    "                    for i,e in enumerate(x):\n",
    "                        eqTemp = eqTemp.replace('x{}'.format(i+1), str(e))\n",
    "                    try:\n",
    "                        yHat = eval(eqTemp)\n",
    "                    except:\n",
    "                        yHat = 100\n",
    "                    err += (y-yHat)**2\n",
    "                err /= len(Y)\n",
    "                return err\n",
    "            \n",
    "            try:\n",
    "                if len(c) == 0:\n",
    "                    pass # do nothing\n",
    "                else:\n",
    "#                     cHat = least_squares(lossFunc, c, ftol=1e-3,\n",
    "#                                          args=(predicted, t['X'], t['Y']))\n",
    "                    \n",
    "                    # for easier comparison, we are using minimize package  \n",
    "                    cHat = minimize(lossFunc, c,\n",
    "                                   args=(predicted, t['X'], t['Y'])) \n",
    "            \n",
    "#                     cHat= newton(lossFunc, c, maxiter=100,\n",
    "#                                  args=(predicted, t['X'], t['Y']))\n",
    "                    predicted = predicted.replace('C','{}').format(*cHat.x)\n",
    "            except:\n",
    "                print('Wrong Equation:{}'.format(predicted))\n",
    "                raise\n",
    "                predicted = 0\n",
    "\n",
    "            # TODO: let's enjoy GPU\n",
    "\n",
    "            print('Skeleton+LS:{}'.format(predicted))\n",
    "\n",
    "            Ys = [] #t['YT']\n",
    "            Yhats = []\n",
    "            for xs in t['XT']:\n",
    "                try:\n",
    "                    eqTmp = target + '' # copy eq\n",
    "                    eqTmp = eqTmp.replace(' ','')\n",
    "                    eqTmp = eqTmp.replace('\\n','')\n",
    "                    for i,x in enumerate(xs):\n",
    "                        # replace xi with the value in the eq\n",
    "                        eqTmp = eqTmp.replace('x{}'.format(i+1), str(x))\n",
    "                        if ',' in eqTmp:\n",
    "                            assert 'There is a , in the equation!'\n",
    "                    YEval = eval(eqTmp)\n",
    "                    YEval = 0 if np.isnan(YEval) else YEval\n",
    "                    YEval = 100 if np.isinf(YEval) else YEval\n",
    "                except:\n",
    "                    YEval = 100 #TODO: Maybe I have to punish the model for each wrong template not for each point\n",
    "                Ys.append(YEval)\n",
    "                try:\n",
    "                    eqTmp = predicted + '' # copy eq\n",
    "                    eqTmp = eqTmp.replace(' ','')\n",
    "                    eqTmp = eqTmp.replace('\\n','')\n",
    "                    for i,x in enumerate(xs):\n",
    "                        # replace xi with the value in the eq\n",
    "                        eqTmp = eqTmp.replace('x{}'.format(i+1), str(x))\n",
    "                        if ',' in eqTmp:\n",
    "                            assert 'There is a , in the equation!'\n",
    "                    Yhat = eval(eqTmp)\n",
    "                    Yhat = 0 if np.isnan(Yhat) else Yhat\n",
    "                    Yhat = 100 if np.isinf(Yhat) else Yhat\n",
    "                except:\n",
    "                    Yhat = 100\n",
    "                Yhats.append(Yhat)\n",
    "            err = relativeErr(Ys,Yhats)\n",
    "\n",
    "            if type(err) is np.complex128 or np.complex:\n",
    "                err = abs(err.real)\n",
    "\n",
    "            resultDict[fName][modelName].append(err)\n",
    "\n",
    "            o.write('{}\\n{}\\n\\n'.format( \n",
    "                                    predicted,\n",
    "                                    err\n",
    "                                    ))\n",
    "\n",
    "            print('Err:{}'.format(err))\n",
    "            \n",
    "            # Calculate error for baselines\n",
    "\n",
    "    #         # tokenize to get input x, input y, and true eqn\n",
    "    #         train_data_x = t[\"X\"]\n",
    "    #         train_data_y = t[\"Y\"]\n",
    "    #         test_data_x = t[\"XT\"]\n",
    "    #         test_data_y = t[\"YT\"]\n",
    "\n",
    "    #         # train MLP model\n",
    "    #         mlp.reset()\n",
    "    #         model_eqn, _, best_err = mlp.repeat_train(\n",
    "    #                                                 train_data_x, \n",
    "    #                                                 train_data_y,\n",
    "    #                                                 test_x=test_data_x, \n",
    "    #                                                 test_y=test_data_y,                                     \n",
    "    #                                                 verbose=False)\n",
    "    #         test_y_hat = mlp.predict(test_data_x)\n",
    "    #         err = relativeErr(test_data_y,test_y_hat)\n",
    "    #         print(\"{}: {}\".format(mlp.name, model_eqn)[:550])\n",
    "    #         print(\"Err: {:.5f}\".format(err))\n",
    "    #         resultDict[fName]['MLP'].append(err)\n",
    "    #         o.write('\\n{}: {}\\n{}'.format('MLP', \n",
    "    #                                    err,\n",
    "    #                                    model_eqn))\n",
    "\n",
    "    #         # train GP model\n",
    "    #         gpm.reset()\n",
    "    #         model_eqn, _, best_err = gpm.repeat_train(train_data_x, train_data_y,\n",
    "    #                                                 test_x=test_data_x, test_y=test_data_y,\n",
    "    #                                                 verbose=False)\n",
    "    #         print(\"{}: {}\".format(gpm.name, model_eqn)[:550])\n",
    "    #         test_y_hat = gpm.predict(test_data_x)\n",
    "    #         err = relativeErr(test_data_y,test_y_hat)\n",
    "    #         print(\"Err: {:.5f}\".format(err))\n",
    "    #         resultDict[fName]['GP'].append(err)\n",
    "    #         o.write('\\n{}: {}\\n{}'.format('GP', \n",
    "    #                                    err,\n",
    "    #                                    model_eqn))\n",
    "            print('') # just an empty line\n",
    "    print('Avg Err:{}'.format(np.mean(resultDict[fName][modelName])))\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print('KeyboardInterrupt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3sAAAJcCAYAAABAE73ZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOzde5ztdV0v/td7A7pBthdgpwIqmmgqECpq5Y0UzfuFo1maIaHV70hXNc3jKTQ9p5taWecY2kHSk2mWd62w8tYxDdS8YUJqgnhBEEEuCsz798daM3vY7pm95rJmrVnzfD4e85i1vt/vWus9awZmXvv9uVR3BwAAgNmybdIFAAAAsP6EPQAAgBkk7AEAAMwgYQ8AAGAGCXsAAAAzSNgDAACYQcIeAMuqqudX1asnXce8qrplVb2/qq6oqpdOup7FVvJeVdVrqurFy5zvqrrj+lU3vSb5tVbVEcPX33cSrw8wTsIewAapqrtU1T9W1beq6vyqevyka9pdVR1fVRcuPtbd/6O7nz6pmvbgZ5N8I8lNu/tZu5+squdU1aeGYfALVfWcjSpsCt+rFamq91bVNVX17eHP6fur6uhJ1wXA6gh7ABtg2DV4a5J3JDkog8Dyuqq600QL25xul+Qz3d1LnK8kP53kFkkeluTUqvqJcRc1Q52hU7v7wCQHJ3lvktdOtpzxmaHvGcAeCXsAG+MHkhya5OXdfX13/2OSf07y1KUeUFU/U1XnVtU3q+rvqup2i849pKo+O+y+/HFVva+qnj48d1pVvW7RtTcYplZVJw+f94qq+nxV/dzw+E2SvDvJocPOzrer6tA9PN9jqurTVXXZsBN0l0XnvlhVz66qTwxre0NVbR+eO6Sq3jF83KVV9YGq2uPvoar6kar61+Fz/GtV/cjw+GuSnJTk14b1nbD7Y7v7d7v7o919XXf/ewYh+75LvM7fVtWpux37t6o6cXj7D6vqgqq6vKrOqar7L7rutKp6U1W9rqouT/K0PbxXf1VVX13UJbvbbiUcUlVnDb8X71v8Pd6tphtX1e9X1Zeq6mtV9cqq2n+l7+tKdPd1Sf4yyV0X1bGtqp5XVf9RVZdU1Rur6qDhufmfs5OGdX6jqv7bosfuU4Nhrv8x/HrPqarbLHrJE6rqvOHP+59UVQ0f97Sq+ueqevnwa/z88OfjacPvzder6qRFr/PIqvrY8Ht2QVWdtujcfI2nVNWXkvzjHt7r/zL8OT5qre8hwKQJewAbo5Y4tsc/KKvqcUmen+TEJDuTfCDJ64fnDkny10lekOSQJP+RJcLMEr6e5FFJbprk5CQvr6p7dPeVSR6e5KLuPnD4cdFudd1pWMcvD+t6V5K3V9WNFl324xl01G6f5JgkTxsef1aSC4ePu+Xw6/ue7twwPLwzyR9l0F16WZJ3VtXB3f20JP83ye8O63vPcl/oMDDcP8mnl7jkL5L85KLr75pB5/Cdw0P/muTYDLqxf5Hkr+bD69Bjk7wpyc2Hde3u3UmOTPJ9ST66h2uekuS3Mvg+fnyJ50iS30lyp2Etd0xyWJLfGJ4b6X1dqeH39ClJ/mXR4V9M8rgkD8zgHy++meRPdnvo/ZLcOcmDk/zGon8M+NUM3utHZPCz9zNJrlr0uEcluVeSH8zgZ+jHFp27T5JPZPDz8BcZhNB7ZfBe/FSSP66qA4fXXplBZ/fmSR6Z5P8b/ve02AOT3GW310hVnZzBe31Cd39qz+8MwOYh7AFsjM9mELKeU1X7VdVDM/iD84Alrv+5JP+zu88ddlj+R5Jjh52fR2QwjPFN3X1tkj9I8tVRC+nud3b3f/TA+5L8fQaBaBRPSvLO7j5r+Nq/n2T/JD+y6Jo/6u6LuvvSJG/PIKAkybVJbp3kdt19bXd/YImhmI9Mcl53v3bYnXt9Bu/fo0f9Ghc5LYPfdWcscf7N2fW+JoNw8zfd/Z0k6e7XdfclwzpemuTGGQSZeR/q7rd091x3X737k3f3/+nuK4bPd1qSH6yqmy265J3d/f7h+f+W5Id363bNB9ZnJPmV7r60u6/I4OdhfmjqqO/rqP6oqi5L8u0kpyZ54aJzP5fkv3X3hYu+pifUDYdDvrC7r+7uf0vybxmEtyR5epIXdPe/D3/2/q27L1n0uN/u7su6+0tJ/im7fm6S5AvdfUZ3X5/kDUluk+RF3f2d7v77JN/NIPilu9/b3Z8cfk8+kcE/Tjxwt6/xtO6+crfv2S8neU6S47v7/JW8YQDTStgD2ADDYPS4DILMVzPoxrwxg47MntwuyR8Oh61dluTSDDqBh2XQUblg0XP34vt7U1UPr6p/GQ75uyyD8HjIiA8/NMl/LnrtueFrH7bomsXB86ok8x2X30tyfpK/Hw7Fe94orzH0n7u9xl4Nh2f+dJJHzoe33Q2D0zuzKzj9RBZ116rqWTUY8vqt4Xt1s9zwvVryfR8OW/zt4bDFy5N8cXhqj4/v7m9n8H0+dLen2pnBPwqcs+jn4W+Hx5MR39fhEMr54bmvXKruJL/Y3TdPsj2DbtubquqY4bnbJXnzojrOTXJ9Bh3FeUt9/2+TQRd6KUs9Lkm+tuj21UnS3bsfOzBJquo+VfVPVXVxVX0ryc/ne3++9/R9e06SP+nupf6bBNh0hD2ADdLdn+juB3b3wd39Y0nukOQjS1x+QZKf6+6bL/rYv7v/X5KvZPCHc5KFzs/ibtCVuWHH8FaLrr1xBkNAfz/JLYd/1L8ru4aZ7q0jdFEGf/Dv/tpf3svjMuxwPau775BBl+5Xq+rBe3uNoduO8hqL6vqZJM9L8uAR/nh/fZKfrKofzqBL+U/D57h/kudmMKTwFsP36lu54ZDc5d6vJ2cwzPOEDELiEfPlLbpm8ffxwAyGi95g6GwGK49eneRui34WbjZcRGXk93W4Uuj88NyfX6bu+evnuvsDGQTJhw4PX5Dk4bv9XG7v7lG+Nxck+f4Rrlurv0jytiS36e6bJXllvncY9Z6+bw9N8oKq+i9jrg9gwwh7ABukqo6pqu1VdUBVPTuDoXevWeLyVyb59Rou6FFVN6uqJw7PvTPJ3arqxOHwuV/MokCXwdyvB1TVbYdDBn990bkbZTAU8eIk11XVw7PrD/lk0EE5eLehhou9Mckjq+rBVbVfBh3K7yT5fyN8/Y+qqjsOA+LlGXSErt/Dpe9KcqeqenJV7VtVT8pgkZB37O01hq/zlAyGOT6kuz8/wkPelUG4fFGSNwy7lUmyI8l1GbxX+1bVb2Qw12xUOzJ4by7JIHz/jz1c84iqut9wftxvJflwd9+g6zSs51UZzK38vuHXeFhV/djw9qjv64oNA/Bds2vO4yuTvGR+2GtV7ayqx474dK9O8ltVdWQNHFNVB69HnbvZkeTS7r6mqu6dQegexaczmGv6J1X1mDHUBbDhhD2AjfPUDLpyX89g8YqHLDO88M0ZLBTxl8MhgJ/KYPGUdPc3kjwxyW9nECSOzGBlz/nHnpXBvKZPJDkni0LScNjiL2YQ2r6ZwR/Cb1t0/rMZdLo+Pxyqd4MhhT1Y3fKnkrwig47To5M8uru/O8LXf2SS92QwF+xDSf5Xd793D1/7JRkMH3zW8Ov7tSSPGn7do3hxBgt5/OsowxaH34O/yaAD9xeLTv1dBgusfC6DYaTXZAXDZZP8+fBxX07ymdxwoZN5f5HkNzMYvnnPDOYM7slzM+iw/cvw5+E92TV3cKT3dQX+eP59y2DbhRd097uH5/4wg5+Xv6+qK4Zf031GfN6XZfBz9/cZhNI/y6CTut7+a5IXDev7jeFrjmQ4z/BRSV41/IcQgE2t1jaHG4BpUFXvTfK67n71pGsBAKaDzh4AAMAMEvYAAABmkGGcAAAAM0hnDwAAYAbtO+kC1uKQQw7pI444YtJlAAAATMQ555zzje7euadzmzrsHXHEETn77LMnXQYAAMBEVNV/LnXOME4AAIAZJOwBAADMIGEPAABgBm3qOXt7cu211+bCCy/MNddcM+lSSLJ9+/Ycfvjh2W+//SZdCgAAbCkzF/YuvPDC7NixI0cccUSqatLlbGndnUsuuSQXXnhhbn/720+6HAAA2FJmbhjnNddck4MPPljQmwJVlYMPPliXFQAAJmDmwl4SQW+K+F4AAMBkzGTYAwAA2OqEvTF5yUtekrvd7W455phjcuyxx+bDH/7wmp7vNa95TU499dQVPebAAw9Mklx00UV5whOesOy11113XZ7//OfnyCOPzLHHHptjjz02L3nJSxbO77PPPjn22GNz1FFH5YlPfGK+/OUvL1x3q1vdKocddtjC/e9+97sr/wIBAIB1NXMLtEyDD33oQ3nHO96Rj370o7nxjW+cb3zjGxMNQIceemje9KY3LXvNC17wgnz1q1/NJz/5yWzfvj1XXHFFXvrSly6c33///fPxj388SfKUpzwlb3jDGxbun3baaTnwwAPz7Gc/e3xfBAAAsCI6e2Pwla98JYccckhufOMbJ0kOOeSQnHvuuXn84x+/cM1ZZ52VE088McmgA/fc5z4397znPXPCCSfkIx/5SI4//vjc4Q53yNve9raFx1xwwQV52MMeljvf+c554QtfuHD8ZS97WY466qgcddRR+YM/+IPvqeeLX/xijjrqqCTJ9ddfn2c/+9k5+uijc8wxx+QVr3hFrrrqqrzqVa/KK17ximzfvj1JsmPHjpx22ml7/Pruf//75/zzz1/bmwQAAIzVTHf2Xvj2T+czF12+rs9510Nvmt989N2WveahD31oXvSiF+VOd7pTTjjhhDzpSU/Kgx70oDzzmc/MxRdfnJ07d+aMM87IySefnCS58sorc/zxx+d3fud38vjHPz4veMELctZZZ+Uzn/lMTjrppDzmMY9JknzkIx/Jpz71qRxwwAG5173ulUc+8pGpqpxxxhn58Ic/nO7Ofe5znzzwgQ/M3e9+9z3Wdvrpp+cLX/hCPvaxj2XffffNpZdemvPPPz+3ve1ts2PHjr1+/dddd13e/e5352EPe9gK3zkAAGAj6eyNwYEHHphzzjknp59+enbu3JknPelJOfPMM/PUpz41r3vd63LZZZflQx/6UB7+8IcnSW50oxsthKejjz46D3zgA7Pffvvl6KOPzhe/+MWF533IQx6Sgw8+OPvvv39OPPHEfPCDH8wHP/jBPP7xj89NbnKTHHjggTnxxBPzgQ98YMna3vOe9+Tnf/7ns+++g5x/0EEHfc81Z5xxRo499tjc5ja3yQUXXJAkufrqq3PsscfmuOOOy21ve9uccsop6/V2AQAAYzDTnb29deDGaZ999snxxx+f448/PkcffXTOPPPM/Omf/mke/ehHZ/v27XniE5+4ELj222+/hS0Ktm3btjD8c9u2bbnuuusWnnP3bQyqKt29orq6+3ue5453vGO+9KUv5YorrsiOHTty8skn5+STT85RRx2V66+/PskN5+wBAADTT2dvDP793/8955133sL9j3/847nd7W6XQw89NIceemhe/OIX52lPe9qKn/ess87KpZdemquvvjpvectbct/73jcPeMAD8pa3vCVXXXVVrrzyyrz5zW/O/e9//yWf46EPfWhe+cpXLoTISy+9NAcccEBOOeWUnHrqqQsboF9//fVW1QQAgE1spjt7k/Ltb387v/ALv5DLLrss++67b+54xzvm9NNPTzJYyfLiiy/OXe961xU/7/3ud7889alPzfnnn58nP/nJOe6445IkT3va03Lve987SfL0pz99yfl68+c/97nP5Zhjjsl+++2XZzzjGTn11FPzkpe8JP/9v//3HHXUUdmxY0f233//nHTSSTn00ENX8Q4AAACTVisdBjhNjjvuuD777LNvcOzcc8/NXe5ylwlVtHennnpq7n73u2+pOW/T/j0BAIDNqqrO6e7j9nROZ28D3fOe98xNbnKTG+xfBwAAMA7C3gY655xzJl0CAACwRczkAi2beWjqrPG9AACAyRhb2Kuq/1NVX6+qTy06dlBVnVVV5w0/32J4vKrqj6rq/Kr6RFXdY7Wvu3379lxyySVCxhTo7lxyySXZvn37pEsBAIAtZ5zDOF+T5I+T/PmiY89L8g/d/dtV9bzh/ecmeXiSI4cf90nyv4efV+zwww/PhRdemIsvvngNpbNetm/fnsMPP3zSZQAAwJYztrDX3e+vqiN2O/zYJMcPb5+Z5L0ZhL3HJvnzHrTj/qWqbl5Vt+7ur6z0dffbb7/c/va3X23ZAAAAN/DcN30iV373uvzxk1c9AHEiNnrO3i3nA9zw8/cNjx+W5IJF1104PPY9qupnq+rsqjpb9w4AABi3i751db582dWTLmPFpmWBltrDsT1Ouuvu07v7uO4+bufOnWMuCwAA2Oq69xxYpt1Gh72vVdWtk2T4+evD4xcmuc2i6w5PctEG1wYAAPA9Op2qzRf3NjrsvS3JScPbJyV566LjPz1clfOHknxrNfP1AAAAxmHzRb0xLtBSVa/PYDGWQ6rqwiS/meS3k7yxqk5J8qUkTxxe/q4kj0hyfpKrkpw8rroAAABWYrPu6jbO1Th/colTD97DtZ3kmeOqBQAAYLW6k004inNqFmgBAACYSp1ObcKBnMIeAADA3my+rCfsAQAALGezztkT9gAAAJbR2ZSNPWEPAABgWRZoAQAAmD0WaAEAAJhROnsAAAAzxgItAAAAM6ijswcAADBzus3ZAwAAmDk6ewAAAEwNYQ8AAGAZFmgBAACYQYNhnJtvHKewBwAAsJzuTbg8i7AHAACwV5uwsSfsAQAALKcTnT0AAIBZY4EWAACAGdRpC7QAAADMmm7DOAEAAGbSJmzsCXsAAADLMWcPAABgBg2y3uZr7Ql7AAAAy+huwzgBAABm0SbMesIeAADA3ujsAQAAzBgLtAAAAMygTqc24UBOYQ8AAGAZ3YZxAgAAzCRhDwAAYMZ0YhgnAADArOlNukKLsAcAALCMTjblRnvCHgAAwHJ6U2Y9YQ8AAGBvahOu0CLsAQAALGNzztgT9gAAAJbV3YZxAgAAzJqOffYAAABm0ibMesIeAADAcrot0AIAADBzepMu0SLsAQAALKPtswcAADB7urMp056wBwAAsBe1CdOesAcAALAXm3B9FmEPAABgOd0WaAEAAJg5m3TKnrAHAACwnME+e5OuYuWEPQAAgL2wQAsAAMCM6bTOHgAAwKzZpOuzCHsAAADL6ZizBwAAMKM2X9oT9gAAAJZhNU4AAICZtDkn7Ql7AAAAy+jejIM4hT0AAIBlWaAFAABgRtlUHQAAYMZ021QdAABg5mzO5VmEPQAAgGVZoAUAAGAGDYZxbr64J+wBAADMIGEPAABgGbZeAAAAmEWbdIUWYQ8AAGAZHfvsAQAAzCTDOAEAAGZMd2/Cvp6wBwAAsCwLtAAAAMygtkALAADA7OnYVB0AAGAmbb6oJ+wBAAAsqwd7L2w6wh4AAMAyNumUPWEPAABgWW1TdQAAgJkzWKBl0lWsnLAHAACwF5sw6wl7AAAAy+m2qToAAMDMsUALAADADOpuC7QAAADMIsM4AQAAZswm3VNd2AMAAFhOdzZla0/YAwAAmEHCHgAAwBK6B2txbr6+nrAHAACwV5twFKewBwAAsJRhY8/WCwAAALNks26ongh7AAAAS1qYs7f5GnvCHgAAwFLmO3ubMOsJewAAAHujswcAADBDFhZo2YRpT9gDAABYQm/iJVqEPQAAgCX05s16wh4AAMDebMJRnMIeAADA3thUHQAAYIbsWqBlsnWshrAHAACwBAu0AAAAzKCFzt5ky1gVYQ8AAGAvDOMEAACYIfODOC3QAgAAMEN6OI5TZ29EVfUrVfXpqvpUVb2+qrZX1e2r6sNVdV5VvaGqbjSJ2gAAAOZt3uVZJhD2quqwJL+Y5LjuPirJPkl+IsnvJHl5dx+Z5JtJTtno2gAAABbrTZz2JjWMc98k+1fVvkkOSPKVJA9K8qbh+TOTPG5CtQEAANxAbcJxnBse9rr7y0l+P8mXMgh530pyTpLLuvu64WUXJjlsT4+vqp+tqrOr6uyLL754I0oGAAC2KlsvjK6qbpHksUlun+TQJDdJ8vA9XLrHhml3n97dx3X3cTt37hxfoQAAwJZnU/WVOSHJF7r74u6+NsnfJPmRJDcfDutMksOTXDSB2gAAABYsbKq+CVt7kwh7X0ryQ1V1QA0Gvj44yWeS/FOSJwyvOSnJWydQGwAAwPfYhFlvInP2PpzBQiwfTfLJYQ2nJ3lukl+tqvOTHJzkzza6NgAAgMUWNlXfhK29ffd+yfrr7t9M8pu7Hf58kntPoBwAAIA9sqk6AADADNq8y7MIewAAAEtqWy8AAADMsE04jlPYAwAAWML8PnubL+oJewAAAEvbxPvsTWQ1TgBg8ubmOh/+wqW56rvXTboUgKl12VXXTrqEVRP2AGCL+tgFl+UnX/Uvky4DYFO46fb9Jl3Cigl7ALBFzXf0fvvEo3PXQ2864WoAptd++2zLD9xqx6TLWDFhDwC2qLnhPJQjb3lgjjn85pMtBoB1Z4EWANii5oabR9VmXHUAgL0S9gBgqxp29rYJewAzSdgDgC1qobM34ToAGA9hDwC2qNbZA5hpwh4AbFG75uxNuBAAxkLYA4AtatjYE/YAZpSwBwBbVC/M2ZP2AGaRsAcAW9T8Pnvb/DUAMJP87x0AtigLtADMNmEPALYoWy8AzDZhDwC2qF0LtIh7ALNI2AOALaptvQAw04Q9ANiizNkDmG3CHgBsUfNz9rbJegAzSdgDgC1qfusF++wBzCZhDwC2KHP2AGabsAcAW9T8nD1hD2A2CXsAsEV15ufsSXsAs0jYA4Atas5qnAAzTdgDgC3KME6A2SbsAcAWNWeBFoCZJuwBwBa1sBqnrRcAZpKwBwBb1HAUp03VAWaUsAcAW9TcnNU4AWaZsAcAW9R8Z0/WA5hNwh4AbFFzC6txSnsAs0jYA4Atqq3GCTDThD0A2KLapuoAM03YA4AtamGfvQnXAcB4CHsAsEXt2npB3AOYRcIeAGxRc+bsAcw0YQ8AtqheWI1zsnUAMB7CHgBsUfOrcRrGCTCbhD0A2KIWOnuTLQOAMRH2AGCLmrP1AsBME/YAYIuyQAvAbNtr2KuqozaiEABgY81vvVDSHsBMGqWz98qq+khV/dequvnYKwIANkR3Z5ucBzCz9hr2uvt+SZ6S5DZJzq6qv6iqh4y9MgBgrLp19QBm2Uhz9rr7vCQvSPLcJA9M8kdV9dmqOnGcxQEA4zOnswcw00aZs3dMVb08yblJHpTk0d19l+Htl4+5PgBgTDo6ewCzbN8RrvnjJK9K8vzuvnr+YHdfVFUvGFtlAMBYzXXbYw9gho0S9h6R5Oruvj5Jqmpbku3dfVV3v3as1QEAY9Ntjz2AWTbKnL33JNl/0f0DhscAgE2su+2xBzDDRgl727v72/N3hrcPGF9JAMBGmNPZA5hpo4S9K6vqHvN3quqeSa5e5noAYBPojjl7ADNslDl7v5zkr6rqouH9Wyd50vhKAgA2wpxhnAAzba9hr7v/tap+IMmdM/gHwM9297VjrwwAGLttNtoDmFmjdPaS5F5Jjhhef/eqSnf/+diqAgDGztYLALNtr2Gvql6b5PuTfDzJ9cPDnUTYA4BNbK7bAi0AM2yUzt5xSe7a3T3uYgCAjdMdc/YAZtgoq3F+Ksmtxl0IALCx5jopaQ9gZo3S2TskyWeq6iNJvjN/sLsfM7aqAIAN0LE+C8DsGiXsnTbuIgCAjTc3l5QlWgBm1ihbL7yvqm6X5Mjufk9VHZBkn/GXBgCMU+vsAcy0vc7Zq6pnJHlTkj8dHjosyVvGWRQAMH7m7AHMtlEWaHlmkvsmuTxJuvu8JN83zqIAgPGb67YaJ8AMGyXsfae7vzt/p6r2zWCfPQBgM+vYZw9gho2yQMv7qur5Sfavqock+a9J3j7esgCAeZdfc23O+OAX853rrl/X5/3URd/S2QOYYaOEveclOSXJJ5P8XJJ3JXn1OIsCAHZ5/+cuzsvf87nsu63WPZydcJdbru8TAjA1RlmNcy7Jq4YfAMAGu35uMHvi737lAfn+nQdOuBoANou9hr2q+kL2MEevu+8wlooAgBvo4W9h8+sAWIlRhnEet+j29iRPTHLQeMoBAHY3N0x7oh4AK7HX1Ti7+5JFH1/u7j9I8qANqA0AiM4eAKszyjDOeyy6uy2DTt+OsVUEANzAQmdP1gNgBUYZxvnSRbevS/LFJD8+lmoAgO8x39kT9gBYiVFW4/zRjSgEANizHq6TZhgnACsxyjDOX13ufHe/bP3KAQB2N6ezB8AqjLoa572SvG14/9FJ3p/kgnEVBQDsYoEWAFZjlLB3SJJ7dPcVSVJVpyX5q+5++jgLAwAGbL0AwGrsdeuFJLdN8t1F97+b5IixVAMAfI9hYy+lswfACozS2Xttko9U1Zsz+H3z+CR/PtaqAIAFbesFAFZhlNU4X1JV705y/+Ghk7v7Y+MtCwCYNzdnNU4AVm6UYZxJckCSy7v7D5NcWFW3H2NNAMAi88M4t8l6AKzAXsNeVf1mkucm+fXhof2SvG6cRQEAuyxsvWCJFgBWYJTO3uOTPCbJlUnS3Rcl2THOogCAXRbm7I06HgcAMlrY+24Pfst0klTVTcZbEgCwWC909gBgdKOEvTdW1Z8muXlVPSPJe5K8arxlAQDzOhZoAWDlRlmN8/er6iFJLk9y5yS/0d1njb0yACDJojl7sh4AK7Bs2KuqfZL8XXefkETAA4AJmGudPQBWbtlhnN19fZKrqupmG1QPALCb1tkDYBX2OowzyTVJPllVZ2W4ImeSdPcvjq0qAGDBwmqclmgBYAVGCXvvHH4AABMw39mzqToAK7Fk2Kuq23b3l7r7zI0sCAC4oV0LtEh7AIxuuTl7b5m/UVV/vQG1AAB7sGvrhQkXAsCmslzYW/wr5Q7jLgQA2DOdPQBWY7mw10vcBgA2UHdbiROAFVtugZYfrKrLM+jw7T+8neH97u6bjr06ACDd9tgDYOWWDHvdvc9GFgIA7Nlct00XAFixZTdVBwAmr6OzB8DKCXsAMOXmuqO1B8BKCXsAMO3atgsArNxIYa+qbldVJwxv719VO8ZbFgMXfgYAACAASURBVAAwbzBnT9oDYGX2Gvaq6hlJ3pTkT4eHDs+iDddXo6puXlVvqqrPVtW5VfXDVXVQVZ1VVecNP99iLa8BALNiTmcPgFUYpbP3zCT3TXJ5knT3eUm+b42v+4dJ/ra7fyDJDyY5N8nzkvxDdx+Z5B+G9wFgy7P1AgCrMUrY+053f3f+TlXtmzVssl5VN03ygCR/liTd/d3uvizJY5OcObzszCSPW+1rAMAssUALAKsxSth7X1U9P4ON1R+S5K+SvH0Nr3mHJBcnOaOqPlZVr66qmyS5ZXd/JUmGn/fYPayqn62qs6vq7IsvvngNZQDA5qGzB8BKjRL2npdBOPtkkp9L8q4kL1jDa+6b5B5J/nd33z3JlVnBkM3uPr27j+vu43bu3LmGMgBgc5jrjqwHwErtO8I1j03y5939qnV6zQuTXNjdHx7ef1MGYe9rVXXr7v5KVd06ydfX6fUAYFMzZw+A1Rils/eYJJ+rqtdW1SOHc/ZWrbu/muSCqrrz8NCDk3wmyduSnDQ8dlKSt67ldQBgVgy2XgCAldlrcOvuk6tqvyQPT/LkJP+rqs7q7qev4XV/Icn/raobJfl8kpMzCJ5vrKpTknwpyRPX8PwAMDPmOimdPQBWaKQuXXdfW1XvzmAVzv0zGNq56rDX3R9PctweTj14tc8JALOr7bMHwIqNsqn6w6rqNUnOT/KEJK9Ocusx1wUADM3NxQItAKzYKJ29pyX5yyQ/193fGW85AMDuOm2BFgBWbJQ5ez+xEYUAAHs2Z091AFZhybBXVR/s7vtV1RUZzNVbOJWku/umY68OABjusyfuAbAyS4a97r7f8POOjSsHAPgebc4eACs3ygItrx3lGAAwHnNtzh4AKzfKpup3W3xnuKn6PcdTDgCwu05svQDAii0Z9qrq14fz9Y6pqsuHH1ck+VqSt25YhQCwxdlUHYDVWDLsdff/HM7X+73uvunwY0d3H9zdv76BNQLAltbd5uwBsGKjbL3w61V1iyRHJtm+6Pj7x1kYADDQtl4AYBX2Gvaq6ulJfinJ4Uk+nuSHknwoyYPGWxoAkFigBYDVGWWBll9Kcq8k/9ndP5rk7kkuHmtVAMCCtvUCAKswSti7pruvSZKqunF3fzbJncdbFgAwT2cPgNXY6zDOJBdW1c2TvCXJWVX1zSQXjbcsAGBex2qcAKzcKAu0PH5487Sq+qckN0vyt2OtCgBY0N0WaAFgxZYMe1V10B4Of3L4+cAkl46lIgDgBrqTbaNMvACARZbr7J2T4ciRPZzrJHcYS0UAwA3Mdaf09gBYoSXDXnfffiMLAQD2bK6TbbIeACs0yj57D9jTcZuqAzDtXvT2z+TTF31r0mWs2We/ekWOOOQmky4DgE1mlNU4n7Po9vYk985giKdN1QGYaq/7l//MIQfeKLc56IBJl7ImP3CrHXnE0beedBkAbDKjrMb56MX3q+o2SX53bBUBwDrpdB5798Py3If9wKRLAYANt5q1vS5MctR6FwIA681cNwC2slHm7L0ig9U3k0E4PDbJv42zKABYD92dbTYjB2CLGmXO3tmLbl+X5PXd/c9jqgcA1s1c73n/IADYCkaZs3fmRhQCAOupezAopXT2ANii9jpnr6oeVVUfq6pLq+ryqrqiqi7fiOIAYLWGWS+yHgBb1SjDOP8gyYlJPtnz/0wKAFNubvgry5w9ALaqUVbjvCDJpwQ9ADaThZXFZD0AtqhROnu/luRdVfW+JN+ZP9jdLxtbVQCwRnPm7AGwxY0S9l6S5NtJtie50XjLAYD1Yc4eAFvdKGHvoO5+6NgrAYB1tBD2bL4AwBY1ypy991SVsAfAptKZX6BlwoUAwISMEvaemeRvq+pqWy8AsFnMDTt7VuMEYKsaZVP1HRtRCACsp10LtEy4EACYkL2Gvap6wJ6Od/f7178cAFgfuxZokfYA2JpGWaDlOYtub09y7yTnJHnQWCoCgHUwvz2sqAfAVjXKMM5HL75fVbdJ8rtjqwgA1kEvzNmbbB0AMCmjLNCyuwuTHLXehQDAepqfs7dN2gNgixplzt4rkuH61YNweGySfxtnUQCwVvO/uEQ9ALaqUebsnb3o9nVJXt/d/zymegBgXexajVPcA2BrWjLsVdXOJDu7+8zdjt+tqnZ298Vjrw4AVmnXapyTrQMAJmW5OXuvSLJzD8cPT/KH4ykHANZH21QdgC1uubB3dHe/b/eD3f13SY4ZX0kAsHZztl4AYItbLuztt8pzADBxCyuL6ewBsEUtF/bOq6pH7H6wqh6e5PPjKwkA1m5ubn6BlgkXAgATstxqnL+S5B1V9eNJzhkeOy7JDyd51LgLA4D1YDVOALaqJTt73f25JEcneV+SI4Yf70tyzPAcAEythU3VZT0Atqhl99nr7u8kOWODagGAdTNn6wUAtrjl5uwBwKbVC509aQ+ArUnYA2Am7ersCXsAbE3CHgAzyj57AGxtS87Zq6pPZtc2Rd+ju22sDsDUmu/sGcYJwFa13AIt89srPHP4+bXDz09JctXYKgKAddAWaAFgi1sy7HX3fyZJVd23u++76NTzquqfk7xo3MUBwGrZegGArW6UOXs3qar7zd+pqh9JcpPxlQQAazcf9izQAsBWtew+e0OnJPk/VXWzDObwfSvJz4y1KgBYo4VhnJMtAwAmZq9hr7vPSfKDVXXTJNXd3xp/WQCwNm2BFgC2uL0O46yqW1bVnyV5Q3d/q6ruWlWnbEBtALBqPb/1gqwHwBY1ypy91yT5uySHDu9/Lskvj6sgAFgPtl4AYKsbJewd0t1vTDKXJN19XZLrx1oVAKxRm7QHwBY3Sti7sqoOznCD9ar6oQwWaQGAqaWzB8BWN8pqnM9K8rYk3z/cX29nkieOtSoAWKO2zx4AW9xIq3FW1QOT3DmDwTD/3t3Xjr0yAFiDYWMvZRwnAFvUKKtx/keSp3f3p7v7U919bVW9YwNqA4BVm5vT2QNgaxtlzt61SX60qs6oqhsNjx02xpoAYM3mO3saewBsVaOEvau6+0lJzk3ygaq6XRb9DgWAaTS3MGdP2gNgaxplgZZKku7+3ao6J4M99w4aa1UAsFZW4wRgixsl7P3G/I3u/oeq+rEkJ42vJABYu/mtF2Q9ALaqJcNeVf1Ad382yZer6h67nbZACwBTbc7WCwBscct19p6V5BlJXrqHc53kQWOpCADWwa7J5dIeAFvTkmGvu58x/PyjG1cOAKwPnT0AtrrlhnGeuNwDu/tv1r8cAFgnFmgBYItbbhjno5c510mEPQCm1nxnT9YDYKtabhjnyRtZCACsp9bZA2CLG2XrhVTVI5PcLcn2+WPd/aJxFQUAazXf2QOArWrb3i6oqlcmeVKSX8hgSbMnJrndmOsCgDWZ09kDYIvba9hL8iPd/dNJvtndL0zyw0luM96yAGCtzNkDYGsbJexdPfx8VVUdmuTaJLcfX0kAsHY6ewBsdaPM2XtHVd08ye8l+WgG/1T66rFWBQBrtGuBlsnWAQCTstew192/Nbz511X1jiTbu/tb4y0LANbG1gsAbHV7DXtVtU+SRyY5Yv76qkp3v2y8pQEwq6659vqc8LL35WuXXzO217h+OI5zn22jzFgAgNkzyjDOtye5Jsknk8yNtxwAtoLLr7k2F37z6jzgTjtz1KE3Hdvr3OKAG+V2Bx0wtucHgGk2Stg7vLuPGXslAGwdw/l0D7vbrfLk+9x2srUAwIwaZWzLu6vqoWOvBIAtY36lTPPpAGB8Runs/UuSN1fVtgy2Xagk3d3jG3cDwEybXzzFSpkAMD6jhL2XZrCR+ie75xeyBoDVm/9lUlp7ADA2owzjPC/JpwQ9ANbL3HAcp6gHAOMzSmfvK0neW1XvTvKd+YO2XgBgrbbp7AHA2IwS9r4w/LjR8AMA1sSG5wAwfsuGveGG6gd293M2qB4AtoD51Th19gBgfJads9fd1ye5xwbVAsAW0Tp7ADB2owzj/HhVvS3JXyW5cv5gd//N2KoCYKbt2mdP2gOAcRkl7B2U5JIkD1p0rJMIewCskn32AGDc9hr2uvvkjSgEgK1jobNn8wUAGJu97rNXVYdX1Zur6utV9bWq+uuqOnwjigNgNs2vxqmzBwDjM8qm6mckeVuSQ5McluTtw2MAsCq9MGdvsnUAwCwbJezt7O4zuvu64cdrkuwcc10AzLBd++xJewAwLqOEvW9U1U9V1T7Dj5/KYMEWAFiVhc7eZMsAgJk2Stj7mSQ/nuSrSb6S5AnDYwCwKm1TdQAYu1FW4/xSksdsQC0AbBE9v/XCKP/kCACsypJhr6p+Y5nHdXf/1hjqAWALsPUCAIzfcp29K/dw7CZJTklycBJhD4BV2bVAy4QLAYAZtmTY6+6Xzt+uqh1JfinJyUn+MslLl3rcqKpqnyRnJ/lydz+qqm4/fO6Dknw0yVO7+7trfR0Aps+urRekPQAYl2VnS1TVQVX14iSfyCAY3qO7n9vdX1+H1/6lJOcuuv87SV7e3Ucm+WYGHUQAZlDbVB0Axm7JsFdVv5fkX5NckeTo7j6tu7+5Hi9aVYcneWSSVw/vV5IHJXnT8JIzkzxuPV4LgOkzbOyZswcAY7RcZ+9ZSQ5N8oIkF1XV5cOPK6rq8jW+7h8k+bUkc8P7Bye5rLuvG96/MMlhe3pgVf1sVZ1dVWdffPHFaywDgEmYm9PZA4BxWzLsdfe27t6/u3d0900Xfezo7puu9gWr6lFJvt7d5yw+vKcSlqjr9O4+rruP27lz52rLAGCC5szZA4Cx2+s+e2Nw3ySPqapHJNme5KYZdPpuXlX7Drt7hye5aAK1AbAB5vfZk/UAYHw2fDvb7v717j68u49I8hNJ/rG7n5Lkn5I8YXjZSUneutG1AbAx5lfj3CbtAcDYbHjYW8Zzk/xqVZ2fwRy+P5twPQCMya6tFyZbBwDMskkM41zQ3e9N8t7h7c8nufck6wFgY8zZegEAxm6aOnsAbBHzYW/P63MBAOtB2ANgw81HPZ09ABgfYQ+ADdcLwzilPQAYF2EPgA1ngRYAGD9hD4ANN2frBQAYO2EPgA3XCwu0AADjIuwBsOF09gBg/IQ9ADbcfGdP1gOA8RH2ANhwu7ZekPYAYFyEPQA23NzC1gsTLgQAZpiwB8CGs/UCAIyfsAfAhptbmLMn7QHAuAh7AGy4hc7eZMsAgJkm7AGw4Trzc/bEPQAYF2EPgA03Nzf4LOsBwPgIewBsOFsvAMD4CXsAbLg5m6oDwNgJewBsuLYaJwCMnbAHwIabX43TpuoAMD7CHgAbbm5h6wVpDwDGRdgDYMPt2nphwoUAwAwT9gDYcPOdPY09ABgfYQ+ADTe/QIutFwBgfIQ9ADbcrgVahD0AGBdhD4ANt7DP3oTrAIBZJuwBsOF09gBg/IQ9ADbcfGdPaw8AxkfYA2BibL0AAOMj7AGw4Rbm7BnGCQBjI+wBsOHmFubsTbYOAJhl+066AIDFrrn2+nzua1dMugzG7CuXXZ3EAi0AME7CHjBVXvj2z+T1H/nSpMtgA9xon23ZR2sPAMZG2AOmyuVXX5tb32x7Xvy4oyZdCmN265vtn/32MZsAAMZF2AOmylx3DrzxvnnwXW456VIAADY1/6QKTJVu87gAANaDsAdMlbnuyHoAAGsn7AFTZa7tvQYAsB6EPWDKdEQ9AIC1E/aAqTLXyTb/ZwIAWDN/UgFTpbst0AIAsA6EPWCqzHUM4wQAWAfCHjBVOhZoAQBYD8IeMFXa1gsAAOtC2AOmypw5ewAA60LYA6ZKm7MHALAuhD1gqujsAQCsD2EPmCrdMWcPAGAdCHvAVBH2AADWh7AHTJWOYZwAAOtB2AOmypzOHgDAuhD2gKligRYAgPUh7AFTpXvSFQAAzAZhD5gqrbMHALAuhD1gqnSSbbIeAMCaCXvAVJnrTunsAQCsmbAHTJVunT0AgPUg7AFTZa6TRNoDAFgrYQ+YKoMFWiZdBQDA5ifsAVNlMIxT2gMAWCthD5gqgwVaJl0FAMDmJ+wBU2Ww9YK0BwCwVsIeMFXmuq3PAgCwDoQ9YLqYswcAsC6EPWCqzHVr7AEArANhD5gqczZVBwBYF8IeMFU6bRgnAMA6EPaAqTI3Fwu0AACsA2EPmDo6ewAAayfsAVPFAi0AAOtD2AOmStt6AQBgXQh7wFSZ646sBwCwdsIeMFXmOilpDwBgzYQ9YMq0ffYAANaBsAdMlUFnb9JVAABsfsIeMFW6baoOALAehD1gqsy1PdUBANaDsAdMle62QAsAwDoQ9oCp0ubsAQCsC2EPmCpz5uwBAKwLYQ+YKp3YegEAYB0Ie8BUmTNnDwBgXQh7wFQxZw8AYH0Ie8BU6U7K5gsAAGsm7AFTpdPm7AEArANhD5gqc4ZxAgCsC2EPmCq2XgAAWB/CHjBVBgu0CHsAAGsl7AFTo7uTxPIsAADrQNgDpsYw6xnGCQCwDoQ9YGrMzXf2ZD0AgDUT9oCpMWzs2XoBAGAdCHvA1NjV2ZP2AADWat9JFwBL+eI3rsw/fvbrky6DDXTd3FwSwzgBANaDsMfU+qN/OC9/87EvT7oMJuCwm+8/6RIAADY9YY+p9Z3r53LEwQfkrc+836RLYQNt25bs2L7fpMsAANj0hD2mVyf77rMtNzvAH/4AALBSFmhhas1121wbAABWSdhjanXbXBsAAFZL2GNqzXVblREAAFZJ2GNqdey3BgAAqyXsMbW6O9tkPQAAWBVhj6k11zbXBgCA1RL2mFqDzp60BwAAq7HhYa+qblNV/1RV51bVp6vql4bHD6qqs6rqvOHnW2x0bUyXuY6tFwAAYJUm0dm7LsmzuvsuSX4oyTOr6q5JnpfkH7r7yCT/MLzPFmaBFgAAWL0ND3vd/ZXu/ujw9hVJzk1yWJLHJjlzeNmZSR630bUxXdrWCwAAsGoTnbNXVUckuXuSDye5ZXd/JRkEwiTft8Rjfraqzq6qsy+++OKNKpUJsKk6AACs3sTCXlUdmOSvk/xyd18+6uO6+/TuPq67j9u5c+f4CmTi5my9AAAAqzaRsFdV+2UQ9P5vd//N8PDXqurWw/O3TvL1SdTG9JjrTlmiBQAAVmUSq3FWkj9Lcm53v2zRqbclOWl4+6Qkb93o2pgubZ89AABYtX0n8Jr3TfLUJJ+sqo8Pjz0/yW8neWNVnZLkS0meOIHamCLdyTY7QQIAwKpseNjr7g9m6e3THryRtTDdOp1tJe0BAMBq+EuaqTVnNU4AAFg1YY+pZZ89AABYPWGPqTXXSUl7AACwKsIeU6u7bbwAAACrJOwxtTqxqToAAKySsMfUmuu2QAsAAKySsMfUsqk6AACsnrDH1LJACwAArJ6wx9SyQAsAAKyesMfUapuqAwDAqgl7TK05m6oDAMCqCXtMrcHWC9IeAACshrDH1NLZAwCA1RP2mF5W4wQAgFUT9phag03VJ10FAABsTsIeU6sTWy8AAMAqCXtMrUFnT9wDAIDVEPaYWnNz5uwBAMBqCXtMNVkPAABWR9hjalmgBQAAVk/YY2p1J2WJFgAAWBVhj6k1151tfkIBAGBV/CnN1JqzqToAAKyasMcUa4M4AQBglYQ9ptZcxz57AACwSsIeU6u7bb0AAACrJOwxtXT2AABg9YQ9ppbOHgAArJ6wx9Syzx4AAKyesMfUmuvONlkPAABWRdhjanViGCcAAKySsMfUGnT2pD0AAFgNYY+p1Z2YsgcAAKsj7DG12tYLAACwasIeU6tjgRYAAFgtYY+pNWfrBQAAWDVhj6ll6wUAAFg9YY+p1fZeAADg/2/v7oPtqso7jn9/8iLlZRAKYsaCppriIFMDRGhaUWgdQYcKURjC0A4II31TqpSZYpm2jLSV0nYYlaaOdhjQSokOYlO0QIyhgBQiSICENxmBCqGllILF8lLg6R973Xq43HNzE5Kcyz7fz8yZu8/aL+vZN2t27jPrWedoo5nsaVaqKgBn9iRJkqSNZLKnWanlen4apyRJkrSRTPY0K73Qsj1TPUmSJGnjmOxpVmoTe7zKOk5JkiRpo2w96gD65ov/cj833vfYqMN4xXvhhVr/QZIkSZKGMtnbxNY9/jR3PfyjUYfRC3vvsRP777XLqMOQJEmSXpEy8amHr0QLFiyom266adRhSJIkSdJIJLm5qhZMtc81e5IkSZLUQyZ7kiRJktRDJnuSJEmS1EMme5IkSZLUQyZ7kiRJktRDJnuSJEmS1EMme5IkSZLUQyZ7kiRJktRDJnuSJEmS1EMme5IkSZLUQyZ7kiRJktRDJnuSJEmS1EMme5IkSZLUQyZ7kiRJktRDJnuSJEmS1EMme5IkSZLUQyZ7kiRJktRDJnuSJEmS1EMme5IkSZLUQyZ7kiRJktRDJnuSJEmS1EMme5IkSZLUQyZ7kiRJktRDqapRx7DRkvwH8MCo4+ix3YBHRx2ENMAxqdnIcanZxjGp2cYxuXm9oap2n2rHKzrZ0+aV5KaqWjDqOKQJjknNRo5LzTaOSc02jsnRsYxTkiRJknrIZE+SJEmSeshkT9P5/KgDkCZxTGo2clxqtnFMarZxTI6Ia/YkSZIkqYec2ZMkSZKkHjLZkyRJkqQeMtnTSyQ5JsnaJC8kWTBp3yeS3Jvk7iSHjSpGja8kZyV5KMnq9nrfqGPSeEpyeHsW3pvkjFHHIwEkuT/J7e35eNOo49H4SXJBkkeSrBlo2zXJ8iTfbz93GWWM48RkT1NZA3wAuGawMck+wGLgrcDhwJIkW2358CTOq6r57fXNUQej8dOefX8NvBfYBziuPSOl2eDQ9nz0e800ChfS/Z046AxgRVXNA1a099oCTPb0ElV1Z1XdPcWuI4FLquqZqroPuBc4cMtGJ0mzwoHAvVX1g6p6FriE7hkpSWOtqq4BHpvUfCRwUdu+CDhqiwY1xkz2tCFeD/xw4P2DrU3a0j6S5LZWKmIpiEbB56FmqwKuSnJzklNGHYzU7FFVDwO0n68dcTxjY+tRB6DRSPIt4HVT7Dqzqv5h2GlTtPndHdrkphufwN8AZ9ONvbOBvwJO2nLRSYDPQ81ev1RV65K8Flie5K420yJpDJnsjamqevdGnPYgsOfA+58B1m2aiKSfmOn4TPIF4PLNHI40FZ+HmpWqal37+UiSy+hKjk32NGr/nmROVT2cZA7wyKgDGheWcWpDLAMWJ3l1krnAPGDViGPSmGn/SUxYRPeBQtKW9l1gXpK5Sbal+/CqZSOOSWMuyQ5JdprYBt6Dz0jNDsuAE9r2CcCwKjJtYs7s6SWSLAI+C+wOfCPJ6qo6rKrWJvkKcAfwHPA7VfX8KGPVWDo3yXy6krn7gd8YbTgaR1X1XJKPAFcCWwEXVNXaEYcl7QFclgS6v/EurqorRhuSxk2SvwcOAXZL8iDwx8A5wFeSnAz8K3DM6CIcL6lyiYEkSZIk9Y1lnJIkSZLUQyZ7kiRJktRDJnuSJEmS1EMme5IkSZLUQyZ7kiRJktRDJnuSNEaSPLkF+zo1yZ1Jvjyp/ZAkTyS5JcldSf5yBtc6JMnl6zlmfpL3Dbx/f5IzNv4O/v86JyapJL8y0LaotR39cq+/nr7PTLI2yW1JVic5aHP2N1NJzkryUItp4vWaUcclSXoxv2dPkrS5/Dbw3qq6b4p911bVEUl+CrglyWVV9Z2X2d98YAHwTYCqWsam+6Lz24HjgBXt/WLg1k107SklWQgcAexfVc8k2Q3YdjP3udUGfH/qeVU1NFFPsnVVPbeh1073JXGpqhdmGIckaQhn9iRpzCV5Q5IVbfZoRZK9WvubktyQ5LtJPjlsVjDJaUnWtNfHWtvngJ8FliX5+LC+q+opYDXw+nbeDkkuaH3ekuTIKfo7MMn1bf/1SfZOsi3wSeDYNst0bJuROz/JzknuT/Kqdv72SX6YZJt2j1ckuTnJtUneMiTUa4ED2zk7Am9ucU/EdECSf27XuTLJnNb+4XYvtya5NMn2rf3CJJ9p8f9gyAzhHODRqnqm/a4erap17fzD26zode06l7f2s5KcPhDXmiRvbNtfb/GtTXLKwDFPtn/fG4GFw+5lJtrv/KtJ/hG4qs3IrkxyMV3CPGy8vDHdLPAS4HvAnjPtU5I0nMmeJOl84ItV9fPAl4HPtPZPA5+uqrcD66Y6MckBwIeAg4BfAD6cZL+q+s12zqFVdd6wjpPsAswDrmlNZwLfbn0eCvxFkh0mnXYX8M6q2g/4I+DPqurZtr20quZX1dKJg6vqCbpZuHe1pl8Frqyq/wU+D3y0qg4ATgeWDAm1gG8BhwFHMjBjmGQb4LPA0e06FwB/2nZ/rareXlVvA+4ETh645hzgHXSzd+dM0edVwJ5J7kmyJMm7Wn/bAV9o93Ew8LohMU92UotvAXBqkp9u7TsAa6rqIODGae5lso8PlHCuHGhfCJxQVb/c3h8InFlV+wwbL+24venG4X5V9cAM70mSNA3LOCVJC4EPtO0vAecOtB/Vti8GpirZewdwWVX9GCDJ1+gSkFvW0+fBSW6j+wP/nKr6t9b+HuD9A7NT2wF7TTp3Z+CiJPPokrBt1tMXwFLgWGAlXQnmkjZD94vAV7vKQQBePc01LgFObf3/HvAHrX1vYF9gebvOVsDDbd++Sf4EeA2wI3DlwPW+3koV70iyx+TOqurJlhwdTJf4Lk23BnE1cF9VfR8gyd8Bp0w+fwqnJlnUtvekS7L/E3geuHQG9zLZsDLO5VX12MD7VQOlvMPGyzLggaq6YQb3IUmaIZM9SdJktQHHZv2HTGlizd7PAdelW7O3ul3vg1V194s6eXEydDawsqoWtRLFq2fQ3zLgU0l2BQ4Avk03o/V4Vc2fScBVtSrJvsBTVXXPQIIYYG1VLZzitAuBo6rq1iQnAocM7HtmYHvK32Nb43Y1cHWS24ET6JK9Yf9Gz/Hiqp3toPuAG+DdwMKq+p8kV0/sA54eWEs33b3M1I+nIlKpmwAAAgBJREFUeT/deJl8niTpZbKMU5J0Pd1sF8DxwHVt+wbgg2178eSTmmuAo9o6uB2ARXTr22akqu4BPgX8fmu6EvhoWiY1UOI3aGfgobZ94kD7fwM7DennSWAVXWnq5VX1fFX9CLgvyTGtryR523pC/gQ/mdGbcDewe7oPVKGt63tr27cT8HAr9Tx+Pdd+kbYWcd5A03zgAboy1rlJ3tTajxs45n5g/3b+/sDc1r4z8F8t0XsLXQnlVKa7l03hZY0XSdKGMdmTpPGyfZIHB16n0ZUmfqiVVf468Lvt2I8BpyVZRbe+7InJF6uq79HNXq2iW+/1t1W1vhLOyT4HvDPJXLpZu22A25Ksae8nO5dulu47dGWGE1YC+7Q1ZMdOcd5S4NfazwnHAycnuRVYS7ceb6iq+qeqWjmp7VngaODP23VW05WHAvwh3e9lOV2StiF2pCtXvaP92+wDnFVVT9OVbX4jyXV0CeCES4Fdk6wGfgu4p7VfAWzdrnM2XSI/1f1Ndy+TDa7ZWz3xQTDT2UTjRZI0Q6nakGodSdK4SPfJkU9VVSVZDBxXVdMmQ9ryWonm6VV1xKhjkSTNLq7ZkyQNcwBwfiupfBw4acTxSJKkDeDMniRJkiT1kGv2JEmSJKmHTPYkSZIkqYdM9iRJkiSph0z2JEmSJKmHTPYkSZIkqYf+D01TpN/wXsa9AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the error frequency for model comparison\n",
    "from matplotlib import pyplot as plt\n",
    "num_eqns = len(resultDict[fName]['SymbolicGPT'])\n",
    "num_vars = pconf.numberofVars\n",
    "\n",
    "models = list(key for key in resultDict[fName].keys() if len(resultDict[fName][key])==num_eqns)\n",
    "lists_of_error_scores = [resultDict[fName][key] for key in models if len(resultDict[fName][key])==num_eqns]\n",
    "linestyles = [\"-\",\"dashdot\",\"dotted\",\"--\"]\n",
    "\n",
    "eps = 0.00001\n",
    "y, x, _ = plt.hist([np.log([max(min(x+eps, 1e5),1e-5) for x in e]) for e in lists_of_error_scores],\n",
    "                   label=models,\n",
    "                   cumulative=True, \n",
    "                   histtype=\"step\", \n",
    "                   bins=2000, \n",
    "                   density=True,\n",
    "                   log=False)\n",
    "y = np.expand_dims(y,0)\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for idx, m in enumerate(models): \n",
    "    plt.plot(x[:-1], \n",
    "           y[idx] * 100, \n",
    "           linestyle=linestyles[idx], \n",
    "           label=m)\n",
    "\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.title(\"{} equations of {} variables - Benchmark\".format(num_eqns, num_vars))\n",
    "plt.xlabel(\"Log of Relative Mean Square Error\")\n",
    "plt.ylabel(\"Normalized Cumulative Frequency\")\n",
    "\n",
    "name = '{}.png'.format(fName.split('.txt')[0])\n",
    "plt.savefig(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NGUYEN-10 --> Target:2*sin(x1)*cos(x2)\n",
      "Predicted:log(abs(exp(1.0*x1)))\n",
      "Err:0.07436004532630844\n",
      "\n",
      "NGUYEN-11 --> Target:x1**x2\n",
      "Predicted:sqrt(abs(1.0*x1**2))\n",
      "Err:0.04597928135266842\n",
      "\n",
      "NGUYEN-12 --> Target:x1**4-x1**3+x2**2/2-x2\n",
      "Predicted:log(abs(1.0*x1))\n",
      "Err:2.662821440368008\n",
      "\n",
      "NGUYEN-1 --> Target:x1**3+x1**2+x1\n",
      "Predicted:2.0*x1\n",
      "Err:0.08973579546591479\n",
      "\n",
      "NGUYEN-2 --> Target:x1**4+x1**3+x1**2+x1\n",
      "Predicted:2.0*x1\n",
      "Err:0.18876219523142349\n",
      "\n",
      "NGUYEN-3 --> Target:x1**5+x1**4+x1**3+x1**2+x1\n",
      "Predicted:4.0*x1\n",
      "Err:0.6610746693797507\n",
      "\n",
      "NGUYEN-4 --> Target:x1**6+x1**5+x1**4+x1**3+x1**2+x1\n",
      "Predicted:4.0*x1**2+sin(1.0*x1)\n",
      "Err:0.06631700903386478\n",
      "\n",
      "NGUYEN-5 --> Target:sin(x1**2)*cos(x1)-1\n",
      "Predicted:log(abs(1.0*x1**2))\n",
      "Err:3.592194233969905\n",
      "\n",
      "NGUYEN-6 --> Target:sin(x1)+sin(x1+x1**2)\n",
      "Predicted:1.0*x1+sin(sqrt(abs(1.0*x1)))\n",
      "Err:0.5954981508716823\n",
      "\n",
      "NGUYEN-7 --> Target:log(x1+1)+log(x1**2+1)\n",
      "Predicted:1.0*x1*sqrt(abs(1.0*x1))*sin(1.0*x1)\n",
      "Err:0.13763041363018041\n",
      "\n",
      "NGUYEN-8 --> Target:sqrt(x1)\n",
      "Predicted:sqrt(abs(1.0*x1))\n",
      "Err:0.0\n",
      "\n",
      "NGUYEN-9 --> Target:sin(x1)+sin(x2**2)\n",
      "Predicted:log(abs(1.0*x1**2+sqrt(abs(1.0*x1))))\n",
      "Err:0.9468142516047345\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# benchmarks\n",
    "import csv\n",
    "benchmarkPath = './benchmark/dsr-benchmark-data/'\n",
    "dataPoints = glob.glob(benchmarkPath+'*.csv')\n",
    "NGUYEN2Eq = {\n",
    "           '1':'x1**3+x1**2+x1',\n",
    "           '2':'x1**4+x1**3+x1**2+x1',\n",
    "           '3':'x1**5+x1**4+x1**3+x1**2+x1',\n",
    "           '4':'x1**6+x1**5+x1**4+x1**3+x1**2+x1',\n",
    "           '5':'sin(x1**2)*cos(x1)-1',\n",
    "           '6':'sin(x1)+sin(x1+x1**2)',\n",
    "           '7':'log(x1+1)+log(x1**2+1)',\n",
    "           '8':'sqrt(x1)',\n",
    "           '9':'sin(x1)+sin(x2**2)',\n",
    "           '10':'2*sin(x1)*cos(x2)',\n",
    "           '11':'x1**x2',\n",
    "           '12':'x1**4-x1**3+x2**2/2-x2',}\n",
    "for dataPoint in dataPoints: \n",
    "    key = dataPoint.split('\\\\')[-1].split('Nguyen-')[-1].split('_')[0]\n",
    "    if not key in NGUYEN2Eq.keys():\n",
    "        continue\n",
    "    target = NGUYEN2Eq[key]\n",
    "    with open(dataPoint, newline='\\n') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "        pointsList = []\n",
    "        pointsListTest = []\n",
    "        for i, c in enumerate(reader):\n",
    "            p = ([eval(x) for x in c[:-1]],eval(c[-1]))\n",
    "            if i < numPoints:\n",
    "                pointsList.append(p)\n",
    "            else:\n",
    "                pointsListTest.append(p)\n",
    "                \n",
    "        #pointsList = [([eval(x) for x in p[:-1]],eval(p[-1])) for i,p in enumerate(reader) if i < numPoints]\n",
    "        #pointsListTest = [([eval(x) for x in p[:-1]],eval(p[-1])) for i,p in enumerate(reader) if i >= numPoints]\n",
    "        \n",
    "        # initialized the input variable with start token <\n",
    "        inputs = torch.tensor([[train_dataset.stoi['<']]]).to(trainer.device)\n",
    "        \n",
    "        # extract points from the input sequence\n",
    "        points = torch.zeros(numVars+numYs, numPoints)\n",
    "        for idx, xy in enumerate(pointsList):\n",
    "            x = xy[0] + [0]*(max(numVars-len(xy[0]),0)) # padding\n",
    "            y = [xy[1]] if type(xy[1])== float else xy[1]\n",
    "            y = y + [0]*(max(numYs-len(y),0)) # padding\n",
    "            p = x+y # because it is only one point \n",
    "            p = torch.tensor(p)\n",
    "            #replace nan and inf\n",
    "            p = torch.nan_to_num(p, nan=0.0, \n",
    "                                 posinf=train_dataset.threshold[1], \n",
    "                                 neginf=train_dataset.threshold[0])\n",
    "            p[p>train_dataset.threshold[1]] = train_dataset.threshold[1] # clip the upper bound\n",
    "            p[p<train_dataset.threshold[0]] = train_dataset.threshold[0] # clip the lower bound\n",
    "            points[:,idx] = p\n",
    "            \n",
    "        points = points.unsqueeze(0).to(trainer.device)\n",
    "        outputsHat = sample(model, inputs, blockSize, points=points,\n",
    "                      temperature=1.0, sample=False, \n",
    "                      top_k=40)[0]\n",
    "        \n",
    "        # filter out predicted\n",
    "        predicted = ''.join([train_dataset.itos[int(i)] for i in outputsHat])\n",
    "        predicted = predicted.strip(train_dataset.paddingToken).split('>')\n",
    "        predicted = predicted[0] if len(predicted[0])>=1 else predicted[1]\n",
    "        predicted = predicted.strip('<').strip(\">\")\n",
    "        \n",
    "        # extract points from the input sequence\n",
    "        pointsTest = torch.zeros(numVars+numYs, numPoints).numpy()\n",
    "        for idx, xy in enumerate(pointsListTest):\n",
    "            x = xy[0] + [0]*(max(numVars-len(xy[0]),0)) # padding\n",
    "            y = [xy[1]] if type(xy[1])== float else xy[1]\n",
    "            y = y + [0]*(max(numYs-len(y),0)) # padding\n",
    "            p = x+y # because it is only one point \n",
    "            p = torch.tensor(p)\n",
    "            #replace nan and inf\n",
    "            p = torch.nan_to_num(p, nan=0.0, \n",
    "                                 posinf=train_dataset.threshold[1], \n",
    "                                 neginf=train_dataset.threshold[0])\n",
    "            p[p>train_dataset.threshold[1]] = train_dataset.threshold[1] # clip the upper bound\n",
    "            p[p<train_dataset.threshold[0]] = train_dataset.threshold[0] # clip the lower bound\n",
    "            pointsTest[:,idx] = p\n",
    "        \n",
    "        Ys = [] \n",
    "        Yhats = []\n",
    "        for xs in pointsTest[:-1,:].T:\n",
    "            try:\n",
    "                eqTmp = target + '' # copy eq\n",
    "                eqTmp = eqTmp.replace(' ','')\n",
    "                eqTmp = eqTmp.replace('\\n','')\n",
    "                for i,x in enumerate(xs):\n",
    "                    # replace xi with the value in the eq\n",
    "                    eqTmp = eqTmp.replace('x{}'.format(i+1), str(x))\n",
    "                    if ',' in eqTmp:\n",
    "                        assert 'There is a , in the equation!'\n",
    "                YEval = eval(eqTmp)\n",
    "                YEval = 0 if np.isnan(YEval) else YEval\n",
    "                YEval = 100 if np.isinf(YEval) else YEval\n",
    "            except:\n",
    "                YEval = 100 #TODO: Maybe I have to punish the model for each wrong template not for each point\n",
    "            Ys.append(YEval)\n",
    "            try:\n",
    "                eqTmp = predicted + '' # copy eq\n",
    "                eqTmp = eqTmp.replace(' ','')\n",
    "                eqTmp = eqTmp.replace('\\n','')\n",
    "                for i,x in enumerate(xs):\n",
    "                    # replace xi with the value in the eq\n",
    "                    eqTmp = eqTmp.replace('x{}'.format(i+1), str(x))\n",
    "                    if ',' in eqTmp:\n",
    "                        assert 'There is a , in the equation!'\n",
    "                Yhat = eval(eqTmp)\n",
    "                Yhat = 0 if np.isnan(Yhat) else Yhat\n",
    "                Yhat = 100 if np.isinf(Yhat) else Yhat\n",
    "            except:\n",
    "                Yhat = 100\n",
    "            Yhats.append(Yhat)\n",
    "        err = relativeErr(Ys,Yhats)\n",
    "        \n",
    "        print('NGUYEN-{} --> Target:{}\\nPredicted:{}\\nErr:{}\\n'.format(key, target, predicted, err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
